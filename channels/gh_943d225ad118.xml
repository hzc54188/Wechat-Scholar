<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AI for Research]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AI for Research公众号]]></description>
    

    <language>zh-cn</language>
    



















    <item>
      <title><![CDATA[Resonance RoPE: 改进大模型上下文长度泛化 | 标记化对语言模型性能的重要影响 | 视频LLM真的理解视频吗?]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdfF7k76P2P9icicGmamT7q7ps3dfJ4LebuEs88jarMyDE6w1tKOCoNaicyLa8qSvibVs6ohb9cTMalm0A/640?wxtype=png&amp;wxfrom=0"/><p>前言：科研就像一场冒险，而看论文就是你的探险工具！只有通过深入阅读，才能找到宝藏，发现那些意想不到的科研奇遇哦！1. Resonance RoPE: 改进大模型的上下文长度泛化能力  标题：Reson</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485019&amp;idx=1&amp;sn=924b2917432759d1e3ecc40b652c4ad1&amp;chksm=c1997dd3d703033ae6795ed9a228e2c9c1fe264c94322a6bfd3b6bbfd52187bda776db0e30c9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 04 Mar 2024 11:04:11 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[本周大模型Top热门论文精选【2024—第9期】]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdePibiaHpr0jOGPyYnZicXNBc6NqsBoz1KN5yoCicZyib0CJwMnhqbyQbZncHxpBq6ZZ6oMkkz4LgVywoA/640?wxtype=png&amp;wxfrom=0"/><p>前言：科研就像一场冒险，而看论文就是你的探险工具！只有通过深入阅读，才能找到宝藏，发现那些意想不到的科研奇遇哦！1. 当规模化遇到大模型微调：数据、模型及微调方法的效应研究  标题：When Scal</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485015&amp;idx=1&amp;sn=fb65c0644bab816c047d66ac53321929&amp;chksm=c1187cc2b2a1c52d4733d27bb6c97e467c4a5f8a577ef77bc1ced880f2295e58f77b0bed113b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 03 Mar 2024 04:23:45 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[为什么大语言模型不擅长编程？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdfgUJuvetPWBZpHO0NbGBMvyzicV25sNFMkZfVrNhOpicogqoBA6msA1pMhOyD9vArTlFeCNtVIe34g/640?wxtype=jpeg&amp;wxfrom=0"/><p>过去一年里，大语言模型（LLMs）凭借其自然语言理解能力展现了惊人的能力。这些先进的模型不仅重新定义了自然语言处理的标准，还广泛应用于各种应用和服务中。随着对使用LLMs进行编码的兴趣迅速增长，一些公</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485012&amp;idx=1&amp;sn=955c7662d57e83bda7403f01c4270e86&amp;chksm=c161f5bd4ec2453cee4ad4c82ecaf9ce29ae4ff639f8d3c12517221877f645a5484d9a006e49&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 02 Mar 2024 11:48:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[StarCoder 2 发布开源！ | 大模型可塑性衰减原因揭秘 | 万亿级高质量“万卷” 语料库开源 ......]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRnH4yaB6GTjVnp27yeyYepkV3jLnfmFE2iaNjOl5oPQgTr4w6BZibjhf9A/640?wxtype=png&amp;wxfrom=0"/><p>前言：平淡无奇的一天又来了，今天要分享的内容主要是关于大语言模型、语言模型、多模态的，喜欢的小伙伴赶紧去阅读相关论文吧。1. 大模型学习能力（可塑性）衰减原因揭秘  标题：Disentangling </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485000&amp;idx=1&amp;sn=cd438eb51c2148db3213f2afd50c48cf&amp;chksm=c1bda30961091845d28909a3dca4a0c7507a92d88718137ed8b3015409df875b2fac9dd3884e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 01 Mar 2024 10:48:40 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Sora：探索大型视觉模型的前世今生、技术内核及未来趋势 [译]]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdc3UP6D7iasLRD3KvGhztr65dicDWEicW7DCcUc67652gcjKIQdFOekYjmaeib0S1IjFwJePlThb3kfsw/300?wxtype=jpeg&amp;wxfrom=0"/><p>原文：Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models作者</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485000&amp;idx=2&amp;sn=8a426eb4deade7e03c17294a48b52147&amp;chksm=c1553f1e0149c371230b2ef774261aec41707fce2037a141c3ff8b35b1948050fe670c1624a7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 01 Mar 2024 10:48:40 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型训练集全面调研—最新综述 | Stable LM 2 1.6B 技术报告发布 | Tokenization 仅仅是压缩吗]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRnFuKxnu49iagmsGPtQp7vD8MjLpOwTfQtpibqbhPLJIUlqrFerFTmUW3w/640?wxtype=png&amp;wxfrom=0"/><p>前言：论文可以让你更快地了解最新研究进展，掌握最新的技术和理论。这对于自身的科研能力和竞争力非常重要，尤其是在快速发展的学科领域，下面小编带你来看大模型最近的研究成果。1. 大模型的训练数据集：最新综</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484974&amp;idx=1&amp;sn=c8382f131e3ab544192b0f944fe23d6f&amp;chksm=c169ac858b2cd26f22fe58751b49c99839ada8043cbd9f6a2b04ebb625109261bcf9121b2bcf&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Feb 2024 06:03:55 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[微软发布Sora技术详细报告！ |  谷歌发布 LLM 微调技术 Scaling | 阿里发布无需训练即可扩展到100K上下文]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdejnS1WjuSnCOggAxNcXicBicGZL6JMfhgnv0CjIqdxRlpD3kpRYLV1BLOyT2yauZ3vTseiabUL76kVw/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：平淡无奇的一天又来了，今天要分享的内容主要是关于大语言模型、多模态、预训练的，喜欢的小伙伴赶紧去阅读相关论文吧。1. Sora: 大视觉模型的背景、技术、限制和机遇综述  标题：Sora: A </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484971&amp;idx=1&amp;sn=aabadfa70c29191006e76ab12aa9761d&amp;chksm=c1a5a46a02de83f9ff1dfa8ba150dd2b53a10b59d1abfcbbaf6284b43ca94ef5ea473049bbc9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 28 Feb 2024 10:51:12 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型训练数据筛选方法调研 | 预训练语言模型的时间对齐 | 大模型实现多语言能力的关键......]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRnNVEE0yeD982yMSwTIiaRrAhzbIESKHt7NfmXznb6Sb86iawL5dg5qFnw/640?wxtype=png&amp;wxfrom=0"/><p>前言：科研就像一场冒险，而看论文就是你的探险工具！只有通过深入阅读，才能找到宝藏，发现那些意想不到的科研奇遇哦！1. 大模型数据选择调研   标题：A Survey on Data Selection</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484965&amp;idx=1&amp;sn=13d2645e7e25ecbf93a4582bbfa37740&amp;chksm=c1c87af5692c0947f8c641d2ed792a3c2a0b1106fbda135e63476e47a31e8b3ffff946957565&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Feb 2024 15:36:03 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[数据增强已死？数据增强万岁！| LLMs中令牌化对算术的影响 | MobileLLM: 构建移动设备上的亿级参数模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRnIRP53Qe4Dia4ss1LsOcI1EMbviayBTYCibOtnm7sdXcicVgib9SibOzgvv7g/640?wxtype=png&amp;wxfrom=0"/><p>前言：论文可以让你更快地了解最新研究进展，掌握最新的技术和理论。这对于自身的科研能力和竞争力非常重要，尤其是在快速发展的学科领域，下面小编带你来看大模型最近的研究成果。1. 数据增强已死？数据增强万岁</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484963&amp;idx=1&amp;sn=59ae5a52742518ea150453fc8ffd8eca&amp;chksm=c19815bfcb1a65a42c1164cc2cee076da4af4bff25f9d65b983d78aac4b8f9393dc28e867444&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 26 Feb 2024 08:11:40 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[本周大模型Top热门论文精选【2024—第8期】]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRnva4cMZ1aO4NaaUnIaiacFuvDpSbh7GqeYGwKv9vej9Gev6yJYJHKiaeQ/640?wxtype=png&amp;wxfrom=0"/><p>前言：平淡无奇的一天又来了，今天要分享的内容主要是关于大语言模型、多模态、语言模型的，喜欢的小伙伴赶紧去阅读相关论文吧。1. 指令多样性增强模型对未见任务的泛化能力  标题：Instruction D</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484959&amp;idx=1&amp;sn=90c5b14b8d204b642254060d68aa3cfa&amp;chksm=c14cefc9a0f3f48e4768fe3cc196b7823072882cb0cbf5155e38e51fc6c007c7cdc441ab8be4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 25 Feb 2024 04:44:23 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Phind 升级至 70B！代码生成质量达到 GPT-4 Turbo级别，But 运行速度提高 4 倍 !]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdfwznTP63YAPAIBhLbsj4rribXqoKfoha0DqofwDPRyOCfEHasiaia0XBv2ugraaI9icxlGqusMAibyx4Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>不知道大家还记不记得Phind，3个月前，它就在huggingface（笑脸）的代码大模型排行榜中位居第一名，也是首个击败 GPT-4 的开源代码项目（不像某软，早早就在 github上吹 beat </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484957&amp;idx=1&amp;sn=d6976e5d0e4ac6e16aaf68926e512290&amp;chksm=c16bc0a19adf42356f5ad518c4c7ffb1e9faad77045d1f2fb345298bc6ba2e21345990573d1d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 24 Feb 2024 03:02:25 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[模型泛化技能数量翻倍，训练语料库规模需要增加多少倍？| 持续训练中如何提升大模型泛化能力？|  聚类算法对预训练语料的有效筛选]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRnH4yaB6GTjVnp27yeyYepkV3jLnfmFE2iaNjOl5oPQgTr4w6BZibjhf9A/640?wxtype=png&amp;wxfrom=0"/><p>前言：平淡无奇的一天又来了，今天要分享的内容主要是关于大语言模型、预训练、多模态的，喜欢的小伙伴赶紧去阅读相关论文吧。1. 难样本加权持续训练提升大模型泛化能力  标题：Take the Bull b</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484945&amp;idx=1&amp;sn=b9814a2e2ba4ecdbec297e6346c7de5b&amp;chksm=c1a130c0850265f3f3a9057af878fb6fab6a1507e4a72a93e0e1667224628444cf321f861094&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 23 Feb 2024 07:10:55 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[1000步微调实现256K上下文窗口 | 文档简单拼接输入对预训练有何影响？| 大模型的达芬奇密码：解密退化知识神经元 ....]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdeicjz7VSRwn1pArVIiaxQPZ64B6SePQ8uOkqgzGBT2ricibRBHgbPfX6cajy3IN2TvyMYd0GP1fXzbsQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：看论文就像是一次美食之旅，每一篇论文都是一道不同的菜肴。有些论文会让你大快朵颐，有些论文会让你欲罢不能，而有些论文则会让你咬牙切齿。但是别忘了，只有尝试了各种不同的菜肴，才能成为一个真正的“吃货</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484943&amp;idx=1&amp;sn=c2a251e7f686d592432bd1d2e17eb897&amp;chksm=c1c850b78fe1e079cac767c2d5ddb11b79754589005fa48ffdba50f2b418b1f8c74017e1ca45&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 22 Feb 2024 07:33:21 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[微调、提示、上下文学习和 SFT 到底需要多少标注样本？| GPT4内在知识如何蒸馏？最新最热的Arxiv论文来啦！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdePibiaHpr0jOGPyYnZicXNBc6gwibYCmus7QgibJYicDQanOkFYqXO7LOLDobEY4Y7VhCMlNkic7eUm8PSQ/640?wxtype=png&amp;wxfrom=0"/><p>前言：论文可以让你更快地了解最新研究进展，掌握最新的技术和理论。这对于自身的科研能力和竞争力非常重要，尤其是在快速发展的学科领域，下面小编带你来看大模型最近的研究成果。1. 微调、提示、上下文学习和S</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484938&amp;idx=1&amp;sn=6233a4282b577d6216bff4c3a6e19dac&amp;chksm=c1aaf2aff8a902e6b97209ed141b189e401b4dee3a0cf0fc1f2e9d5f21f8c97ecd50ac706d0c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 21 Feb 2024 07:15:41 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Sora 技术报告参考的32篇论文都讲了啥？—— 从引用角度窥探 Sora 技术实现]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdcjVOsBXgLLBH5hQKJ7Bfr4Yaic7WIGBo58UkGVJR0leWg3WW8hvsaThEjNMLE9syhxiccibUwB4558w/300?wxtype=jpeg&amp;wxfrom=0"/><p>CloseAI 从chatGPT到dalle3，再到现在的Sora，秉承了它一贯的作风，好像说了又好像什么都没说，真是听君一席话如听一席话，不过好在他们还初心未泯，做研究嘛，不能说的不说，但是该说的还</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484938&amp;idx=2&amp;sn=b3a901e87c2304da637b038eb6062f32&amp;chksm=c1f36f91bc0553073f4aceb355a8e2c13aef9830f22d3fc5a56bafef6ae3b504917e779c7fc9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 21 Feb 2024 07:15:41 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Arxiv论文精选 揭密预训练数据对大语言模型的影响]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRn1icWicbhtMY7BWWYduvxTmP2nEiasP2ybfdXkvSrHsoBlia3718vCaxbDg/640?wxtype=png&amp;wxfrom=0"/><p>前言：论文可以让你更快地了解最新研究进展，掌握最新的技术和理论。这对于自身的科研能力和竞争力非常重要，尤其是在快速发展的学科领域，下面小编带你来看大模型最近的研究成果。1. Multi-Task In</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484932&amp;idx=1&amp;sn=7634174c8b9b1341716ac7cf564a3bd0&amp;chksm=c1cc810890fac36b943111e422c5c2a2525dd8fae233def47ab3dc6253f3ac5360181448a9ab&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 20 Feb 2024 12:57:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[1000万token大海寻针：递归记忆法找到了大模型忽略的内容 | 指令多样性是任务泛化的关键 | 用小模型为大模型选择微调数据]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBde55plOgIOgEmibQf9DRMInCUd0aqtaCEUJpfXWRTmv3X06SFbYGmprtfRv7W0YkmM5f1VU5fnJTkg/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：论文可以让你更快地了解最新研究进展，掌握最新的技术和理论。这对于自身的科研能力和竞争力非常重要，尤其是在快速发展的学科领域，下面小编带你来看大模型最近的研究成果。1. 在1000万token的大</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484920&amp;idx=1&amp;sn=d5604b02218a9af6791b0608a1406743&amp;chksm=c1906102abcc0f9185bc3ded082e4433d2e73d1e49f74598f94e97806d27e142b9d54c8815d8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 19 Feb 2024 09:19:43 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[本周大模型Top热门论文精选【2024—第7期】]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_png/0XibHbUBQBdeI89uevGP3Meic1MpBTBaRnca32DCQeSzUQOwS5lr9oAZAurFHibjqdRjDkIKKJYf1zcp2CiaAKGQ7Q/640?wxtype=png&amp;wxfrom=0"/><p>前言：平淡无奇的一天又来了，今天要分享的内容主要是关于大语言模型、指令微调、预训练的，喜欢的小伙伴赶紧去阅读相关论文吧。1. 在微调预训练大模型时应重复训练哪些样本以防止遗忘？  标题：Which P</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484905&amp;idx=1&amp;sn=37c338d9423885676089f7cf1e09c54f&amp;chksm=c109c96e502bcf5db7114cb2baa28daead41b7bdcf53414de09b70aade6aa61bf981be2587f1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 18 Feb 2024 07:40:25 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OpenAI Sora 实现技术猜测 —— 看看 CV 大神谢赛宁的观点......]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddtTggmXibaA7gOmhF7NOefibHiculsVu1Et0SH4mvlw0fichCABvukedHicT8W8BcpPCWLwMfBSKTLHjQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者介绍：谢赛宁(Saining Xie)，纽约大学计算机科学助理教授。本科毕业于上海交通大学，18年获加州大学圣迭戈分校CS博士学位；毕业后加入在Facebook AI Research（FAIR）</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247484903&amp;idx=1&amp;sn=be487be6b4ab5bf20ffb535db2fbb676&amp;chksm=c16422a6c2a2e95fe608d3759326593110d0edabfa4f8a4039143f2101e7d73c9ce7b25326fe&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 17 Feb 2024 13:59:15 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
