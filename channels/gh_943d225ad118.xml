<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AI for Research]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AI for Research公众号]]></description>
    

    <language>zh-cn</language>
    














    <item>
      <title><![CDATA[时间序列大模型的规模定律 | 脑机接口能提取多少字节的大脑信息解码文本？| 360发布智脑技术报告....]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddvoaeRHLGib0oCFColkftyLV23AVV9u0KN4F5JWl4Q3woyYbqBurHOKc90rIZwYMkvRicMJt1WsCbQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：看论文就像是一次美食之旅，每一篇论文都是一道不同的菜肴。有些论文会让你大快朵颐，有些论文会让你欲罢不能，而有些论文则会让你咬牙切齿。但是别忘了，只有尝试了各种不同的菜肴，才能成为一个真正的“吃货</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485868&amp;idx=1&amp;sn=2e0d1e7254b336d2aaab439fe627f0ae&amp;chksm=c117e915859ba6bb610c66975e62b55fa2fa2ea33f5c8db9c482106896ab09d9227a110f07bd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sat, 25 May 2024 08:23:54 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[给普通人的26条提示工程策略.....]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddvoaeRHLGib0oCFColkftyLEDHQY0WIJcI3Rsia2BTaQiaWQJMjic3ZwnBllFTSIbujCbYicibvJXrrs4Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>1、与LLM交流不需要礼貌，所以无需添加“请”、“如果你不介意”、“谢谢”、“我想”等短语，直接切入主题。2、在提示中整合预期的受众，例如，受众是该领域的专家。3、将复杂任务分解为一系列更简单的提示，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485868&amp;idx=2&amp;sn=a802f8416dde945583214cae6f36d52b&amp;chksm=c1689731988b0b52ace066312a3380d17b80c9f3d4e6c78c9c995e8c230b6273c5a8a88eda51&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sat, 25 May 2024 08:23:54 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[Transformer其实是线性的? | SirLLM: 流式无限记忆LLM | SPO: 多维偏好序列对齐与隐式奖励建模...]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdfmvZicHsqH21kk2E1CyoxsIyBP6bpPvnMRMZQROgRRcAwFAm14pzXagjHXL3mPfEvlwMicia6sv6Ymg/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：看论文就像是一次美食之旅，每一篇论文都是一道不同的菜肴。有些论文会让你大快朵颐，有些论文会让你欲罢不能，而有些论文则会让你咬牙切齿。但是别忘了，只有尝试了各种不同的菜肴，才能成为一个真正的“吃货</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485839&amp;idx=1&amp;sn=94b4a4eafd778671500ee13879ef4c90&amp;chksm=c18ab57fe4066fa0eaf216d5c8b73d2d66bcd572dac1c1aa2995ae31a52b4db33187f37505aa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 22 May 2024 09:58:51 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里发布OpenRLHF：易于使用、可扩展且高性能的RLHF框架 | ViViD: 使用扩散模型进行视频虚拟试穿...]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBded6wh8TjURa0VW8vweniaBHh6xO5WYCXLdGzmyQib95ENsawRiaYOjd7kPN8DQ2d64E8d0yjsNJOHKQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：论文可以让你更快地了解最新研究进展，掌握最新的技术和理论。这对于自身的科研能力和竞争力非常重要，尤其是在快速发展的学科领域，下面小编带你来看大模型最近的研究成果。1. OpenRLHF: 一个易</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485834&amp;idx=1&amp;sn=34198ebebd4745a846da8932f37c14dd&amp;chksm=c1444d87b3765c2021858385f9f5c3e2817e0f467dcb68757d9e375d6b349992ad0f198f8c9d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 May 2024 13:16:30 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[缩放定律与模型性能的可预测性 | 从Sora能看到什么：文本到视频生成的综述 | OpenAI发布人类交互评估新方法...]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddPhicbNBqwPHDD9zNw9CiaAtFTuWkRZP1sHJRcOKd480yicBxUlKkt6DxPHIeRMtvWKp4o8pYhkic4nQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：如果你想成为一只科学界的“独角兽”，那么看论文是必不可少的。只有掌握了最新的技术和理论，才能在这个竞争激烈的市场中脱颖而出，成为那只最闪亮的“独角兽”！1. 缩放定律与语言模型性能的可预测性  </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485830&amp;idx=1&amp;sn=019fd9b211817e0dbee6a1f811527e8c&amp;chksm=c1f5b021974cbaf130e23e3a9b20e00f7699e8d7d2be4b286e63cddc8379f0891f65618ccab8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 May 2024 11:16:17 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[本周大模型Top热门论文精选 —— 第20期]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdfzuQvLa7ac5ibsm62p2KNybic1nDWfibbqcdYhwkvDDXPEa3R548KQ5TpFxfDmTx6dyd0J2IsE8lfBQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：如果你想成为一只科学界的“独角兽”，那么看论文是必不可少的。只有掌握了最新的技术和理论，才能在这个竞争激烈的市场中脱颖而出，成为那只最闪亮的“独角兽”！1. Beyond Scaling Law</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485825&amp;idx=1&amp;sn=7496b8d5b0dbdf0fb9868ef61e942bd0&amp;chksm=c173e3fade425c3edac0ea2b4555cb8a82389c70ee58c6c0c8ab961b428fe4f4048393609f72&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 May 2024 05:39:58 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[为什么简单地增大 Transformer 模型的规模并不一定能提升性能？华为研究人员通过关联记忆找到其中....]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddcxfvMx8ZDPYAxJicu6zdpFnSkpFwPEBHwP7hnKnvmlUS1QiaDoW5ibqQyziazkM7g1BlapvgypzddAA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天我们来聊一个非常有意思的话题：Transformer 模型的规模与性能之间的关系。你可能听说过，更大的模型通常意味着更强大的能力，但真的是这样吗？最近，华为技术有限公司的研究人员在一篇新论</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485825&amp;idx=2&amp;sn=2299f84e1f7453c8cc0e7d4e71020842&amp;chksm=c13bb43877558c5d7a74478917bb614fc4388e6dabaacff1c61acb4e7ba599cda9a9631f23aa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 May 2024 05:39:58 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[AI 原生应用产品开发之道，通过 10 个工具一天内启动在线业务！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddcxfvMx8ZDPYAxJicu6zdpFbTWNYdMKX9icDI7xX7VIelxWsFyln4aYd9M8eV9AicoMYIKBGclb80Kg/300?wxtype=jpeg&amp;wxfrom=0"/><p>目前很多AI相关的工具在国外已经非常成熟了，基于这些AI工具，完全可以快速搭建一个MVP，所以如果我是一个编程小白，如何基于AI把自己的天才想法实现呢？以下是基于AI快速打造一个应用产品可能用到的AI</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485825&amp;idx=3&amp;sn=311ecb94db707cea92e2e2a5e3f9aa75&amp;chksm=c10acbd6d205459337fd5e3808e311998f4215c1d715af2786f8334d6fb445d48eef64c060a4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 May 2024 05:39:58 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[为什么 GPT-4 比一年前更“聪明”了？主要都是后训练（Post-Training）带来的！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddcxfvMx8ZDPYAxJicu6zdpFaSEjO0jLHJg4hQsuPtwSveic2ERW7am05JKMEiaW7lJicUFKfRZnLGZNg/640?wxtype=jpeg&amp;wxfrom=0"/><p>OpenAI 创始人 John Schulman 访谈节选：为什么 GPT-4 比一年前更“聪明”了？他认为，在强化学习研究领域，研究人员需要具备丰富的经验和敏锐的直觉。了解整个技术堆栈，并对各个部分</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485792&amp;idx=1&amp;sn=c2dfff32640382d04d16a34fa2f130e4&amp;chksm=c17344101d87246c9cf199c80e9c800914e32dd35890a1fb5f65c9d0cb58ad84bce9cc8eb591&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 18 May 2024 10:59:58 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[数据增强方法综述 | LoRA学习新知识较少但遗忘更少 | GPT商店挖掘与分析]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdc9ScXU1UzibP3xuBfwicbVFiceClt52lISib7OLbxj682Drzwx53A4WGZpKoQEnfibpickAP45iaNEpZ2xg/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：科研就像一场冒险，而看论文就是你的探险工具！只有通过深入阅读，才能找到宝藏，发现那些意想不到的科研奇遇哦！1. GPT商店挖掘与分析  标题：GPT Store Mining and Analy</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485784&amp;idx=1&amp;sn=a071406b205f53f292808b7db724f150&amp;chksm=c1bf99515c34fe8c432e9bdd07889243b58ac8db6022a27b918487dd91337948f3019cb39ca1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 17 May 2024 14:32:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ALPINE: 揭示自回归学习在语言模型中的规划能力 | 小型LLMs也可以通过领域特定的自回归训练方法实现专家级性能！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdecbqC1q0culibe72QOFZVVL3hYW8lA20NovUud4lvfyT5DY6vbT2sum71yP4SRj2kPBiatGtwJ6qIQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：平淡无奇的一天又来了，今天要分享的内容主要是关于大语言模型、LLMs、Transformer的，喜欢的小伙伴赶紧去阅读相关论文吧。1. ALPINE: 揭示自回归学习在语言模型中的规划能力  标</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485780&amp;idx=1&amp;sn=1672dcd38b16a5128446215d29daaace&amp;chksm=c18a61ea84988fd2297a6e21eace69c15f1ca03a9a57a3e7b224f25f1fd42a2b775a51948928&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 May 2024 13:05:10 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯发布Hunyuan-DiT中文理解能力超强 | 基于Phi-3的更好的评估模型 | “思考令牌”用于语言建模....]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdf7JJ1d3bPojlQvHjRHKJTjpDOgGFVujHzSN0fQpMfIQhlXMI9vnPTlefgb7BM1bfl9iaTKX5JG0TA/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：科研就像一场冒险，而看论文就是你的探险工具！只有通过深入阅读，才能找到宝藏，发现那些意想不到的科研奇遇哦！1. Hunyuan-DiT：一种具有细粒度中文理解能力的强大多分辨率扩散Transfo</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485776&amp;idx=1&amp;sn=340faa316959ad9a0f0eec599f196191&amp;chksm=c1b79a3084bf947ce51e881cd2810b16c6750a210c5240b84247ac308f5fcbe1c054b96013ee&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 May 2024 11:31:21 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[RLHF Workflow: 从奖励建模到在线RLHF | 我们是否真的需要Mamba来进行视觉任务？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddLBiaqFIkshzC4icDreDd98eU13nhFHmzdme9V35KqeSEHgCcuZqOJF74w3Xy2OSUYvdibia96r5cY6g/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：平淡无奇的一天又来了，今天要分享的内容主要是关于大语言模型、多模态、预训练的，喜欢的小伙伴赶紧去阅读相关论文吧。1. RLHF Workflow: 从奖励建模到在线RLHF  标题：RLHF W</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485769&amp;idx=1&amp;sn=d19aec36e168e74d423c21a60025f7f8&amp;chksm=c1498a0e735b6d8e682eeb73ba48c94dd497728fb18a0dafba93416b5e7a019aee405f0b4271&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 May 2024 09:51:21 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GPT-4o 全能模型发布完整视频[译]]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBddLBiaqFIkshzC4icDreDd98eSTIicuzhpZBcXYa3pzricWmTlJPulPAajYUBs1nsgsibJZxRlh2UKFKNA/300?wxtype=jpeg&amp;wxfrom=0"/><p>OpenAI 刚刚发布了 GPT-4o，这是一种新的人工智能模式，集合了文本、图片、视频、语音的全能模型。能实时响应用户的需求，并通过语音来实时回答你，你可以随时打断它。还具有视觉能力，能识别物体并根</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485769&amp;idx=2&amp;sn=b0d5b22665d52cbae24991eb36de5fd1&amp;chksm=c129784aed810b04db4b91b9b0d13103a8e4fbbac3bf33244a91144039a1a9c26f1dd099f054&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 May 2024 09:51:21 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型的困惑度是否能反映其对长文本理解的能力？通过讨论框架和角色扮演增强大模型创造力....]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/0XibHbUBQBdcAUsbibCcibicN46DzH0B2x2mr4FE5ibuusRmfOyecmErx2nVWZDRMzA7DZNA0ciabYjj3SrwOGmxz6ibQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>前言：科研就像一场冒险，而看论文就是你的探险工具！只有通过深入阅读，才能找到宝藏，发现那些意想不到的科研奇遇哦！1. 大模型的困惑度是否能反映其对长文本理解的能力？  标题：Can Perplexit</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5OTkwMDY4Mw==&amp;mid=2247485757&amp;idx=1&amp;sn=418d355d39df274effc3d1ce5794d347&amp;chksm=c161cf717e941a781734f5e4f8c7a5725d42fae52a39cef6d4e403e45cc8924eda46952d08cc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 13 May 2024 09:45:58 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
