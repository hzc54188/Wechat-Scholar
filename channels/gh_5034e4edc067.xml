<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AINLP]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AINLP公众号]]></description>
    <language>zh-cn</language>
    <item>
      <title><![CDATA[寒门要不要读博？看看群友的回答]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8lAiaFOkIB3B9cOAG8Sf0fsJicatL8lDfvQ4ibcvL15I14hOia26Q6j4qhHqu3dd0nmv6Gg9CcPQEbVA/640?wxtype=jpeg&amp;wxfrom=0"/><p>我是一个在读的分子生物学硕士研究生，家里是农村，比较穷，我是应该出来工作呢？还是在学术的道路上继续不顾一切地奋斗呢？回答一 作者：韩愈字退之
链接：https://www.zhihu.com/ques</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=1&amp;sn=928b5d2ff343ac9734eac4b17528ace0&amp;chksm=bf1b5dac378fa1b46da535757e2ec47cf83ff8a91764669bfbf1f673b6d2905ee93e9b138d04&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[赠书 | 信息检索与深度学习]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLsIGh6w9pRgzmqj6kIThhjl2APLn7BrbLUQyeYvjs1NXGo2iaflfJ0qJFHeNztoQ9reibWMw7yibvgg/300?wxtype=jpeg&amp;wxfrom=0"/><p>在这个信息爆炸的时代，获取、理解和应用信息变得愈发重要。也正是在这个背景下，郭嘉丰、兰艳艳和程学旗三位作者共同打造了《信息检索与深度学习》这本开创性之作，不同于大厚本的臃肿，这本书以其紧凑的结构，将信</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=2&amp;sn=a86499dbe2cca2efe5687c2a2b2a7a52&amp;chksm=bf0722756b6a626f4030493244e4bab52ce304b24e14d2d216cf34ab7cc47e8ad554f15702cb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[LLM之RAG理论 |  RAG综述论文详解]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SicnDlFVicbHEeK2IapcumNWCTHUhPjwl9ibWRClrKloxkkU6koxcKRz25fRjc5DicAjSt30buFSZKX6fg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文地址：https://arxiv.org/pdf/2312.10997.pdf       大型语言模型（LLMs）展示了强大的能力，但在实际应用中仍面临挑战，如幻觉现象、知识更新缓慢，以及在回答</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=3&amp;sn=9897ceacc8df5c6c437cbed33117ecb7&amp;chksm=bf4b58b5e361f8358f0a5f550dbeacfbb70327e3d06f8892ab2455d1c976ac1ba6917714dff6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[知识图谱融入预训练模型方案汇总（一）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/s7YKINJYHDACMSyM71KZfrbVqdibGickZRYa0KrC8eXH53nSxQCbbHsyTg9Zef3A3HfMetZ7vygoJQ0nuOsUPQEw/300?wxtype=jpeg&amp;wxfrom=0"/><p>知识图谱融入预训练模型主要有以下方案：改造模型输入的融合方式（Before-training enhancement）这种融合方式被定义为对进入预训练模型的输入进行改造，既包括在预处理阶段将知识图谱的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=4&amp;sn=5d585ba9f43e43ac6b6737addb3af5bf&amp;chksm=bfde799183b913ecc0b1f5e0740d3a0973d4db9a3910e2d84c21f07b3005735e16058f52be54&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[【LLM】FuseLLM：大模型融合trick-知识融合LLMs]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/kJguDvfjOGDq26flbEVJwgK200ND7Q0L1iaicukErIOxmzGkhpIba6BEQzMFoY4flckFxf0cKP1Jfgib2IfMxukxQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言 传统的模型融合方法分为集成的方法和权重合并的方法，这两种方法在以往的NLP的比赛中非常常见，是一种提分手段。然而，上述两种方法都需要预训练或者微调相应的模型。在大模型场景下，对每个源模型都进行初</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=5&amp;sn=fc9c836045d6e48f1daf97e2c9f548da&amp;chksm=bfd1ce02592aaef40a3de786bef13adf1d07375e9696aea06f2b009a6baec75fe8190bd475ee&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
    </item>
  </channel>
</rss>
