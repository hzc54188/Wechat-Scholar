<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AINLP]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AINLP公众号]]></description>
    

    <language>zh-cn</language>
    


























    <item>
      <title><![CDATA[AI 之王 GPT-6 猎户座 来了！大模型杀疯了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSL9USXuo7TZj4klLIpC70xibrqUjGNHGJ9xNsG15WEoRcnvOOrD4uQ3jaaEcxiaKGxqvMPkzWXeruAA/640?wxtype=jpeg&amp;wxfrom=0"/><p>OpenAI不装了,祭出大杀器！新的AI大模型 "草莓"（Strawberry）和"猎户座"（Orion）。值得一提的是，OpenAI还计划利用"草莓"生成的合成数据来开发最新的大型语言模型"猎户座"</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444462&amp;idx=1&amp;sn=be983211ea6d95d9b372ccfaf02771e2&amp;chksm=bf6077b55b9ce290c0f8352162508192aa1db49d808b7823375ab2708085826cc4f3851783ce&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 02 Sep 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[如何估算LLM推理和训练所需的GPU内存？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSL0QuKNxzYruAYPLOMohXq1dMiclTYicfroTiajrV9qDYrzRLqHgk0wibCFoicaicrHFeL5bWHz9mend4Bg/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：孙鹏飞，南京大学 · 计算机科学与技术原文：https://zhuanlan.zhihu.com/p/716317173在实际工作中，经常有人问，7B、14B或70B的模型需要多大的显存才能推理</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444462&amp;idx=2&amp;sn=5967d81909a8066a01c5fe3b04cbf545&amp;chksm=bf68ad35b401f2de4ab301189734a7c418969cbcdd11dc76735ebb750f39af34300585012c92&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 02 Sep 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[【超级对齐】CriticGPT: LLM Critics Help Catch LLM Bugs]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/d6902wUyyvJ6XPVaDJWZibt0y2cje3a4icicQjd17LdDoZFJlse28gAWU538gh35uAzR3KSfZkP0oHvoJNXbib27GQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>超级对齐类似ChatGPT这类大模型在构建过程中，在对齐阶段仍然需要大量的人工标注数据的参与，而最终模型的性能与标注数据的质量极其相关。当前gpt4在个别任务里面已经超越人类，大多数任务中的表现也很难</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444462&amp;idx=3&amp;sn=ce609a2bc9812784243c48612753ac41&amp;chksm=bf7a6e4a6646a0631d833a64e5bb3d5080106dc2bf4b4a1c98e4111004459cfbe7e4f451eed4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 02 Sep 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[KDD 2024 工业界搜广推工作整理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/TnZw73HawgkBgfCQbvrVZ8dKgNic34PYAzqib458icuzWknmzR9g69LPsuuatZKLVysGoAeB8LU4ickJeMNyOBhhyw/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，KDD 2024论文新鲜出炉，整理下工业界搜广推方面的工作。文末还整理了大模型相关的搜广推工作。排名不分先后，由于工作较多，存在遗漏的可能。AlibabaAlibaba工作涵盖面广泛：推荐上，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444462&amp;idx=4&amp;sn=ca976242655a5190ab34a5deaa8135de&amp;chksm=bfee96671b12338226f5c06ca66fd34277b186e6b6f9fdab07cca21d23a502c51edb4f05c1be&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 02 Sep 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Qwen2-VL：Qwen系列已在开源的路上一骑绝尘]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mISTgTBRotAFockGXIkpnyIJib3vPczAVQmVE6yBHYCzDJOvnCe9gJWfQVzwZ0XBcRAib8gVejKpEw/300?wxtype=jpeg&amp;wxfrom=0"/><p>良心Qwen，开源了Qwen2-VL的2B和7B，72B需要API调用暂未开源。该说不说Qwen系列模型真的是在开源路上一骑绝尘，全全全！vl、audio、text连续更新，kpi直接拉满！HF: h</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444462&amp;idx=5&amp;sn=e057c8abea98f13af2cb2326d1254cd8&amp;chksm=bf498fb6bcbcf4699948895adba007a9e595855b1567ec0cad300f672009b5f36fbbb71a5571&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 02 Sep 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[如何解读 Yann LeCun推文建议学生不要在大模型方向工作？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKX3fLUSrGjMUgIT8L8ZeWSrBhcHAPbWh2J0SPmJ7jiaEjYluK1wkFrlAhRAia4vEic6e82MNqQq4VCg/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者：摘星狐狸链接：https://www.zhihu.com/question/656903686/answer/3527956804Yann LeCun的建议说得很直白，LLM已经在大厂手里了，作</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444448&amp;idx=1&amp;sn=0b86be0b36082e11ce5dad936aaded18&amp;chksm=bfaf6f51c3457e3186db84fe79fbfcd92c156356493fbb1ddad84600a56aadefafa0fc11e3b9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 01 Sep 2024 14:16:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【赠书】AI for Science：人工智能如何重塑科学研究]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKX3fLUSrGjMUgIT8L8ZeWSECZzLwcmfr08kdhPH74pEwic026ztTIqNgUF6u1HAB3rREuWrRhaTqw/300?wxtype=jpeg&amp;wxfrom=0"/><p>--文末赠书--在古希腊神话中，工匠之神赫菲斯托斯曾打造出拥有人类意识与智能的黄金机器人，这可以被视为人工智能（AI）最早的思想起源之一。此后，人工智能的影子便无数次出现在人们对未来的幻想之中，但也仅</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444448&amp;idx=2&amp;sn=cfe7f2132c8a587ff34cad17fafdb94b&amp;chksm=bf9e24495e47fa5cf28a6c7fb86865ff7634b720631f812eacb9650a16057e0756ee6d71ce90&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 01 Sep 2024 14:16:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【LLM模型微调】LLMs-微调经验-LLaMA微调指南v7.0]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/svfB1Sp4FdDO4yUOGegp19peWnYgbVibF6LPfaibM4icSia1fOpM2KQ2MQiao92z2sRsHj6nYTITJfjrGCBNWh7pAlQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>【导读】：本文是LLM模型微调第七篇，分享Meta于20240807的开源三篇文章：Methods for adapting large language models，To fine-tune or</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444448&amp;idx=3&amp;sn=25e2b7456d733a2243df30fe5a40a22d&amp;chksm=bf2584472ea8653b3f0a5bfd3a97d96171b5f1b7af5172708c0c2f3585b622709532dba24f45&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 01 Sep 2024 14:16:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[多模态大模型: 盘点&amp;Highlights part1.5——从LLaVA-NeXT到LLaVA-OneVision]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/jT5Sp7MICPygPdaZeYpHjmrxT0eFO8NA7ibcuP6ehGBKptWAXVto2GzciaHDQShJkcAvqmzsj2dTic4I8mJvtykww/300?wxtype=jpeg&amp;wxfrom=0"/><p>Hi大家好，我叫延捷，是一名计算机视觉算法工程师，也是叉烧的老朋友了。我们计划发布一系列关于多模态大模型的文章，帮助大家快速、精准地了解多模态大模型的前世今生，并且深入各个多模态大模型领域优秀的工作，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444448&amp;idx=4&amp;sn=e94ddd090f058c74b66ccd0958ac444a&amp;chksm=bfa4b3a40b217aa9e19ff398c56cbf23b8b706237fef313d6b7d0aaaa7e684d57769947fbd5a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 01 Sep 2024 14:16:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[样本权重对深度学习可能没那么有用？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/QLDSy3Cx3YIfxXUuLHTibgcBCMTibfW2OaYk9reicHMhy5jmxlEABjZ6HkZTqgoPPsopTgqLicg0xOpAgrRWkK5nHA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在上一篇文章中：机器学习中的样本重要性权重 (Importance Weight)我们通过简单理论分析似乎说明，使用目标分布的先验构造的样本权重，可以使得学习出来的模型更加贴合目标分布。在“回归问题中</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444448&amp;idx=5&amp;sn=9eaf6e33aa50abd19abb0c0cce6eb0bd&amp;chksm=bf7c8e627cb9da613e34273e5c46f16213aaad48762167419fe609d442c33a37de6678b6353d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 01 Sep 2024 14:16:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型微调终极指南]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSI3KiaZyOrB7CBgPJOfvicKO3rVkyYLss0X8f82uiaxs4hmQXiaa5FKlHceqR839IBhFWKOR1EzSKnvMw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来一篇大模型微调相关的最新综述，主要大模型微调归纳为7个阶段分别为数据准备、模型初始化、训练环境配置、模型微调、模型评估与验证、模型部署以及模型监控与维护。Paper: https://a</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444428&amp;idx=1&amp;sn=c063aeb1c800622c85bf69a759633bbe&amp;chksm=bfcd8fa9f8b98c8f3fce64b111473add241a94bd8bfd73f4a0d47bc3350fd78854c24010d3c6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 14:45:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[校招生做大模型，选预训练还是SFT？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSIURkEnvhgvAZACdIicTK4TtdhVoxV0eNwFxodzKOiblVsHzxibx5mvibfEH3ht3cMFJl0d7XOk7yoqiaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：ybq链接：https://www.zhihu.com/question/635761315/answer/3608088928我推荐选 pretrain，理由如下：pretrain 提高工程能</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444428&amp;idx=2&amp;sn=4ff95d3b5f12e40b66489dc2a8631c98&amp;chksm=bf1814efe9072eb6d6e5bc25edfd11e04da052ca55b9c3e90ce4be5834a562220599db229793&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 14:45:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【LLM模型幻觉】LLMs-模型幻觉-大模型幻觉缓解技术综述-v2.0]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/svfB1Sp4FdCw4YiaggpSvfJiaYksAg6eE1jgTE2V4GyH5UjnXPIg8pj1xBV2tk2gIkB0XG8D2TfIBOmcF6NTfo8w/300?wxtype=jpeg&amp;wxfrom=0"/><p>【导读】：本文是LLM模型幻觉第二篇，分享大模型幻觉缓解技术论文综述：A Comprehensive Survey of Hallucination Mitigation Techniques in </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444428&amp;idx=3&amp;sn=aa9d7a9948363df436baec7c5d341259&amp;chksm=bf57fd1cbf0dcb760ed52547d8a90f0e90cfdc6c83cea6e8fda31d224bc38e9cff9acfa66349&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 14:45:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[prompt综述的解释和个人思考]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/jT5Sp7MICPwicoQDRqib3uEVs3f20oIQJia8uQTRbfs168aKszsyKAT5ibg8AIfOauqEMRGgPvdJXeZUwbIiaPtYCWA/300?wxtype=jpeg&amp;wxfrom=0"/><p>上周手上不太方便，即使后续好了也没有搞定（不过说实话，这篇文章的量似乎没读完也不好搞定）。最近是有3篇prompt的综述非常出名：The Prompt Report: A Systematic Sur</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444428&amp;idx=4&amp;sn=d2241a200e180232bfbbf30f00a07826&amp;chksm=bfb6ccdc6644f30ed081866ac14d93a79454d1405b44e836aaaccda3a6fc54977bf2ea58d17b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 14:45:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一次失败的实验 - 无限注意力，我们为什么坚持实验]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/5LJDib8HPR2rJLNiaUdNmEc0xuDqUIVHcY4fM8usrrw4KJRIl9xYrg2txZjObNQ56y2UtIibHt9BmrpibRuxWYJJuQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>总结: 随着我们增加内存压缩次数的次数，Infini-attention 的性能会变得越来越差。据我们所知，ring attention、YaRN和rope scaling这三种方法仍是将预训练模型拓</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444428&amp;idx=5&amp;sn=821202221df2322d0649f9917035ae98&amp;chksm=bff2c02089793f1ecb9712c51ae269d5007868f2f6e1136fd1b98aa34a1b712a89bff8612a40&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 14:45:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型的基本功]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSIURkEnvhgvAZACdIicTK4TtOUC093l7xjQrpru5GeT0uQ9298wUghH1jmZtPBgXCs5ffl4lyO0DoA/640?wxtype=jpeg&amp;wxfrom=0"/><p>Author: [ybq]Link: [https://zhuanlan.zhihu.com/p/716344766]这篇文章给大家推荐几个大模型的练手程序，也就是所谓的“基本功”。先问个问题，除了 </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444410&amp;idx=1&amp;sn=e805ae79fbbba4daf2d5fd066fa921f3&amp;chksm=bfee2ffff1a99fb6b774d7befbd6eee80f78dd5cb0ddb44abd2928a8bc872df3c7d375529a6a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 15:17:11 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[449页pdf！深入探讨大语言模型的世界：赵宇教授新书《自然语言处理：大模型理论与实践》]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSIURkEnvhgvAZACdIicTK4TtKNticcLMn7SGPcBFl6fc6pPUHfBe0TANSOeewiaOQkjTCnFFCjmX0Jhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>教材官网：https://nlp-book.swufenlp.group/PDF预览版下载链接:https://nlp-book.swufenlp.group/%e3%80%8a%e8%87%aa%e</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444410&amp;idx=2&amp;sn=285fd97ecd92170f6f7300718441963a&amp;chksm=bf9bc053f1bb630f8f1aa8fc0fc97caef37853e863eda09ae86f597e56d48c4a7eb506868f44&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 15:17:11 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【LLM模型微调】LLMs-PEFT[微调]-QLoRA总结笔记v6.0]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/svfB1Sp4FdAGjG6CTCktBa8hUfec1MISxXE7Yvktanp4iaCxgnnEBA0OBfxjop6WPvprmxm29mDJy9nbNAAbxzg/300?wxtype=jpeg&amp;wxfrom=0"/><p>【导读】：本文是LLM模型微调第六篇，分享论文QLoRA: Efficient Finetuning of Quantized LLMs的解读。主要内容有论文解读(提出背景、技术原理，细节补充...)</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444410&amp;idx=3&amp;sn=e35eaf27b0c684ade62a8db04bf5c270&amp;chksm=bf2ae3c8ffa953e805d02fbfc4963824345cb341fd487a7ad4538b36419d38b9af3d44120342&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 15:17:11 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Monte Carlo Tree Search介绍]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/d6902wUyyvLjYIwlnRicx0bEVVOnPZ7YGtNv5uwCSDKMibaFfOm0j7vSZUWicwYicvp2Qr5z4MY7Nibuics6rbouibc1w/300?wxtype=jpeg&amp;wxfrom=0"/><p>背景最近Andrej Karpathy在网上对RLHF的吐槽引起了很多人的精神共鸣，总结下来核心就是RLHF中很难定义清晰的奖励函数。RLHF训练的目标是构建一个有用的LLM助理，意味着要处理各种通用</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444410&amp;idx=4&amp;sn=c13e630930d94ced5a1be5a8877c7626&amp;chksm=bf1926c8d4cbcb800bb1125c4813083eeae56da44b963a2352dff6139c59e33a49cc3eaa0d4f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 15:17:11 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[使用重排序（Re-Ranking）来改善LLM RAG检索]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/xicQMWhcBia0I8lib9Qicic5FuYTe9b5UicgtcTgyqPHnILn6eQdPYJ4Oaibm3aC1omXmcL3cF7NofG02QiavmYG5sqicNA/300?wxtype=jpeg&amp;wxfrom=0"/><p>基于大型语言模型（LLM）的聊天机器人可以通过检索增强生成（RAG）技术来获取外部知识来增强大语言模型的能力。这些外部知识可以减少错误答案（幻觉）并且还可以让模型获取到训练数据中没有包含的信息。在RA</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444410&amp;idx=5&amp;sn=150fd9e638808a158bd6c0750b8c59eb&amp;chksm=bf9d0aa6e2c74a799bac44ce0b1ab6b2406936394859105ccab44203f391cd29460ae9b1aa23&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 15:17:11 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LSTM+Transformer王炸创新，荣登Nature]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSIjsADKYbj6sxmhrwk8CcR1WepTGV9ajCQV2Ppc11UwZtq44HaW7TVLegJgJMSUuhYUNsmricWTZMA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今年LSTM火了！LSTM原作者分别提出xLSTM和Vision-LSTM，解决了以往的局限性。同时，LSTM+Transformer登上Nature；LSTM+CNN、LSTM+Attention等</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444387&amp;idx=1&amp;sn=e97c0e309bcd9e3ba37c92af968d69e7&amp;chksm=bf9e521b1ea1def5f31588abf4350331e1e50c21cc7db3fde6499ec826419bbf255b4ee41969&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[这段时间搞大模型的血和泪]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5m7xaFAUayiclBI8EoHkTF0qotaA06wic167BrLHtuYG63cFgYefd4qu9czkq1Ric56LwtRUQDKZKS8g/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家分享一篇好友知乎@赵俊博 Jake在这段时间搞大模型的心路历程。作者：@赵俊博 Jake知乎：https://zhuanlan.zhihu.com/p/716420396李沐大神最近分享了很</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444387&amp;idx=2&amp;sn=eef3dc84d54b4e9452e7e38f25e06f3b&amp;chksm=bfbe19ec8b95b8a41e7c45410bf703c9172cd9edd6a06a89c1d5579ac50d1c5f5c06849ffd2d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型知识蒸馏的两种方式]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/vImdeGStOOeLic4Xg0fgSyjt4lgyymYHWD2RKMBKuOJxoVPucFTGsKJh2VKITMcSBmiby8rHJWtxAh9gGibo57XQA/300?wxtype=jpeg&amp;wxfrom=0"/><p>      上个月llama3.1的405B已经发布，除了感叹开源模型效果的厉害之外，另一个普遍的感受就是，跑不动，根本跑不动，没资源，就算能训练，也部署不起。所以很多人就自然而然关注到了知识蒸馏，通</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444387&amp;idx=3&amp;sn=6181d42138f21d4f6920edda1393655a&amp;chksm=bf615aa5ad6380d4e8dc61757dd5e2b7d4b8b00456606f260ccb191f09cb23b4941b12b7ead7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ggml 简介]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/5LJDib8HPR2ouTCuWXvqUCnn5PxnBpBJQ4JN2AouvrhDlkg2bHYic5unPZ3ea1cTWCZTicGIOfOOj4yIpz5mfN9Fw/300?wxtype=jpeg&amp;wxfrom=0"/><p>ggml是一个用 C 和 C++ 编写、专注于 Transformer 架构模型推理的机器学习库。该项目完全开源，处于活跃的开发阶段，开发社区也在不断壮大。ggml 和 PyTorch、TensorF</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444387&amp;idx=4&amp;sn=6471e0cf2a63ca8b352413fbeba5c099&amp;chksm=bf6cb4e8753de60eccf51dfe8852ed27d494c0f94cfec3fbb748622719b3c8850dae3cd169d6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【微调数据过滤】One-Shot Learning as Instruction Data Prospector]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/d6902wUyyvI8BOEvEicIpu4uTIo4M82fG20U2ucZbdmMkic2PwOkVhwAq47UhFxyTGtk0UnBrKhQibMWO2dXg8xQg/300?wxtype=jpeg&amp;wxfrom=0"/><p>这篇论文介绍了一个挑选数据的trick，这么说好像论文很简单，但确实非常简单。虽然论文里还用公式阐述了提出算法的实施流程，但我感觉完全是形式大于内容。既然是trick，那我们就尽量用简单的语言描述清楚</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650444387&amp;idx=5&amp;sn=af4efa5e332a3a87db0ffd600bed1f55&amp;chksm=bf4c685111284b2a8020655ad1c40082dafbafa1ab66f049f5b1522e344cf9ee6bdb3cb77b0c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 02:10:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
