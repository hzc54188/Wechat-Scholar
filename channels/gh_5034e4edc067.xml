<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AINLP]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AINLP公众号]]></description>
    

    <language>zh-cn</language>
    






    <item>
      <title><![CDATA[《Kaggle 2023年鉴》下载]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/5mUr2jjo0gr2ouDiab97DbkdOk1OTAOG651opu7zsGPcPIwOfH6gLIrsMiapqpKd3AL556aFCRBMG2SAaxteicCag/640?wxtype=jpeg&amp;wxfrom=0"/><p>2023年，Kaggle共举办65场比赛，总奖金300万美金，共吸引了全球8万支队伍参加。如今，Kaggle是全球最顶级的权威性数据科学竞赛平台，以及全球最大的数据科学家社区。2024年，无论你需要求</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440763&amp;idx=1&amp;sn=3027e867344d6972ad3cb009aa3abc90&amp;chksm=bf1f1ca15725e122c8b5775107b698902c8c8c523cb2452700a37f17883133e577c656a46dd7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 29 Jan 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hq9ANWCLRic2h1VziajMFZQhoddCeNxl7YVf17jMHHQZPTwwbqHDDaRFSTcBWyXyciaP3J7sV9Ckufof0vkjseBSQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推更新记录0731 更新ChatGLM2-32K长度外推示例引言开篇引用一下苏神博客的介绍，对于Transformer模型来说，位置编码的加入是</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440763&amp;idx=2&amp;sn=f65851c835c27e42c817586063f51924&amp;chksm=bf47a08af82be564ab4cc2623ccde500d161b3b79e42295e4aae558b437b4a049f04745b41cc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 29 Jan 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[一张图系列 - “kv_cache”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/2KUCQBHkydmGyz5O5Hq8GibOreUcdfUNw91U3nQqO0zRHnicicxHfNPULdE9XibFZMFB4pv2sKrU9ibArKic3SDr1mCQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>【问题】Transformer推理性能优化技术很重要的一个就是KV cache，能否通俗分析，可以结合代码? 【知识点】1、multi-head-attention是如何计算的？attention的数</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440763&amp;idx=3&amp;sn=27119fa2c85522a03fbdfd1f2da403ff&amp;chksm=bf2189b205c69c59f4555aca8372e435ec4089954368206f7d6a5ce76574e7f2c8033f0d5561&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 29 Jan 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[LLM | Gemini语言能力深度观察]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SicmXv6ghyJmnCNPibPYassT8ycZFEiafEzPel3SqeohyrcqH5n9mib8nBuzWmXxI1RNiayqhVXAGVmxFVQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文地址：https://simg.baai.ac.cn/paperfile/fc2138ce-cadb-4a36-b9f7-c4000dea3369.pdf      谷歌最近发布的Gemini系列</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440763&amp;idx=4&amp;sn=b6255e8c8aca62018c661ae269c79205&amp;chksm=bfa804066cc909f981ec684e6c820846ababff4aa20b6eac3c6e51deaf4bb5fa49d70a736283&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 29 Jan 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[大模型检索增强生成误区总结]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/vtIvcrPJjh6k3APclKsQj93sMKxHibIpnTbrGXnhhGrAxNcICQPzMFd8twePoTpD9pia4ev55AI2eSqmX4TlICrg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：Seven Failure Points When Engineering a Retrieval Augmented Generation System摘要：随着大型语言模型（LLMs）如</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440763&amp;idx=5&amp;sn=d3d5eed22c22ed45f24e2a89260546fb&amp;chksm=bfd8dcd24edff7c725bb8b64203fc5d42d7f137445af0b9993140e03984db6ce2458c7cc8b5f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 29 Jan 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[寒门要不要读博？看看群友的回答]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8lAiaFOkIB3B9cOAG8Sf0fsJicatL8lDfvQ4ibcvL15I14hOia26Q6j4qhHqu3dd0nmv6Gg9CcPQEbVA/640?wxtype=jpeg&amp;wxfrom=0"/><p>我是一个在读的分子生物学硕士研究生，家里是农村，比较穷，我是应该出来工作呢？还是在学术的道路上继续不顾一切地奋斗呢？回答一 作者：韩愈字退之
链接：https://www.zhihu.com/ques</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=1&amp;sn=928b5d2ff343ac9734eac4b17528ace0&amp;chksm=bf1b5dac378fa1b46da535757e2ec47cf83ff8a91764669bfbf1f673b6d2905ee93e9b138d04&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[赠书 | 信息检索与深度学习]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLsIGh6w9pRgzmqj6kIThhjl2APLn7BrbLUQyeYvjs1NXGo2iaflfJ0qJFHeNztoQ9reibWMw7yibvgg/300?wxtype=jpeg&amp;wxfrom=0"/><p>在这个信息爆炸的时代，获取、理解和应用信息变得愈发重要。也正是在这个背景下，郭嘉丰、兰艳艳和程学旗三位作者共同打造了《信息检索与深度学习》这本开创性之作，不同于大厚本的臃肿，这本书以其紧凑的结构，将信</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=2&amp;sn=a86499dbe2cca2efe5687c2a2b2a7a52&amp;chksm=bf0722756b6a626f4030493244e4bab52ce304b24e14d2d216cf34ab7cc47e8ad554f15702cb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM之RAG理论 |  RAG综述论文详解]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SicnDlFVicbHEeK2IapcumNWCTHUhPjwl9ibWRClrKloxkkU6koxcKRz25fRjc5DicAjSt30buFSZKX6fg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文地址：https://arxiv.org/pdf/2312.10997.pdf       大型语言模型（LLMs）展示了强大的能力，但在实际应用中仍面临挑战，如幻觉现象、知识更新缓慢，以及在回答</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=3&amp;sn=9897ceacc8df5c6c437cbed33117ecb7&amp;chksm=bf4b58b5e361f8358f0a5f550dbeacfbb70327e3d06f8892ab2455d1c976ac1ba6917714dff6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[知识图谱融入预训练模型方案汇总（一）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/s7YKINJYHDACMSyM71KZfrbVqdibGickZRYa0KrC8eXH53nSxQCbbHsyTg9Zef3A3HfMetZ7vygoJQ0nuOsUPQEw/300?wxtype=jpeg&amp;wxfrom=0"/><p>知识图谱融入预训练模型主要有以下方案：改造模型输入的融合方式（Before-training enhancement）这种融合方式被定义为对进入预训练模型的输入进行改造，既包括在预处理阶段将知识图谱的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=4&amp;sn=5d585ba9f43e43ac6b6737addb3af5bf&amp;chksm=bfde799183b913ecc0b1f5e0740d3a0973d4db9a3910e2d84c21f07b3005735e16058f52be54&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【LLM】FuseLLM：大模型融合trick-知识融合LLMs]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/kJguDvfjOGDq26flbEVJwgK200ND7Q0L1iaicukErIOxmzGkhpIba6BEQzMFoY4flckFxf0cKP1Jfgib2IfMxukxQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言 传统的模型融合方法分为集成的方法和权重合并的方法，这两种方法在以往的NLP的比赛中非常常见，是一种提分手段。然而，上述两种方法都需要预训练或者微调相应的模型。在大模型场景下，对每个源模型都进行初</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650440753&amp;idx=5&amp;sn=fc9c836045d6e48f1daf97e2c9f548da&amp;chksm=bfd1ce02592aaef40a3de786bef13adf1d07375e9696aea06f2b009a6baec75fe8190bd475ee&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 28 Jan 2024 12:56:50 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
