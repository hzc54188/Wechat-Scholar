<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AINLP]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AINLP公众号]]></description>
    

    <language>zh-cn</language>
    































    <item>
      <title><![CDATA[Gemini大考终于赢了GPT-4o！Jeff Dean连续转发三次！Video-MME首个视频多模态基准来了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLKeMfIzI8ARAQ9lUXlnWvQuO09jcFT5uusor08k2fiaAk4D8Hd7kD6MkwhOMicINbdibuIKVRYT6Xnw/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，中科大、厦大、港中文等高校联合推出多模态大模型视频分析综合评估基准Video-MME，全面评估多模态大模型的综合视频理解能力，填补了这一领域的空白。Gemini 1.5 Pro在这份榜单中遥遥领</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443106&amp;idx=1&amp;sn=95768902fa4d2b71a0cbf5b701607dec&amp;chksm=bf806b9a3f82efe52a5c0512cfb1989efb4dade6f6f9f2e24e175ad7519f8a1983d47be0f370&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 19 Jun 2024 10:21:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[聊一聊大模型应用落地那些事]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLKeMfIzI8ARAQ9lUXlnWvQVrE3V04uMlchTKNgxPoyeDrjJPic9Yne3ZODK3OmBibR2JyUq1cS6REQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：混沌福王，专注于 web 前端技术和通用软件架构、代码整洁、研发效能、复杂性系统 。主页：http://www.imwangfu.com声明：本文只做分享，版权归原作者原文：https://zh</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443106&amp;idx=2&amp;sn=c9483e5d1085eba2bc1dbc3979e2dfb9&amp;chksm=bf9e577335849008e7982a3225a9b0c17292373e3a3a1c389e67eea56dd002d390e9314b50cd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 19 Jun 2024 10:21:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[LLM的重复生成和ICL]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW464hE3O3kjib0ecqro2vKP6djQvgPrxODEEl9CeAfoOqFPlysItBzvlZuAkZE1iagWxjvMomX4kjvzicw/300?wxtype=jpeg&amp;wxfrom=0"/><p>LLM的重复生成问题，俗称复读机问题。对于这个问题的研究很多都和in-context learning相关。1.背景在目前这个时间点，其实已经很少会遇到字符级别的重复生成问题。自ChatGPT发布以来</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443106&amp;idx=3&amp;sn=c712434587e7952de7a6e044a4591b1a&amp;chksm=bfeb77ee2fb1f39992049a2227586f9557b4af2d9a32ff17f35a53fe963f3d1f9b7f0c486988&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 19 Jun 2024 10:21:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[将强化学习重新引入 RLHF]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/5LJDib8HPR2qS3reQqUpolwqaCk04rzkHCgw3RdVl09Hn0PU1WqYmCfNm7kaseJI0RZz9YA7vG06XAAa4ztYn4g/300?wxtype=jpeg&amp;wxfrom=0"/><p>我们很高兴在 TRL 中介绍 RLOO (REINFORCE Leave One-Out) 训练器。作为一种替代 PPO 的方法，RLOO 是一种新的在线 RLHF 训练算法，旨在使其更易于访问和实施</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443106&amp;idx=4&amp;sn=40dae13ec54af9e24fce455fb0d83075&amp;chksm=bfbaea40b31cc49e7eb1bfb7708c5e8dc51bb14a67b55a90d7b2fc5d3028a4fa156014bcc09e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 19 Jun 2024 10:21:50 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[【京东】数据/爬虫实习生]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLKeMfIzI8ARAQ9lUXlnWvQ48Co5bDbtEPJ7PdLzw7Qia0Aj50Vh4ia6OWibCnqlfbkGoRXqTQqjvmVg/300?wxtype=jpeg&amp;wxfrom=0"/><p>【京东】数据/爬虫实习生（北京）【岗位描述】1、负责爬虫工具的设计、开发与优化2、负责爬虫策略算法的更新维护、确保高效、持续的数据爬取能力3、负责文本、视频等模态数据的预处理等工作【岗位要求】1、有爬</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443106&amp;idx=5&amp;sn=cec0b0493efe951670dbf26550fef255&amp;chksm=bf75d66bbb0d7c4f2d6443a41b29551f7afa672c4aebf4c7bfaaab561c816a7f65085c505cee&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 19 Jun 2024 10:21:50 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[LSTM又火了：xLSTM追平LLM新SOTA，LSTM-transformer混合架构荣登Nature]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSIPTfm5Kdc8h7ygWj0ZLNqy38EbWibgzy1c7fq5jrw74NOEAGBfFpKCiaH1KicxzsbGicTP16hu5bgPWQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今年上半年，LSTM火了！LSTM原作者分别提出xLSTM和Vision-LSTM，解决了以往的局限性。同时，LSTM+Transformer登上Nature；LSTM+CNN、LSTM+Attent</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443078&amp;idx=1&amp;sn=5e881cc7d7a10da4381e324dcc8bd7e0&amp;chksm=bf26b3ddc657ed5a8e882b8eee8238d363d6c0f95ef2eed18908b75d9cb12fcea39de30d599a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 18 Jun 2024 02:09:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[模型调优（RLHF/DPO/ORPO）终极指南]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJzORgQ0rkdiaLsSlgRWPGhRWoHy8t50gcmwW1wS9MJDzy3nzvaOWv2DIaMl4IEic0dssEIRSUuQEgA/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：FelixCoder，致力于分享我的编程经验原文：https://zhuanlan.zhihu.com/p/692594519前言虽然大规模的无监督语言模型（LMs）学习广泛的世界知识和一些推理</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443078&amp;idx=2&amp;sn=305eefdb4c20fa0b67d69be9045fb0a4&amp;chksm=bf651c5205aebc00e68242321d330cb5eecee21bdb633ff6d9f1e3d3322b9e9a9f76ece48cd7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 18 Jun 2024 02:09:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从loss视角理解大模型涌现能力]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW464zkN6dJr2zNWDOFNrfa8CPD4OYPStVKibFJ84K3KlmkwNvDz0RAo8xP9gkANzWmfeBm0hVuzQySiaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>智谱在《Understanding Emergent Abilities of Language Models from the Loss Perspective》中提出一个观察大模型涌现能力的视角 </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443078&amp;idx=3&amp;sn=721ae6d376b556a9e31f2a4e78609841&amp;chksm=bf2ab2931bcc9118a716f5335f539f4515623ab09cce550ceff22f0c437f81eebf8ecc3722a5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 18 Jun 2024 02:09:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Text2SQL之Vanna优化]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/h4lbevcvkgyhb8WbReHb5MddHjdURWXUOzGTg96HeAfAbsIbP8khHXQib3BZ0wsDBAeunAFeE0VdpD86CvfhHZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言前阵子写了篇Text2SQL的简单介绍，发现其也是RAG只会，写下了Text2SQL之不装了，我也是RAG最近也一直在做Text2SQL的优化，于是把自己的一些心得，总结于这篇文章。一、优化方向既</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443078&amp;idx=4&amp;sn=8a6bc2291a26573f943d245bf6553378&amp;chksm=bf7235d2ee77f92716b16ace04d79c54b7ad10aa1cc99d8dd103dc314a1098466f3b26878253&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 18 Jun 2024 02:09:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【LLM】PISSA：一种高效的微调方法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/kJguDvfjOGDyPfGLncJOtKrkjA4A2vBTCiawyicrKoRdWb5wOJaOVKZF5V62R2UicBMDDMRja294CtE9qnKqfNA9w/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言介绍PISSA前，先简单过一下LLMs微调经常采用的LoRA（Low-Rank Adaptation）微调的方法，LoRA 假设权重更新的过程中有一个较低的本征秩，对于预训练的权重参数矩阵，( 为</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443078&amp;idx=5&amp;sn=f35550d1a83eb40bd25b3438b7dd75ba&amp;chksm=bfdcea89eb66c3558399cdea10b4f1546056a588cbb2f07878b16c3f9ab7ace72bb02932c0d6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 18 Jun 2024 02:09:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[真心建议大家冲冲这个新兴领域，应届生年薪炒到65W＋]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLrGVInuf8VoGGibMVRoAZdZPPp8Fics0qib1N7WJyDUjDQngMgjRnoqmicohlwOWLnzy98PcOQj8TVIQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着ChatGPT的火热出圈，AI大模型在全球掀起一轮开发浪潮！Google、百度、腾讯，阿里等各个科技公 司，都在高薪挖掘AI大模型人才！不少企业甚至开出百万年薪挖掘大模型人才！！不夸张地说，未来A</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443059&amp;idx=1&amp;sn=a79b82d2a38627268c1668399e281699&amp;chksm=bf57e1674c8945fb7f38a3803f499ec8597ee33b58703773dbb9c67dd28e5cefd6adf2313fcb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 17 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLaMA Factory 实战——单卡 3 小时训练专属大模型 Agent]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLrGVInuf8VoGGibMVRoAZdZSiaGggobmJru7x7CO16YqJZdzGLRk8cE7fLQmtfm6leL2qZuCzg4Xpg/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：hiyouga，NLPer原文：https://zhuanlan.zhihu.com/p/678989191引言Agent（智能体） 是当今 LLM（大模型）应用的热门话题[1]，通过任务分解（</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443059&amp;idx=2&amp;sn=80d2a893f03b96dd5f66e7192f5d2584&amp;chksm=bf11c144b796fe7713698ab1c76139a1b8586a185288c872a2a50c9d7dfea507ee8b36f76a22&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 17 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Index-1.9B: 小巧精炼的B站大模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/1FD1x61uYVe9P3cc5EJrhDsDxdBsIHEKVkwSLYpEnGgJ017lEDxB2p8w4CnE67IfibSkOVCr092SVUdWXHmOjQQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言今天，B站大模型团队发布了Index-1.9B系列模型，是Index大语言模型系列中的19亿参数规模的模型，如下所示Index-1.9B base : 基座模型，具有 19亿 非词嵌入参数量，在2</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443059&amp;idx=3&amp;sn=49995054631e7410767bfdbd4c00b0e3&amp;chksm=bf1bcc59c8d41e8fedbd17f768271d1eb9a8a9b2b14cf5355791dd233a0df54ae2b44523b0b2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 17 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型推理加速-MEDUSA]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW464ZcXgG5rb5zXSN3WQ98H5t4EIuaC4GVCxAibZaRFQLbtLA9dW7h2kVsfrdcmzbZy5iaISibGpRrqepg/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前对speculative decoding的做法做了介绍：大模型推理加速-投机解码。本篇介绍一下另外一个热门的解码加速算法，MEDUSA。MEDUSA在不同的训练方法下能提供×2.2~×2.8的解</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443059&amp;idx=4&amp;sn=a045c2fe820e1adb828cfd6dfaca27d4&amp;chksm=bfb3bfe4e24ab90452447dfafa598496f96d4ac1a89e2b35896c94c3e134b8e427833d1ea54a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 17 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【文档智能 &amp; RAG】RAG增强之路-智能文档解析关键技术难点及PDF解析工具PDFlux]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/kJguDvfjOGANq7aTicjwcbibyBibLAvY0gzictbShvYr6iaT7pnaEuoHR9M1BPrIQRAVUQhQEfxZmFyOpXLAPm6Rnrw/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言在私域知识问答和企业知识工程领域，结合Retrieval-Augmented Generation（RAG）模型和大型语言模型（LLM）已成为主流方法。然而，企业中存在着大量的PDF文件，PDF解</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443059&amp;idx=5&amp;sn=74d87faf129bbc55e9515bda1afba981&amp;chksm=bf3ecf1c3e73d80e6926694f3e48c6d0491fa6ebad04c2c6263bce28f52e69106dd66fa2dc75&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 17 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM后端推理引擎性能大比拼]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLrGVInuf8VoGGibMVRoAZdZsI0eykDTVe2jWX2dyqyO7EibrXa51jaX5QyI4WTqsk18ZzZX6pyR2lQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>原文：BentoML 工程团队翻译：OpenMMLab原文链接：https://www.bentoml.com/blog/benchmarking-llm-inference-backends选择适宜</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443040&amp;idx=1&amp;sn=5a751e7ce8c6a19e0764bba08f9b928d&amp;chksm=bf9716bce79c54f66ac14306929dfbb1eff913871fd70ac66370c07ad9eedf75cbc7f150f56a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 16 Jun 2024 03:56:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【文末赠书】大模型时代，如何用时间序列与机器学习解锁未来？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLrGVInuf8VoGGibMVRoAZdZEhTcSonsur30UXpH4l1picd0hhqFlacGUoH3aOZanaqWUsNJuDyicCBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>--文末赠书-在人工智能领域，大语言模型（Large Language Models，LLM）特指那些具有大量参数、需要巨大计算资源来训练和运行的深度学习模型。近年来，随着计算能力的提升和数据可获取性</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443040&amp;idx=2&amp;sn=21799e3f725075c6ef4d59eeb0d72370&amp;chksm=bffd309999943ee29b95ed40b84cd5f0c5f8d4edfcacff3534a3fb8299ab32635ff6598062d0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 16 Jun 2024 03:56:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Ollama 本地CPU部署开源大模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/4WgILHBwVH90R6CriaBywevOnmQQ6ibN9nWNwibzbee2syNbtR1Hx2Eq2RZS0dy8UOIt9MJ3NOPLPiaib1bOLsUwLcA/300?wxtype=jpeg&amp;wxfrom=0"/><p>Ollama可以在本地CPU非常方便地部署许多开源的大模型。如 Facebook的llama3, 谷歌的gemma, 微软的phi3，阿里的qwen2 等模型。完整支持的模型列表可以参考：https:</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443040&amp;idx=3&amp;sn=b5baf2504c21de81d178bc8356f76bcc&amp;chksm=bf4e39dd839049004e6bdcf1ac5f1c74e40be40f414f0ac9aa0e759b27baa7bdf7d8ddbe0b6f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 16 Jun 2024 03:56:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[数据合成方法-让模型自己说出用了哪些指令对齐数据]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nCvMu19JKoJtyJyApt7SKSnnZoPicb4SN4PKfjf95l4VVBC9301A2UMME2FiaialNeYgA3TBeIsSjKg/300?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面大模型时代，数据至上，如何利用大模型合成更多高质量数据也备受关注。今天给大家分享一个有意思的大模型合成数据方法-MAGPIE，在不需要种子数据和额外人工干预的情况下，挖掘出对齐过的模型自身的指</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443040&amp;idx=4&amp;sn=6b66de9d047aa418cd92c9003f755101&amp;chksm=bf6a72d8d81d5257514258474402649527da9df14e390661f9805f52b71280ad54ab5649990b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 16 Jun 2024 03:56:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【文档智能 &amp; RAG】RAG增强之路：增强PDF解析并结构化技术路线方案及思路]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/kJguDvfjOGASKcfJtp9G00sykMjuXXExSOpjZgibV3nZA4WCaUibGI3tHUPdT9HVibyMxXCrlaPOejUcdOticjOWIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言 现阶段，尽管大模型在生成式问答上取得了很大的成功，但由于大部分的数据都是私有数据，大模型的训练及微调成本非常高，RAG的方式逐渐成为落地应用的一种重要的选择方式。然而，如何准确的对文档进行划分c</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443040&amp;idx=5&amp;sn=8fd04b517df755f7cd7cdaca87200e7d&amp;chksm=bf912d130016a22fb696749ac94ae3586a14c9e6c89b6db4dd73ceb236fbbb26fd394177d264&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 16 Jun 2024 03:56:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[RLHF替代方案：在SFT以外，我们还能拿SFT数据做什么？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJDLuhKtUOCQeyhumxAOKuo2sV9CpOLcIsjhGJVnXd7UI7SGxwQ9QdzdiaqrE7u8GlOUGxRBc0Eo6w/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者：孙浩，PKU-MMLab-Cambridge｜RLBeliever主页：https://holarissun.github.io/原文：https://zhuanlan.zhihu.com/p/</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443014&amp;idx=1&amp;sn=3a6ec6fb12c5c9ed6aa9bab4d7a0f63a&amp;chksm=bf4fad24311763dd81cdf6db188899e99efbb5b094b3b0ffdd746017dc8bf9592ebf59a0909f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 14 Jun 2024 10:35:55 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[McEval：超大规模多语言代码评测]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSIcfjbKe9GXoAcKGiakyLPswgRsmJoUZibtDO8uvtmpk49rq1MYic1ks33eCUdzgxlxm5zJOkQUx4ruw/300?wxtype=jpeg&amp;wxfrom=0"/><p>为了更加全面的探究大语言模型的代码能力，该工作提出了一个涵盖40种编程语言的大规模多语言多任务代码评测基准（McEval），包含了16000个测试样本。评测结果表明开源模型与GPT-4相比，在多语言的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443014&amp;idx=2&amp;sn=f3b031ce59719ff21c22b10997488a31&amp;chksm=bf21fde75a791ce5ae58a047339fcb8ed55d1a329f262d9a3ae7b75a510a8f5a9bfde4aff6c1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 14 Jun 2024 10:35:55 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[夏令营 | 关于北京语言大学信息科学学院暨语言智能研究院举办2024年全国优秀大学生夏令营的通知]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/LbbS6DhRQVVOVtxxuQLibvIiaJ6aYz2YABlibiavClxHt3eWutFOKqia74Dul1Ekw5CABxVg0GJbZQJc1G9zmmFVaZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>语言是人类交流最重要的手段，是人类文明演化的基石。语言智能学科的任务是赋予机器以感知、认知人类语言的能力，并进行类人智能的计算和推理。北京语言大学信息科学学院是国内较早开展语言信息处理研究的单位之一，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443014&amp;idx=3&amp;sn=30ebb0f2cbb4b959d2616835cdef4c01&amp;chksm=bf0739830ab57f1c789bc09a89ad3f2ca1633e5d03b677698071697c64198feab305775ec65b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 14 Jun 2024 10:35:55 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[聊一聊搜推广粗排思考]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/DHibuUfpZvQe1Tz5kqsMdiaP2lfGhbEAf3fl4rjsJn6ugtPRQV3YYHaDzoA7k3pUFaTGT1Q4klYAPIumZs5kv6xg/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者 | 阿狗 https://zhuanlan.zhihu.com/p/699188815粗排模块的目标五花八门的说法越来越多，但是粗排的意义本质还是由于在工业界中业务链路性能、算力的约束下，漏斗链</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443014&amp;idx=4&amp;sn=a434b257ea7c925288f80bd65ed4ca6c&amp;chksm=bf27085031b948549272f42529f43a085ed238d23e05db5414a7b99e354b6a98b4a1cd29dab7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 14 Jun 2024 10:35:55 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[KDD2024-WhoIsWho-Top3开源方案]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/1FD1x61uYVe9P3cc5EJrhDsDxdBsIHEKSJBxT3E6HjGOambZSgTO45ScUeHNQ2QLxljdk4Hk2DRR2eRhibewPkg/300?wxtype=jpeg&amp;wxfrom=0"/><p>KDD2024-WhoIsWho-Top3KDD2024-WhoIsWho-Top3开源地址：https://github.com/yanqiangmiffy/KDD2024-WhoIsWho-Top</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650443014&amp;idx=5&amp;sn=135a5d22d8ef9b372c397606bef26b93&amp;chksm=bf450930cb91cf9d33ab14055e347e5e039e34ea9b8fa180bee2d7c69fd50081290b217f7e0c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 14 Jun 2024 10:35:55 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[手把手拆解：从零实现Llama3大模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKN0SBrQB4pxsRib3btwklaKgGVFljzJrG6cPcCUxD5TtdXutqwwhDt4Ue1P9NCUyXmqibVHGm1ia8bA/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近出现了一系列令人激动的开源大语言模型，如meta的LLaMA、清华的ChatGLM等。伴随大模型一起爆火的，还有大模型的微调方法。然而随着模型规模和任务数量的增加，对整个Transformer模型</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650442999&amp;idx=1&amp;sn=63e28d4b62be807e2f853eed575dfff9&amp;chksm=bfe490b607d689cebcd963ec3eb3c9695418f913a8080beac575856848e2d328c6d9ff82fec1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 13 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adam学习率Scaling law的「浪涌现象」]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJDLuhKtUOCQeyhumxAOKuoGkLnDibGafFyGPPNDSrCe3XN4n0ew5o96Bws7oOBnjtuGzSd5LUGkvA/300?wxtype=jpeg&amp;wxfrom=0"/><p>Batch size放大后，对应放大学习率是一个约定俗成的规律，但随着现在模型尺寸、计算量的增大、不同优化器的研发，这个结论是否仍然成立仍有待探究。近期腾讯的一篇工作就得到了不一样的结论。下面有请一作</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650442999&amp;idx=2&amp;sn=f8fcabeeeb046aefbc71108bb309f775&amp;chksm=bfaf1b296665ce3a6718742366210377397025eea9ae592bbeece69d99cdc4bdff7a2e49d91d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 13 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【米哈游-北京】NLP算法实习生]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJDLuhKtUOCQeyhumxAOKuoic8zicJ6OafJia9fQCG8tSSr56PSgTpb3jGA2PKgvZAc2B1JbibaIrSjzA/300?wxtype=jpeg&amp;wxfrom=0"/><p>【米哈游-北京】NLP算法实习生【岗位职责】1. 探索NLP前沿领域研究，包括但不限于数据建设、模型训练、智能体、对话系统等工作。2. 负责优化和提升模型效果，持续提高算法的效率和性能。3. 参与NL</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650442999&amp;idx=3&amp;sn=f4a1fddcded6b7242c94490355e62abc&amp;chksm=bffcf82c7528f0946e9a22dba17b284608da0b50448f2a85b94dd75e97dd377e06f0196230ca&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 13 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型算法题(7)]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW465VXIeBot7KEGppOP9sSfPRNuSI9cZRFaSianExMgfArKRSWrvm0Py3GBO9Yua3FZiaCiaxV3VZPlZFQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错漏，欢迎指正~1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？MoE使用多个并行的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650442999&amp;idx=4&amp;sn=227b1ef053227385e7b9659486201b86&amp;chksm=bfbfb4b58519c04bf5cacbc6a220f3abd9a23bd085c7ae598a8e51ac3cb119c5df09d4115cf9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 13 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[JiuZhang3.0：通过训练小型数据合成模型高效提升大模型数学推理能力]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/G7ia3FZ0o0Oo8ZT4CdYTHzE2za2GyGZfUdic5e9TLQxgnkwKdJ7xQBOn8iaZfoHkcX7yURicnEeAM6UvfOPtAF2LZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>© 作者｜张北辰机构｜中国人民大学研究方向｜自然语言处理、大语言模型大语言模型解决复杂数学推理问题是一个广受关注且具有挑战性的主题。已有工作通常通过从现有语料中收集大规模数学相关数据进行预训练，或者是</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650442999&amp;idx=5&amp;sn=b2846d4bdedc8e25932f766fba16b039&amp;chksm=bfb9053d95456f115028fc02f189cd35fd1476d3834f925a67e35fb157515323309e73de6255&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 13 Jun 2024 02:10:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
