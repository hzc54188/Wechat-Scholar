<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AINLP]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AINLP公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5034e4edc067.jpg</url>
      

      <title>gh_5034e4edc067</title>
      

    </image>
    























    <item>
      <title><![CDATA[开源！Transformers 快速入门书]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJS7qhNsT6qCjBDA3SLCPvo3Y01NCoaRibxfrolxY7iaE3IF3KN348iar6s22rIslRN1xyOAC6ZSyymQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用。从 BERT 和 GPT 模型取得重大成功之后， Transfor</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446091&amp;idx=1&amp;sn=2dd56ff9aa7905caafcd302f5c1c3cff&amp;chksm=bfae135963be150fbed0ef4ede6475c7c62322d73df8b846e642e964a225980e8e8681bd24b0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 25 Nov 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[万字长文，大模型分布式训练的学习过程总结]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJqdSN91vD15iaLpicVy6KqUjavGeoPr6mVNaIhmwEicEZgdRAAF6249bIwKgz5Sce4dPM2QlrK9ZZCg/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者丨elihe@知乎 注解：仅用于学术分享来源丨https://zhuanlan.zhihu.com/p/688873027编辑丨极市平台导航Stack for AI专栏：https://www.z</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446091&amp;idx=2&amp;sn=9abad58c54d3d905a6f0b942221587a3&amp;chksm=bfc7ccb8a39cccad7de75cefab4847b87f01fc7cceb4f7936fb445cd18030e72b2d1aadcdc44&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 25 Nov 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[阿里Marco-o1推理大模型技术报告解读]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/IictSfTIpvuxpB9taq8yyz4SDBc3F3H6ND3ibdspKxQf5NfgUia6KZRB9jHib2EliayVcE1cl7VKMdCzDsdGoia4ib2RA/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：青云遮夜雨链接：https://zhuanlan.zhihu.com/p/8752961062前言OpenAI 最近推出了开创性的 o1 模型，以其卓越的推理能力而闻名。该模型在 AIME 和 </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446091&amp;idx=3&amp;sn=d37a88b32adf7f3ff61f13869a46eed9&amp;chksm=bf8310fb30f780069a39dd47fca1778877b23076c5c5db3d8424f61633f3dc615d578db4a746&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 25 Nov 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[投机解码中高质量draft tokens不该被拒绝]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaJ1z6xvIXpgHC2eibAwFeRVpVDdcvia74Yiagupia6H70B2KPtocCTnMqDh5HibM7UY6VL2FYhqickOVIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>💡ICLR25的8/8/10/5的高分工作，确实值得读一下！论文：Judge Decoding: Faster Speculative Sampling Requires Going Beyond M</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446091&amp;idx=4&amp;sn=ffa565cf5525e593d5bb7488823daeef&amp;chksm=bfc1789b92f1cf96918f12aabc230446631283df60bc9010c7e906e7712f93c63dbe164a1805&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 25 Nov 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[凑个热闹，测试一波DeepSeek新上的o1推理模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kDibADpzOCECibyFLcIUOeqX0yV7utugBSxJ3fbBKXbgp1ZeLPE2A2wuQagM6RHvyM93Zek16WFUibw/300?wxtype=jpeg&amp;wxfrom=0"/><p>自从openai的o1出来之后，各大厂都在默默发力，几个月了，国内也开始有o1模型了。前两天，有kimi的 k0-math模型视频露出，昨天deepseek就来个大的，直接发布线上可测试的r1模型（每</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446091&amp;idx=5&amp;sn=6a1990cef88be025934213cc607b820c&amp;chksm=bf9624669c2bd44334674f8ae51698686079641984d8540de861381652625b01c76b85b37e31&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 25 Nov 2024 02:10:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[特朗普上台，第一刀再次扎在了留学生身上。。。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJS7qhNsT6qCjBDA3SLCPvov1d2nQGiaBrDkxYrbSJp3NibD1StCg2HGKNWKNw8bicewQyz3xlaEVWVw/640?wxtype=jpeg&amp;wxfrom=0"/><p>还没正式上任，经验丰富的特朗普已经开始布局了。业内人士分析：结合大选后的一系列动作，“准总统”特朗普的系列动作可能已经箭在弦上。关于“大幅调整美国移民政策”的前路，也已经基本清晰，大概率要有大改革。美</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446077&amp;idx=1&amp;sn=d50b13aa52ebef2ee01f8741161ee603&amp;chksm=bf652920e726d60cf4be7ec27673df1eae2e6984982ae44037777833a609e2f53b1d3a3178b6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 23 Nov 2024 13:40:54 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[探索 OpenAI O1 模型复现：从 Kimi K0-Math 到 DeepSeek R1 Lite]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagYMeVbsIFQQ9miatD9Gic6xTm9QhyGESWYBJemg1F8kiaEqPP8oul8sJjicxMuD5BZBc6PCZuic607wpA/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：初七123334（已授权）地址：https://zhuanlan.zhihu.com/p/8102196012最近，随着Kimi K0-Math和DeepSeek R1 Lite模型的发布，Op</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446077&amp;idx=2&amp;sn=d0832754ad2f5c6fb01617f19a6f1657&amp;chksm=bf74e77e39f9c5ef21b0e3c0da2692916bd297a9e46af352abbb0b3cfcdf592d0b13492ddca2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 23 Nov 2024 13:40:54 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里云大语言模型算法研究实习生(base杭州，可直通秋招转正)]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJS7qhNsT6qCjBDA3SLCPvoRE9icK7SGIegtGkc5sILMkwwhwkRY590j3PbvfdzvHUrCveMic2iaKaWw/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里云大语言模型算法研究实习生(base杭州，可直通秋招转正)我们是阿里云人工智能平台（PAI）深度学习团队，专注于人工智能算法及框架开发，我们团队在深度学习各大方向都有很多的研究和算法创新工作，在顶</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446077&amp;idx=3&amp;sn=1089a3cd2f14f4eff9e8988fc28b0190&amp;chksm=bfb7c735b58685a7289d23b3dc874bc55e54a99798b3b1eeed34872ffb79e00f4b933a23210b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 23 Nov 2024 13:40:54 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM实践系列-详聊OpenRLHF中的各种Loss]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mZ5ljdNGNXWYMRuYDvGFibggAmG8libojRt9O4FSVdknRAqhuA4uiaiauKvUCFlDFAtpZcyvtclzibMrw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家来带好友知乎@ybq关于OpenRLHF的学习笔记，主要介绍其中的各种loss内容。作者：ybq 知乎：https://zhuanlan.zhihu.com/p/6290579087从这篇文</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446077&amp;idx=4&amp;sn=2329567c9fbb4ae055b2a4e6358dd2b9&amp;chksm=bfc792d2efbeaeeca0706db90f410aeb88a9d9dc689bbabb6a1656affb34c64447da0b75b59a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 23 Nov 2024 13:40:54 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ICLR 2025 多语言大模型相关论文速览]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/G7ia3FZ0o0OoHeZC4KTMkZrxzd9xYA6ibQfdqhL8CEYYlOBuFpKIuMcdPPLI4eW1rxWfXs67phKaApQ1bDExbXLQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>© 作者｜罗文扬机构｜中国人民大学研究方向｜大语言模型Dataset and benchmarkMMTEB: Massive Multilingual Text Embedding Benchmark</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446077&amp;idx=5&amp;sn=73ce5b06de4eb267a67322b53db798ff&amp;chksm=bf7d49c0366468451b0b51335914f28e9318a70f8cb9fc24f47ae16ec57b5430ba449cb853f7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 23 Nov 2024 13:40:54 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[深挖：埃隆·马斯克与OpenAI 分手内幕]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJgOicGfbopwGWdkGSwib5K8Lv4iaVdPw8Zzs9W7w9MEia0QdE2XFYwSG7PkhVYZBLoaN4tZtXw4EWwIw/640?wxtype=jpeg&amp;wxfrom=0"/><p>上午在微博上看到阑夕大佬的这篇解读，很有意思，推荐一下，感兴趣的朋友也可以直接看The Verge的英文原文：Inside Elon Musk’s messy breakup with OpenAIh</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446056&amp;idx=1&amp;sn=efc6f5ba84a48d3e025fc52bb49a41ba&amp;chksm=bfd99ad71200ca647ce30aa0c022857cd10d4f42f350a76d40f2d53fc6c44b4c951b3cd10c0b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 21 Nov 2024 14:57:44 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[王者归来！白皮书《从头训练大模型最佳实践》开源了。。。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLicqqqr1gllU6vbdichMgZyoia42QoMmWbldIU2pH06v2TvgZ6fuEMC5mumNfdHTOgOsaOqcIeTc9jQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>介绍《Current Best Practices for Training LLMs from Scratch》是由Weights &amp; Biases（W&amp;B）提供的一份关于从头开始训练大型语言模型（</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446044&amp;idx=1&amp;sn=502b991734e03e9b6708c9945dad5777&amp;chksm=bf3bbe608cc5fbe0a370d6a357ac10eec20402f49c5f427b7b2c0c7ab98398574a771e1609cb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Nov 2024 02:03:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[FlashAttention 系列技术详解：加速大模型训练的利器]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/G7ia3FZ0o0Or0PE0VQ6SgHe8HKGOvUiaVmMCBuFSOV4uPRez7iaSSSmyRobvQiaOeEPMCcNOeEYspGgMBIM1THoG2Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>© 作者｜陈杰机构｜中国人民大学研究方向｜自然语言处理、大语言模型本文将深入介绍 FlashAttention 系列技术的核心原理、算法优化及其在大模型训练中的实际应用。通过减少内存开销和优化 GPU</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446044&amp;idx=2&amp;sn=15e295dc6389cc9cd2ca3c22623ed868&amp;chksm=bfa3e7ed08448c9fc3971203bd1e4a78b3fc89602c72572d94eb9f500dfb75be7da949e92628&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Nov 2024 02:03:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[深度求索DeepSeek 系统方向核心研发岗位招募]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSInibbGYAXASFBGttPRAPUEvt2Diawypicwk8PYvubiaJmdSrla4e8KzcqAE7W1IBzBtChcwHkE7gxqIA/300?wxtype=jpeg&amp;wxfrom=0"/><p>工作描述：1. LLM 的成功是 AI 基础设施的成功，这是一个相当复杂的系统工程。每个成功的大模型背后，都有无数个从硬件到软件，从存储调度到算子编译器的极致优化故事，最终将这个模型的训练效率推动到了</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446044&amp;idx=3&amp;sn=61288acc441f99f314d8e7053f47271e&amp;chksm=bfc990648f2372f9f3731c7c261446dd77de91f1e2b51b7603f927ecfc60ca4cf7d436f839f0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Nov 2024 02:03:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[对话语音合成大模型GPT-Talker: Generative Expressive CSS]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/C0j7wmyZGH6RmDIQmInVNDGSlv8LgooTRicW4ficMsG9N0LGnbaqxHWFhnSpsCqiarCicicKsOQoRTYXXfM7ic2FxYdQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>本次分享由内蒙古大学刘瑞研究员与字节跳动青年科学家任意、香港中文大学(深圳)李海洲教授合作发表于ACMMM2024的对话语音合成大模型工作GPT-Talker《Generative Expressiv</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446044&amp;idx=4&amp;sn=b98f7e24e67e3246e5f1178fac6f16d5&amp;chksm=bf4744bdbb57db89bf453c11dcd6c5fdde64c2a9f762e4478601d98644c282b519f497fc5a1d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Nov 2024 02:03:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型也有侧脑？揭秘WISE如何带来终生学习新突破]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaHO0PdInkbhA7ppFsq7vqPkBazibwibnD5UCXNXiaicwk50icUTEKJOjhcqBx02icPRib3vaZ4HSaY97XQw/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者: bhn论文：https://arxiv.org/abs/2405.14768代码：https://github.com/zjunlp/EasyEdit人类的学习能力是独特而强大的。我们不仅能够</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446044&amp;idx=5&amp;sn=b6fa4428f5b03c627f1f3f353e4e8564&amp;chksm=bf04bec4b19d7d00a989e1f1572c2f465a3d79233d36d595acc10184c32cdb0333b02f2e3cae&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Nov 2024 02:03:22 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[打破多模态检索的瓶颈，OmniSearch实现智能动态规划！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLicqqqr1gllU6vbdichMgZyoWkcGodA6oTL2XLCxZxVBpYkHZFB7ywOs9DfibNSgW3Udv9n6ma1OCIw/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着多模态大语言模型（MLLM）的广泛应用，模型在理解复杂问题时经常会出现“幻觉”现象，即模型生成的内容与事实不符。多模态检索增强生成（mRAG）技术旨在通过外部知识库的检索来解决这一问题，但现有的m</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446032&amp;idx=1&amp;sn=aa5a26c598c9dffa9d80ce7e791ba09f&amp;chksm=bfe08ac77e81ac0847e76a573bb4bd6b3f1873b5403c966c3108a2c4c930deb733004c6b29c5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Nov 2024 15:07:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型中上分技巧大总结！！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLHYFxh9h81c9I2fPxHjfia0kCibwVqTfXJicytW0lQBFFfXYocM8aj5lRcUSZib1h67vHGCEJQ3BRtlw/640?wxtype=jpeg&amp;wxfrom=0"/><p>这个文章与其说是上分技巧，不如说是刷分技巧~~~很多论文你看了看发现变动不大，但是就是效果变好了，可以对应着看看这个文章。我估计，都在下面的总结了。文章内容仁者见仁~~作者:  黄哲威 hzwer链接</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446018&amp;idx=1&amp;sn=e85f97dddb74dde6322aa0cf282454d1&amp;chksm=bfcc419547f737ae947003a64ba4a1ac2515ffe84bf372487072522387d25aab145c984ef23d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Nov 2024 07:06:30 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[最值得参加的LLM盛会！多模态/Agent/具身智能/安全/评估等15个论坛！早鸟注册最后一天]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLHYFxh9h81c9I2fPxHjfia0DwKBdjkwictic6BV140TYYtghpRyO7Mdzeesw9CkVpZ3VI76Hn4TMicSw/300?wxtype=jpeg&amp;wxfrom=0"/><p>会议简介中国中文信息学会（CIPS）是中国中文信息处理及其相关领域的学术团体，大模型与生成专业委员会（LMG）是中国中文信息学会旗下的专业委员会，全国大模型智能生成大会（LMG）是该专委会的旗舰学术会</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446018&amp;idx=2&amp;sn=63df715c8aee0f0e7aec57948b85a0c3&amp;chksm=bf2c2ed5a6d4bef6f3919313d53f8a891529db828c36a6205cb7d7c5ec6962deb7bdceae0b88&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Nov 2024 07:06:30 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2万字洞察Scaling Law的"终结"or"新起点"?——开源实践者的深度思考]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/yToxjhYT5ibb9Hy6qdvR9gcibt8ZeysozQw7iab7rJbPc9kicLGfdEAxSU5BVmW8nh37dRbm3FvTv0w1iafDIOVgycw/300?wxtype=jpeg&amp;wxfrom=0"/><p>| 作者：宋大宝，与大宝同学因那篇《回顾·总结·展望「融合RL与LLM思想，探寻世界模型以迈向AGI」》结识于今年春天，虽我们当时某些思想观念有些出入，也碰撞出了很多火花与共鸣，并持续地相互启发的走到</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446018&amp;idx=3&amp;sn=dfb727dd93399f4c8be1551b6af6a6e3&amp;chksm=bf4ae67bd5384706f7362c63af2d4097c6659de44b0ac6cdb828bc0b188d920aacd1476eff41&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Nov 2024 07:06:30 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[训练数据合成(二)]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW465zM1pOYlC9Gt4FCttlhz2ib1h6mraibl4s4fecscDavk1Vw97H2SfvibGLk6EUOBc9ysOicGBRjmATow/300?wxtype=jpeg&amp;wxfrom=0"/><p>书接上回，训练数据合成(一)，继续看一些重要的数据合成工作。1.self-instructself-instruct算是大模型数据合成的经典工作了，它的目的主要是为了低成本获得大量用于大模型微调的指令</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446018&amp;idx=4&amp;sn=afd4768ee01f5180b044b5dacef05ff4&amp;chksm=bf0eb610c92a9211775529c9484955772614b89bceaf527c2cb0e593a53b89455f59fbffed28&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Nov 2024 07:06:30 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[突破长度偏差:Meta AI的LIFT方法让大语言模型更懂分寸]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hq9ANWCLRic1IWYzkjQ4jYkYaMK6XP8IUzbribMPphMV0HITZfOu7lDE5CN53d4FQ8ME8Yf7yhsy5VePZQYX0gag/300?wxtype=jpeg&amp;wxfrom=0"/><p>引言在人工智能快速发展的今天,大型语言模型(LLM)的指令跟随能力已成为衡量其性能的重要指标。然而,一个长期困扰研究人员的问题是:这些模型似乎有一种偏好,倾向于生成冗长的回答。这种"长度偏差"不仅影响</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650446018&amp;idx=5&amp;sn=1e27b6e3559231c84a9d46f815019573&amp;chksm=bf8bbf9f43d4cb7fd85d54d4901bee1747d749ad39773a1672d48e463c1d6288be408219acbc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Nov 2024 07:06:30 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
