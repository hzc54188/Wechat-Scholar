<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AINLP]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AINLP公众号]]></description>
    

    <language>zh-cn</language>
    





















    <item>
      <title><![CDATA[transformer再突破！基于Vision Transformer的视觉语言新SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLEibeP8JRtlhLUOZzJS3mxibsKw4plQy65F7vlmLAzjewxdTliaqvVhypfvLPWP729ZCUTpY0qTXw8A/640?wxtype=jpeg&amp;wxfrom=0"/><p>Transformer 如今已经成为主流，为各种任务创造了 SOTA 结果，它是一种新型的神经网络架构，用于处理多种感知模态数据（如图像、文本、音频等），比如机器翻译和文本生成。我们邀请到哈工大博士李</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441398&amp;idx=1&amp;sn=4ceafbc5ce8a5d7e271d8d896727c2ea&amp;chksm=bf65e3d5afdd074fb3a9f6be8e1012199c5fa755bf74d269d6c3889f6dd4d66f5e3e90914a84&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 20 Mar 2024 03:00:51 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[增量预训练baichuan-13b-chat遇到的那些坑]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/h4lbevcvkgxpEa1tBhkuFdyk35sT8KiaxzKjZ1soQ0UfI0ZxsxNlAmcjyrYfHbicCvtBuUNTEUKIfmVlAlhw6vNQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言资源单机两4090，如图在这里插入图片描述单卡24G，baichuan-13b-chat单卡推理需要至少26G，因此仅用一张卡，我们是无法加载百川13B的模型，所以，无论是推理还是训练，我们都必须</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441398&amp;idx=2&amp;sn=2854422b50ac020dbb44f8a941ebe3ba&amp;chksm=bf7c21a1548ce8a190bbe5be9c98b933f0b4e34920bb8cd5f5c9d87118b500d4a255a9a22042&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 20 Mar 2024 03:00:51 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[大模型检索增强生成(RAG)高质量报告]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/vtIvcrPJjh6LwXNicqj4G2tibibibAYyiaOWcrJibYgBSMSicXVKxeYx38A1oXz0fwxicU5oNkXbgPSo2JJTkNuyNZOD5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天分享一个来自同济大学Haofen Wang的关于检索增强生成的报告：《Retrieval-Augmented Generation (RAG): Paradigms, Technologies, </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441398&amp;idx=3&amp;sn=ba391dc4e131e7f280178aeb6affc321&amp;chksm=bfd719ea1c1f67983ece345e7416a4d11e6778f6893cce7f8f9f9cd7792fd05591fb97ec2ee1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 20 Mar 2024 03:00:51 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[大模型的模型融合方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/IictSfTIpvuwIlZeeWqxu6BzsNJhXFfLzict73Nx1QnDzNnqtpEQbkrpkLaEHgt6f4o588IkBIUg8ocgLhgyRhDQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天来讲一下大模型中的模型融合，并给出大模型融合的有效方法的原理和实现。模型融合大家以前用的很多，特别是在判别模型里，属于永远都能稳定提升的那一类方法。但是生成语言模型，因为解码的过程存在，并不像判别</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441398&amp;idx=4&amp;sn=6646e0c7ff7b008b7f3179ebf18bb6cc&amp;chksm=bf4053a549a21dadf8128c97f9f31075db8d4367ca3bad903f4e280e9bce8578fe9ec6dd3a18&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 20 Mar 2024 03:00:51 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[COLING2024 | 面向编程的自然语言处理综述]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/58FUuNaBUjp1tGAcib25ibSH9GUR5NUA2Hx7y12dQvGT0MxTR4J5Z4N6dWt2k3iaeicGLsm7GQ1KlUNAX34xicxd6VQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名称：A Survey on Natural Language Processing for Programming论文作者：朱庆福，罗先镇，刘芳，高翠芸，车万翔原创作者：罗先镇，朱庆福论文链接：</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441398&amp;idx=5&amp;sn=392976ffa369c49ce863857488ed3bfb&amp;chksm=bf8181048755687774729f63a263a9ad0d82df84de4a4e67f1e81f48990ab63b0d5c3a60c2d2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 20 Mar 2024 03:00:51 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[保姆级 Transformer 中文教程，来了！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSJGdU9mv2W7e08dYWG5EicNWZicD4KU9KsJlA0jwUzrGlpOYO52SCCicskTd4myCZQsOeVg6w3YSw58A/640?wxtype=jpeg&amp;wxfrom=0"/><p>短短五年，Transformer就几乎颠覆了整个自然语言处理领域的研究范式，也促进了计算机视觉、计算生物学等领域的研究进展。这次我邀请了多位顶会大咖，做了22节最全Transformer系列课程，带你</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441384&amp;idx=1&amp;sn=fd1f9b0f8412a1c6dd6df3c54421c681&amp;chksm=bf1de2df6864da35bf37a5f5b297b41aac9201b5d6536605c60e1ea46884d0ebd0c805ee87a3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[探秘Grok-1 - 马斯克旗下xAI开源的大模型，参数量3140亿]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hq9ANWCLRic3obUlcwce1zeoq9y9krpghzp7vMZ9fhEYusDdibzYNhAbkuFSWNvkmwn4MV5thloTrjr5QvluHSdg/300?wxtype=jpeg&amp;wxfrom=0"/><p>引言Grok-1是由马斯克旗下的人 工智能初创公司 xAl 开发的一款大型语言模型，是一个混合专家 (MoE）模型，拥有3140 亿参数，使其成为目前参数量最大的开源大语言模型。Grok-1 的开发和</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441384&amp;idx=2&amp;sn=042e7328f8fc560eb8facf67066d6380&amp;chksm=bf005e33675020ce146259a35413994478511cbb382ab646d603c6aa24ed229eccc4a1b35ed6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[字节跳动商业信任与安全团队招文本大模型算法工程师]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSLEibeP8JRtlhLUOZzJS3mxibW3YVJKeWjbukibd2XtW9TKwHrKDjc77JeVfQbna01zAmCibErZ43pfibw/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节跳动商业信任与安全团队招人啦！团队氛围好，业务空间大，欢迎来勾搭[社会社会]（北京/上海）文本大模型算法工程师-商业信任与安全职位描述商业信任与安全算法团队，聚焦于通过人工智能技术（包括但不限于N</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441384&amp;idx=3&amp;sn=e12e43918942be306a7984e979831c80&amp;chksm=bf718067c21b4080c63dbf240b6921630638e5812663a0d4f367ee4b8bdd6bbbf71c5ff90420&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[指令微调数据的高效筛选方法-CaR（排序&amp;聚类）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mNvynY0vLaBXRw2X7AKLDENlGrl7t5RAdkzicFoEvoUuvqtTRrUhKSbw1iaUmfbDF3yxpfE9DPmygQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面大模型时代，指令微调是必不可少的技能。那么如何使用更少的数据，调出更好地效果，更节省训练资源&amp;成本呢？之前已经给大家分享过几篇数据筛选的方法：DEITA、MoDS和IFD。今天给大家带来一篇通</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441384&amp;idx=4&amp;sn=322e77823e37297479627c60efa96308&amp;chksm=bf85d6c7fc40459108fc248ab8629981bb0a5cd790b6dae2bcc934368b285729f983d105a3f5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[EMNLP 2023 | LLM工业界快速落地之PromptMix: 一种有效的混合数据增强策略将LLM能力迁移到小模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gTSf9kr5zrPeJ4Tw3AWavyQLk8FOay0mMpexs1NHyTicJ4f5DWeVaPb04C2GOcl9CARpnXiaNHAmkGGUDE0FtfBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天分享一篇接受到EMNLP 2023的文章，Title: PromptMix: A Class Boundary Augmentation Method for Large Language Mod</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441384&amp;idx=5&amp;sn=2e2760c9a5cb688a34101df8ab37bf00&amp;chksm=bff1fe2c7cac0461555d882b8ffdde30301db5c2f151e3b4708d5874608944b4e69385802288&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一个薪资和前景都不错的方向，建议都冲一下！！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSIIhPggiazR0jAUXkENfVAM7tS6dMYC9TzErBTHzPLcEXaiayabD1ibW1c4UWo0pGQXl6mgYcsGKl70Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>“大模型狂热”从未停止国内巨头战队华为、百度、阿里在AIGC的厮杀中从通用大模型渗透到各垂类应用市场就连中国创投资本也独宠AIGC企业百度、科大讯飞市值分别增加27亿和45亿美元这导致AI人才缺口大、</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441351&amp;idx=1&amp;sn=eb14aa2cb0f95b0e529d02ca27a3fb95&amp;chksm=bf377b73bf711965c1fa33b37568f84f64e3a83b5f52c08d7082c9b7b9f1a1a24ba978c66ea4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 17 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一文读懂RAG的来源、发展和前沿]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDOU2fvfbCgEUPghd6Vicj7qtFqy5RMLjUbic573G2icz8PfapSlHz0yIpA/300?wxtype=jpeg&amp;wxfrom=0"/><p>检索增强生成(Retrieval Augmented Generation，RAG)结合了检索 (Retrieval) 和生成 (Generation) 两个过程，旨在提高机器生成文本的相关性、准确性</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441351&amp;idx=2&amp;sn=fc14ae863480688463708276b67d8b93&amp;chksm=bfbc09fef65b796de3d6393dae76e11cc031e4664745e0256b4c3e22122e57009838c307d805&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 17 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[稀疏注意力计算:sliding window attention]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW466ooF3KOJ8RhRovvK1TC4QLrlngNDzohlT3Hm5tjufbvuQicFerKkop7TpGqPNEQn6axpcvhdtyiavQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>LLM的长文本能力现在已经是各个大模型巨头的必争之地。我们之前在《LLM长上下文的问题》简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在《理解Attention:从起源到M</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441351&amp;idx=3&amp;sn=c0cdb5df048d1dffa579baed4a39c96b&amp;chksm=bf8cae070ff4da7eac10651ff641f95758298a1c46b384346c77dae395004f11adb911b6727d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 17 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[DEITA：融合复杂度、质量、多样性的高效数据筛选]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hq9ANWCLRic0k2jhPhh6VOicFk7w68aR7ILSYfMNdSZcr0FMOkHay901xvxtXfcmZkXCHa1VVRL0s7osTcHdNDBg/300?wxtype=jpeg&amp;wxfrom=0"/><p>引言在复刻ChatGPT语言模型系列-（三）指令学习微调 一文中，我们介绍了指令微调的起源、重要性、主要目的，同时介绍了如何获取多样性的数据、如何选择高质量数据和如何组合高质量的数据。本文将再介绍一个</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441351&amp;idx=4&amp;sn=217e49d56dec4a8baafbbbfe919c4535&amp;chksm=bfb6e8f0922d98a1623e7141775e1a349257c3737eb1f1fbcecb1cedf6a49158fb44a5c54c95&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 17 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【LLM之上下文窗口扩展】| 无需微调的自扩展大模型上下文窗口]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/N5aX12H1SicnEDzXmW1QvqEjr21MoOwnneIGNHCwTsibXJezuTeK70kSNWB4su7zCMxIROa0vfzpIMuO3zk8Ksww/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文链接：https://simg.baai.ac.cn/paperfile/a34ae7f4-f0ce-4f8f-b8f2-e8e4d84bbee5.pdf       目前大模型基本都采用tran</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441351&amp;idx=5&amp;sn=551b82a5bb35a3db1c9f390aa1713dfb&amp;chksm=bfa9862de8d26cc61c67f21abdda7c6408d82d7360f4ac6d7e3ad3f69aaa431bda82e3a7214c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 17 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[薪资真猛！小红书24届春招官宣，大模型、算法、开发全都有！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKmkYpddjDlYm3A8yKEIqTmJR0oRn8I6GSM6m378TroE82ic5l3twty3x09jE4E1aM5IPXZorJueuA/640?wxtype=jpeg&amp;wxfrom=0"/><p>📢 小红书24届春招官宣！大模型、算法、开发岗全都有建议秋招拿没拿offer的同学都去冲一波试试毕竟小红书这几年发展特别快给校招生的待遇也是直冲Top1梯队了在校招生的投入和培养上真的是下足了功夫 建</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441304&amp;idx=1&amp;sn=8e280a8f16141ef8abd2722858f748ef&amp;chksm=bf4f885f5d9f84bc8ba300ada48edbf0ee0e72e7fc6419c27703cf9d5c15b7e37d9ca25e0f79&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 14 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[角色扮演大模型的碎碎念]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kol8X0EIZzT2IDicKJyfSuODatkgR353h16EsKblYpwgXyicJxbGgAeebp0BEc3tCXOJb3jqzYbRMw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来一篇角色大模型相关思考的文章，来自知乎@快乐子涵酱（已授权）：https://zhuanlan.zhihu.com/p/685823865什么是角色扮演大模型？首先提两个问题，目前大模型</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441304&amp;idx=2&amp;sn=43ccc35ff29bd1a9f756e94a60a1f605&amp;chksm=bfb71d3c748d8a35b6ed186f53eaa2c880e67a668e898084fd1265195b0eb304a122d46028eb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 14 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2张卡训练70B的大模型（上） - 百亿大模型部署系列]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/uibOQg13DptzLvy1HujeEhYCPGqzlHhe9JsZgGib66QxqN14XFSdwuw33pab2kTjvI9ibgQcIDK3d6rrwBibrAhOKw/300?wxtype=jpeg&amp;wxfrom=0"/><p>0x00 前言自从《在家训练70B的大模型》 发布后，很多人都在私信问我是不是写错了，不是70B而是7B，最近因为一直都很忙，所以并没有完整的时间测试。通过这几天断断续续的跑起来的结果来看，应该是可以</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441304&amp;idx=3&amp;sn=052efdf07c71305988733da3b6cc4e61&amp;chksm=bf484f8b628980d25333fcf1c90f2ef5e52bd7dd9099c24494de9239db5c91735171ec8830d4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 14 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM之Agent再探]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/h4lbevcvkgxTCibHzPoD5KJjPwOoadn4SziajVSVHVwLVc8xn6r7TsMrAvBJu4hyBHriaN4PfhyeH0nuW6dbTGskw/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言重要： 阅读该文章之前，一定要先阅读：LLM之Agent初探1、当业务中有大量的tool时，比如有上千个，这些tool的描述加起来，总长度已经大大超过了LLM的最大输入长度，即使能接受这么长的to</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441304&amp;idx=4&amp;sn=d2eaa2a9fbfb08ec6f2ac8ffd0012c4a&amp;chksm=bfe5b4ee14484570aae09a6a5b56fb4cdb811cf2922d0af76a88185a308efe4eea36be23353e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 14 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[关于Sora、国内大模型及通用人工智能趋势]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/1LfVzoWAAFddBY8KgqemyFxZjRGKutNPlyjnMfBYfEvicjsUhDdCQGSyXHJa8lgXiclA8Qne2F8XXTvuTJqptBLg/300?wxtype=png&amp;wxfrom=0"/><p>TJUNLP作者 | 熊德意  编辑 | 黄宇霏【导读】OpenAI于2024年2月15日（美国当地时间）正式对外发布Sora文生视频大模型，这是继ChatGPT之后的又一次重大技术里程碑事件。天津大</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441304&amp;idx=5&amp;sn=31d6fdfeff4ce9e18de0abe2be238bb1&amp;chksm=bff0e2944c9fd6750d1afb6e94cf60555c6b11a970c932396b38beca1f436340253ee5f8a5a5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 14 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一文讲清LLM大模型x知识图谱最新SOTA方案]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSL4VuvWZs0KTbSGmqnSLaXAcnVCVnfjHX3XOuQLBazTqpLNpI8syE6gCXqUxR4ZXadOJheCahVJIA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型 (LLMs)，正在自然语言处理和人工智能领域掀起新的浪潮。然而，LLMs经常因为其幻觉问题，以及缺乏可解释性而受到批评。首先，它们在专业垂直领域的知识方面仍然不足。其次，生成大模型容易产</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=1&amp;sn=850bdf735de06eda073c15ae558555a8&amp;chksm=bff7f0b40db0bb0c4279083cde2d3d50e56a46912fd8def9b8aa840a20eb70de69cc9c9a69c9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[理解Attention:从起源到MHA,MQA和GQA]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW467C3t0CCgtKNWCDpCejE8aB0LWo53Piazn1clniaiaauhtyM4ElLdqKyibKicFxDeF1Rc2mG5EwdPBKI4A/300?wxtype=jpeg&amp;wxfrom=0"/><p>Attention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head Attention）、MQA（Multi-Query Atte</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=2&amp;sn=d3cc0c76e0ee30a5bc1261b7266cd144&amp;chksm=bfc925eb413c98d3e1f642a67b831194c65a01af3fd5b5d2bc5f759c43b2411869a33420d84e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM之Agent初探]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/h4lbevcvkgyRsPAEfBC9kxPibdNC4jicXEhjJHQwmeu9HYJcT33onXBEh0jJKcpT3mBxVtcrLjTUM224wE4cwTDw/300?wxtype=jpeg&amp;wxfrom=0"/><p>Agent是什么？Agent一词起源于拉丁语中的Agere，意思是“to do”。在LLM语境下，Agent可以理解为在某种能自主理解、规划决策、执行复杂任务的智能体。Agent并非ChatGPT升级</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=3&amp;sn=63656a3a44a6e6cc0ecd4eb2a45171fa&amp;chksm=bf9cdcb2fa2575291c4cfb72c0baeef2274b68dae96b93bc9eafc10e6261668f5df20b9b408a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【北京实习】百度自然语言处理部｜ERNIE大模型实习]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKmkYpddjDlYm3A8yKEIqTmUPVfbNXBePHbMO9ib1epdWD0ic8UY0H2o1NNSGoAicmxiaic2GUY44krjGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>【北京实习】百度自然语言处理部｜ERNIE大模型实习团队介绍：百度文心（ERNIE）模型团队致力于预训练大模型基础技术的研究和应用，在预训练大模型领域具备深厚的技术积累。文心ERNIE自2019年诞生</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=4&amp;sn=a24841cf0fead0c37b0d38867c44fdd6&amp;chksm=bf6eceb37c46ed9e86695575809635a177e9010291a98be4c934f7c5276937439fb5a36d0140&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一张图系列 - "speculative decoding"]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/2KUCQBHkydl3fhJTJJSGxKByYqY1nVlibIvbbA1nUggZNaKI7kU9FWicH8mZ3UFaibmnPBVHMbNzElAEtGeeYQKiaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>目录：算法逻辑自我测试反思总结部分截图算法逻辑投机采样(Speculative Sampling)算法的整体流程和核心逻辑:1. 生成 draft tokens    - 使用一个较小的自回归模型(d</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=5&amp;sn=c90b7fc7d61d00370b6fdac498188102&amp;chksm=bf7f07c4dc5ce6d2ac2819dec251a5265ceb4e19c1a8a058396da003d6deb0c9fe48ca9d2dce&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
