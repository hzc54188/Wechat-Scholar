<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[NLP工作站]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[NLP工作站公众号]]></description>
    

    <language>zh-cn</language>
    










    <item>
      <title><![CDATA[大模型之战的新序幕-从Llama3之后]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nDLbOmJz6SQ1POVAdMVXHUhtjoPic9JseO77w29b918iaNicx7Xia4icETnVebnaWSU4YeDl36Gol495g/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文要点概览：文本数据的扩展可能已经达到了极限，因为易于获取的网络文本资源（如Common Crawl、GitHub、ArXiv等）已基本被充分利用。尽管如此，通过更深入地挖掘互联网资源、搜寻图书馆藏</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247489022&amp;idx=1&amp;sn=012b9a197745fc484e025a9508c2c702&amp;chksm=ce529b43469ebc1b353ee55b5f883a284a4809f87b26e584ae4b8f1ad4d281dabfff4803d69d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 25 Apr 2024 05:30:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[总结！大模型微调（Tuning）的常见方法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mib0iaJibNaUWDcv856Iddll8NHzibzPoOcIkbZNFicYYIicyIQBQKLus8iausbmRUVVFoEQEYjnSATiaEvA/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着大模型的飞速发展，在短短一年间就有了大幅度的技术迭代更新，从LoRA、QLoRA、AdaLoRa、ZeroQuant、Flash Attention、KTO、PPO、DPO、蒸馏技术到模型增量学习</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247489007&amp;idx=1&amp;sn=0db6eee691b78d2752afb0b6831f8e73&amp;chksm=ced71c64bbda0c93e680f8eee87f91b08d9b1f4bc1c02c5ebfaa249928aa27c11ddf16872858&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 24 Apr 2024 02:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一大堆Chinese Llama3正在袭来]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kAtCrqMZn1zObKFd9JrC3WdjqICuNcLh6GALUodzDoUUuUvCztoxXIWgQsAUYbR7icQP03Z5uAMBg/640?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面大家好，我是刘聪NLP。Llama3模型已经开源，可以就在这短短的两天之内，就出现了很多Chinese-Llama3 repo，开源社区也是相当的卷。再看到Llama3相关报告的时候，就预料到</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488971&amp;idx=1&amp;sn=4fa0ba79c48190bffe64b5d6b4103dd3&amp;chksm=ce6c7bb880923939733a5c6fdc93bb133989cfd59f7d5b5c29f45b3f15c916bc5f96ff51e3bf&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 21 Apr 2024 05:08:59 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[浅谈Llama3、大模型开源与闭源以及合成数据]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5m6glvtibBsfsnb6kMbKibSLnyiabDBKvibzg5HhoJR8R0Ajgpqa35GLAp01CJv3XkTOiajVFAhm7BwWjw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天凌晨MetaAI发布Llama3，不少大佬纷纷发表看法，在这里分享一篇@张俊林大佬的想法。知乎: https://www.zhihu.com/question/653373334/answer/3</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488956&amp;idx=1&amp;sn=d9fffff4dfe462406d6c2d5268fea758&amp;chksm=cea4fe2a7ac4332f9cf1f07b6151ea5f74df5d972a0457ee3ab1412a7862c76f0b19f95c17f5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 19 Apr 2024 08:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM2LLM：迭代式数据增强策略提升大模型微调效果]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5m5YZBAccpTia1LDlGKmSctER4HDZjiatyylSKbvwPzuf5D3Pia2gcxDfXHKaHJQKls6cxIwSgib7ptaA/640?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面大家好，我是刘聪NLP。大模型目前已经在很多领域、很多场景中都取得了较为优异的效果，但很多实际场景中仍然需要进行模型微调，那么如何在有限数据情况下提高大模型微调效果呢？今天给大家带来一篇针对性</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488950&amp;idx=1&amp;sn=b4ca115d1c7ac41557668a6c72e7eb59&amp;chksm=cedfacadfac7b7d707e3b0ac9aef4998a9d8e1ed32b3afb85eb6b86433165a59bacd7323515f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 15 Apr 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[智猩猩AI智能体技术研讨会最终议程公布！6位学者和开发大牛现场解读AI智能体内涵]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5m5YZBAccpTia1LDlGKmSctELk9GIzLrpvZ780dpzQJ4fjVMdaiceFPAkJswtT3FVGOuIftpqZ4mFsg/640?wxtype=jpeg&amp;wxfrom=0"/><p>自ChatGPT面世以来，生成式AI加速狂飙，在学术界、产业界、投资界掀起滔天巨浪，冲击着千行百业。AI大模型飞速迭代，创新应用层出不穷。我们正处于技术野蛮生长的爆发时刻，见证着AI向通用人工智能全速</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488937&amp;idx=1&amp;sn=25698f1061be0078f1bdc50205165932&amp;chksm=cedd6d5662e97b178e87882b6a1c43325b835fb3f0b880a0529d9820422af438a622778e02d6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 15 Apr 2024 10:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[如何快速提高大模型的向量表征效果？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mXOFtJgI4mCgCfFHCkkYsDaicBadRrJicQamjUtriajEe3pOXcIYmvIQYg0aGvMxNPFPjjQ9cxibnLJw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是刘聪NLP。RAG已经成为了大模型落地的重要场景，那么如何retrieval到正确的证据片段变的十分重要。目前一般采用采用向量匹配的方式来增加语义召回的结果。在METB榜单上，英文向量表征</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488931&amp;idx=1&amp;sn=2cb3de888db88fe86b9d8a10cf770bc5&amp;chksm=ce7d8aa2b7a97e1e73761a5eceb6fe59af3b8b23d66451edbf15465df538599a84b4d5f0e64a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 11 Apr 2024 14:40:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[RAG系统中答案无关的片段对LLMs生成结果有何影响？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kCAC7ux3Zr7cftQFIHIELam7fDJIvYYuU3TV9ibzeT7k5XwwjljvqnIhk4KJoxkOFN3Dg0icSE8uRw/640?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面大家好，我是刘聪NLP。RAG（检索增强生成）通过检索系统找到用户问题相关的信息片段，利用大模型综合生成一个答案，极大解决了大模型幻觉、信息更新不及时等问题，已经成为了大模型落地的重要手段。但</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488921&amp;idx=1&amp;sn=ebfc324068e6b828de73f5cd4588c545&amp;chksm=cefe5ff3abcd0eb126fe81b901a20c13415485b25b6c9a9b1ce677be796be54988f60b6be1e0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 08 Apr 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[PiSSA方法 | 仅修改Lora初始化方式显著提高模型微调效果]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kCAC7ux3Zr7cftQFIHIELam7fDJIvYYuU3TV9ibzeT7k5XwwjljvqnIhk4KJoxkOFN3Dg0icSE8uRw/640?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面大家好，我是知乎@孟繁续。今天给大家带来一篇高效微调LLM的文章，仅修改Lora的初始化方式，就可以显著提高微调效果，论文全称《PiSSA: Principal Singular Values</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488843&amp;idx=1&amp;sn=a5be220757270d82db795d90072c96be&amp;chksm=ce4bcc5d4a195ce231d0bd19dab9de2b770b2397677024550215c1b7d1fc3ca575f1830ba0cb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Qwen1.5开源32B模型-将开源进行到底]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mA6jTVRCia6bWxcWgWWdkE3J1UA2cmCFsloAL3VnZNfDEN1RbO6uZDYzrbAuAnX5YbGiba7jpqiapOw/640?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面大家好，我是刘聪NLP。阿里开源的千问系列模型，一直受到业界好评，之前版本有0.5B、1.8B、7B、14B、72B，但一直缺少的30B级别开源模型，这也一直是一个遗憾。怎么说呢？72B模型太</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247488818&amp;idx=1&amp;sn=1b5b9d354198cc6b6c444249571a27b8&amp;chksm=cee8e19c56238ae22698b02ee98d02b00022b8813991d98db907f0547db89c2d1d953fa4d162&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 06 Apr 2024 12:35:04 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
