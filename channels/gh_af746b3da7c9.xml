<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    






















    <item>
      <title><![CDATA[教你从0开始发一篇SCI，科研小白必看！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWAJgQIahQH2SuxZMgrGRSLRcxpM9OgxMlzuExWloqCxhdU6op04UiaoA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天向所有在2024以及未来几年内发论文的同学分享一些资料：23年各大顶会论文合集、80个代码中的即插即用模块、论文写作方法论、以及完成初稿后的论文润色。发论文，首先大家需要解决idea的问题。最有效</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=1&amp;sn=9df06a828cf03a91f18547f401759456&amp;chksm=ea44d0f81511eaa6d5a72b0bd4866fb9dfd8f523116744c4749fce099ed17e400fc40d473d88&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | 邱锡鹏团队新作：探索LLM预训练的Data Mixing Laws]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWu5nJHLJIB9zhbDAbJ6d7s1hTblTGatbEV2xqOLzzWdH3cP5gv2LTnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：大语言模型的预训练数据由多个领域（如网络文本、学术论文、代码）组成，其混合比例对结果模型的能力有着至关重要的影响。现有的研究依靠启发式方法或定性策略来调整比例</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=2&amp;sn=1d761a15296acc6bcfee5e1febf4477d&amp;chksm=ea8eebfca822d6d8df3fa93eb57dc977fd53342df88af294e7995497d6766d198db3a42b70e9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | DeepMind提出SAFE，用LLM Agent作为事实评估器]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWu5nJHLJIB9zhbDAbJ6d7s1hTblTGatbEV2xqOLzzWdH3cP5gv2LTnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：大语言模型（LLM）在回答开放式话题的事实搜索提示时，经常会生成包含事实错误的内容。为了对模型在开放域中的长式事实性进行基准测试，我们首先使用 GPT-4 生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=3&amp;sn=c983c5f727685842f22d1d6becb0a323&amp;chksm=ea59a648e1c549d8ea6c2cd1443cba873bbce7b01c77928cbe35c698ddf263ea8bdcfe3d75f3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | sDPO-不要一次就把对齐数据用完]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWu5nJHLJIB9zhbDAbJ6d7s1hTblTGatbEV2xqOLzzWdH3cP5gv2LTnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：https://arxiv.org/abs/2403.13269Q1: 这篇论文试图解决什么问题？A：这篇论文试图解决的问题是如何在大型语言模型（LLMs）的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=4&amp;sn=ee248439173e5c359944fb4b37043458&amp;chksm=eaaaa58fccd65f96a9c3ed0ad0e8288ac47ac3382be1ce0f3952459f514812319bb7c636e267&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[全面解析LoRA、QLoRA、RLHF，PPO，DPO，Flash Attention、增量学习等大模型算法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOoS9b7j62Xe1s04eVP0YlXAC5J9EicHZYTVbmRbsEBUw5XKI9a6GQIYg/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着大模型的飞速发展，在短短一年间就有了大幅度的技术迭代更新，从LoRA、QLoRA、AdaLoRa、ZeroQuant、Flash Attention、KTO、蒸馏技术到模型增量学习、数据处理、开源</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=1&amp;sn=e9439999d46eb2c4987061d8d53c275a&amp;chksm=ea0df41f1cdd732cb15061e3ed4bca93b94545693c6ba646bfc14ec4336a08abe5f1ec8b2dd2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[RAG实践中的关键模块解析]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOBEBqTuhfvWh8HYqOkxvibFCxaXdctZhl0QLtUfep34M0Y8ZQunpF0ibA/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：孙鹏飞，南京大学 · 计算机科学与技术，互联网行业从业人员声明：本文只做分享，版权归原作者，侵权私信删除！原文：https://zhuanlan.zhihu.com/p/682253496编辑：</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=2&amp;sn=aef77d67acbd9cc7db697711f4ad21f6&amp;chksm=ea224ac74e2715aedf6c6dbd47cb098afd3ac3aeff44ba65db3a7951d29ead130906e61c5e36&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[复旦MOSS团队：数据配比的scalinglaw]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOiaOGs9EdqqFQM2rYic4ItPJaCE4oGLJvyZiaVafMf6Cd8eWJqMrv6ppJQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：包包算法笔记在前文我们提到过，大模型训练中数据的多样性和质量是最重要的两个维度，并且在结尾挖了一个大坑，希望有大佬愿意研究多样性的scaling laws。这次，复旦MOSS团队带着数据配比sc</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=3&amp;sn=e49f6c0aaaddd6d1c89001e1f84eae32&amp;chksm=eaff9e2a54ba6ddbb2e10ee2f8016a24aeb93970fd4854203cf99fec84ef5def3e2544a18ca6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[长文本之罪：Claude团队新越狱技术，Llama 2到GPT-4无一幸免]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOicp314zx0hiaURDrGSglDvr5bibQQm12TYogNHnqpZ94rDJgPEtOAaaeg/300?wxtype=jpeg&amp;wxfrom=0"/><p>机器之心报道作者：杜伟、陈萍Anthropic 发现一种新型越狱漏洞并给出了高效的缓解方案，可以将攻击成功率从 61% 降至 2%。刚刚，人工智能初创公司 Anthropic 宣布了一种「越狱」技术（</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=4&amp;sn=9fe5a5dea900cc4bfb000ebc9acefe35&amp;chksm=ea2e69ec2929bc4a6cd45bc752d0e060d34b8329211651118fbb7430fe1b1eacd6bed640f569&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从 大模型接受弱智吧再教育 谈指令微调对齐]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTrmk0LnGnGAcmLLO8sEgPaZKHMmjLQ6LzW1S67oTEjwthXEdYgH1RHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：hzwer原文链接：https://zhuanlan.zhihu.com/p/690667537仅学术分享，侵删这两天一篇论文以离谱方式火了：CQIA：“用弱智吧数据训练的 AI 爆杀了所有中文</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=1&amp;sn=6c2d6bf3594b5201c52881630044ecb2&amp;chksm=ea2aa0c5959e97c86265cbcea09b58a4a33178d163671a9a671b23c65cc5fe822d79c242effa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | RLRF: 从反思反馈中不断迭代进行强化学习对齐]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTvFIQJ5VKDGrr016eIX7XwOcT1BbtM95hfvHjxa6OvQUt1B1EiaBGia1Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：尽管 RLHF 在使 LLM 与人类偏好相一致方面大有可为，但它往往会导致表面上的一致，优先考虑风格上的变化，而不是改善 LLM 的下游性能。不明确的偏好可能</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=2&amp;sn=ec0cfedef98843c710d3420d61f52b43&amp;chksm=ea1d70b19a543682530b3cdca5f5c5502a2dc214070ba02f7222deca5ebc9ee74f648e4900b9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 一次编码平行解码：高效Transformer解码]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTaJcsKibBMYNsK1NgowaS7syPCl3snNZU2kssSP3U5dHvFWs07Qhfsmg/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：基于Transformer的 NLP 模型功能强大，但计算成本较高，限制了应用场景。经过微调的编码器-解码器模型在专业领域很受欢迎，其性能优于 GPT-4 等</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=3&amp;sn=114cf4ec777732490531ab76845c637d&amp;chksm=ea42d2342e3183b712c87cdc87f38c5ae29cd6447da94b62ac79283903da5cbf6d21b148b65f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | NAACL'24：自生成翻译记忆缓解翻译持续学习遗忘问题]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTVpa9Rlt2sUtSKKnSAq8QUvgZCsDydzX3ks4libtavx0HribfsGq7xyKQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：现代神经机器翻译系统在几种不同的语言中表现出强劲的性能，并在不断改进。然而，它们的持续学习能力仍然受到灾难性遗忘问题的严重限制。在这项工作中，我们利用enco</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=4&amp;sn=fc226988e6685cee9941423d8bdd1270&amp;chksm=eaa699a9b6fd4ea5e31732682fc45c5296a957bbf5251997a03743d54452b8064cbbb2442700&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
