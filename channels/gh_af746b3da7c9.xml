<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    




















    <item>
      <title><![CDATA[Llama 2 高调开源，大模型微调我已经上手了(附99个大模型微调模型/数据/工具)!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia5B8nlrH1E3yxIkqWYt6xkE3qUQywQyX30WxkEtFWkXLt0V36yIicrogmYMhSvAHo8lYWlib1DicsQw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近出现了一系列令人激动的开源大语言模型，如meta的LLaMA、清华的ChatGLM等。伴随大模型一起爆火的，还有大模型的微调方法。然而随着模型规模和任务数量的增加，对整个Transformer模型</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526210&amp;idx=1&amp;sn=47a5bf39cdc319e6c111fa59548b5d4a&amp;chksm=ea7cbc236855625f46aeb6ae5a73c2aa5a952222845ce30f639ce5de66e9a33cfad6d24f0518&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 05 Mar 2024 06:22:20 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | I-LoRA：解决PEFT中的灾难性遗忘问题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia5B8nlrH1E3yxIkqWYt6xk5q7V9BwibnDlv7uRR3BIibzDciaXhs0ibW42cQfnrZ5LMyxreLRQibbticNA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp师姐1个月攻下LLM的所有知识的捷径Analyzing and Reducing Catastrophic Forgetting in Parameter Effi</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526210&amp;idx=2&amp;sn=c8079e53ef2770ede1c0d72ade8c1599&amp;chksm=ea65b205f307023aad09b1bed717ed43498b8e2c6ed599297f44ffcefb21df509d65075abe6f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 05 Mar 2024 06:22:20 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | 检索是更准确的生成（RAG相关）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia5B8nlrH1E3yxIkqWYt6xk5q7V9BwibnDlv7uRR3BIibzDciaXhs0ibW42cQfnrZ5LMyxreLRQibbticNA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp师姐1个月攻下LLM的所有知识的捷径Retrieval is Accurate Generation摘要：标准语言模型通过从固定、有限和独立的词汇表中选择token</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526210&amp;idx=3&amp;sn=b977d47d9c0449167171979e2b0a99ff&amp;chksm=ea34ae8538310ec8845ef0db02bdbac940eb87bc1aaaab2f52078f2985a2bf375b8ad256d956&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 05 Mar 2024 06:22:20 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | 持续学习会给大模型带来什么？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia5B8nlrH1E3yxIkqWYt6xk5q7V9BwibnDlv7uRR3BIibzDciaXhs0ibW42cQfnrZ5LMyxreLRQibbticNA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp师姐1个月攻下LLM的所有知识的捷径Investigating Continual Pretraining in Large Language Models: In</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526210&amp;idx=4&amp;sn=61e268f0910bcc6dc9bb8747143d8f7d&amp;chksm=ead0923a0aa133c0f158896f6f9ad9dc124d64b19fa7ddb1f30dc281fbb3da7ceecc894bae54&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 05 Mar 2024 06:22:20 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | 大模型是如何处理multilingual场景的？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia5B8nlrH1E3yxIkqWYt6xk5q7V9BwibnDlv7uRR3BIibzDciaXhs0ibW42cQfnrZ5LMyxreLRQibbticNA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp师姐1个月攻下LLM的所有知识的捷径How do Large Language Models Handle Multilingualism?摘要：大语言模型（LLM</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526210&amp;idx=5&amp;sn=c8ef3580d71eb22fd971729332244b26&amp;chksm=eac85623b0ffd00b070a049e1b5e60eba24796b23afd1a1ba46990407d4c33ba4cdb1adfec21&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 05 Mar 2024 06:22:20 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[全栈大模型微调框架LLaMA Factory：从预训练到RLHF的高效实现]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaSWUfBaqRzXFEf3QHaOF4jiamLIvrtv3y0b9Jh5zEW3ic3qwVzrSjgicY0Qrg2JJ5ZFe4d5GThZuRjg/640?wxtype=jpeg&amp;wxfrom=0"/><p>主题全栈大模型微调框架LLaMA Factory：从预训练到RLHF的高效实现个人简介郑耀威，北京航空航天大学博士生。以第一作者在CVPR、AAAI、WWW等国际会议发表多篇论文，担任AAAI、EMN</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526170&amp;idx=1&amp;sn=b6a46827c8487e79cd30a9a4cdb8d6c4&amp;chksm=ea76e2133fbdb2a537dde5e844c1a8b9410d4ff358045c44063459baddaa6a9ba19c0ccd22ca&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 04 Mar 2024 15:26:05 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从0开始预训练1.4b中文大模型实践]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagkKMfGIPj1sxLHv1ibfqzGlpeeHnYa2wN2g99173g9SibWB2ZD4HoSiay4ia0zh9UtWvtdBcrCXGp7NA/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者：Lil2J@知乎（已授权）链接：https://zhuanlan.zhihu.com/p/684946331简介这篇文章主要记录了我个人对1.4b中文大模型的实践复现过程。我选择了QWEN作为基</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526159&amp;idx=1&amp;sn=5c9de926bc62fdcac8e227e3d392827a&amp;chksm=ea81e2c8da39f61b6f986d42ffa5ebeac8ba31913715a15f1ed909ffad4f8a8a08f38b4c7c23&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 02 Mar 2024 12:25:34 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[合成数据(Synthetic data)微调大语言模型实战指南：背景、方案、案例、代码、评估]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagkKMfGIPj1sxLHv1ibfqzGlxfZkbNtCqicLbR5ynhTL1lbaLsWMpygnWTiaicYKDGRbJMfWE6fPbFrOA/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：旺知识应该微调自己的模型还是使用公开大语言服务接口(LLM API)？创建自己的模型可以完全掌控，但需要数据收集、培训和部署方面的专业知识。LLM API更易于使用，但会迫使将数据发送给第三方，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526159&amp;idx=2&amp;sn=34e9830656726ceb81f15bd3ecfca77e&amp;chksm=eabc8e812e6ce0e24c7cf50c2e7a95b1f4c80de67dbd47d469582f6e861c532903d5f931c79c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 02 Mar 2024 12:25:34 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一文带你了解sora技术背后的论文【附32篇论文合集】]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaE6d3R7iagDxRcZjYa0bia8tm37l2OEuFHFFCWkuw1GkmBDhTf61AEHH7kuZcaC0LiaoTSWGzNTDPlQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>2024开年，OpenAI 又发布王炸级产品——Sora，这是一款可以根据文本提示词直接生成视频的工具。（文末送sora书籍）而Sora技术报告中曾引用了一项研究成果——DiT模型，出自谢赛宁与Sor</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526131&amp;idx=1&amp;sn=7fb41d4eaecec3a4a453be57a2a56a86&amp;chksm=ea30977d9ece34e1d47310b38de3a649deadfc5580795145744d22a745af81eb88f0d8bcb788&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 01 Mar 2024 08:23:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GNER: 生成式实体识别的新 SoTA]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajWbMuYPhiasg38HRaY1icgxF4O3d2jmr7nJtq8diaKRVsudcibeTJA7oibXTQ0HhZ73iatIEWbkpYkwodA/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：dyy深度学习自然语言处理 分享论文：Rethinking Negative Instances for Generative Named Entity Recognition地址：https:</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526131&amp;idx=2&amp;sn=239a6f44bebb9bab3994e000b8679055&amp;chksm=eaed58988f2722034f70fa5a935936407f752f9d1b48985889c742f45f6a675b80e1366632d4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 01 Mar 2024 08:23:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 用于参数高效微调的小型集成LoRA]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaE6d3R7iagDxRcZjYa0bia8tupS58YOnauVmXtzZib0KeDGPg02Ffibz6lsoz9OdStFeNkjm4EBhv1uA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：ppMini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning摘要：参数高效微调（PEFT）</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526131&amp;idx=3&amp;sn=5402af0ecdd6967da76cb5afb061ef8a&amp;chksm=eab0e61cb27933d3573fb7c52711c7da74af0bebecfadb350375efed0984ca184b6ef34bd4b6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 01 Mar 2024 08:23:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 当缩放遇到LLM微调:数据、模型和微调方法的影响]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaE6d3R7iagDxRcZjYa0bia8tupS58YOnauVmXtzZib0KeDGPg02Ffibz6lsoz9OdStFeNkjm4EBhv1uA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：ppWhen Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526131&amp;idx=4&amp;sn=586e74e51aabbb6557430ee9804dcfd7&amp;chksm=ea69a4932a37c44d003a266064c240eaa90f654c340c8c8b4d6fa0fd2ab14412cd0d50782f67&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 01 Mar 2024 08:23:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 语言模型的最优学习]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaE6d3R7iagDxRcZjYa0bia8tupS58YOnauVmXtzZib0KeDGPg02Ffibz6lsoz9OdStFeNkjm4EBhv1uA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：ppTowards Optimal Learning of Language Models摘要：这项工作研究了改善语言模型（LM）学习的一般原则，旨在减少必要的训练步骤</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526131&amp;idx=5&amp;sn=407a845804f7b0ced6e01b4d1b6bc5fd&amp;chksm=ead6288643c83994c92ecea065b33d327d743c30a2eae2aafd96e2122eb1602c295e01e6f34d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 01 Mar 2024 08:23:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[师姐1个月攻下LLM的所有知识的捷径]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajWbMuYPhiasg38HRaY1icgxFfXQkib0sGStmWQfA0dumhPq6XxEML8TOjeTPbicGaPSrISFIX6flxvVg/640?wxtype=jpeg&amp;wxfrom=0"/><p>正月十五过去啦，有些同学已经上班了，有些同学假期可能刚结束（羡慕）~新的一年希望大家都有新的变化，对于咱们NLPer来说，估计LLM是咱们今年必定绕不开的地方，多模态的Sora也告诉我们，学不完根本学</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526060&amp;idx=1&amp;sn=a5e2cdcd4bf0c15b0e81b7a5e8d33155&amp;chksm=eae3fe8bef35c0eeae348219df55acf1a5afd95a4e4ca2d312a173826adab683507633f6da59&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Feb 2024 10:07:42 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 1-bit LLM时代：所有LLM都在1.58Bit中]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajWbMuYPhiasg38HRaY1icgxFvs5E1BhcrN5Hv0kfx4UY8gEIic2dgwXuKv1iav3tgOpXkLMRMMAia4D5A/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：ppThe Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits摘要：最近的研究，如BitNet，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526060&amp;idx=2&amp;sn=67b21136228cf5d7ff56ecce38c09587&amp;chksm=ea380569813acb8406b5695e66858e9f961caaa87f08c9c7c9a10816d87fc3b6cb7d54aa1aba&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Feb 2024 10:07:42 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 基于例子还是基于规则：Transformers是如何进行数学运算的？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajWbMuYPhiasg38HRaY1icgxFvs5E1BhcrN5Hv0kfx4UY8gEIic2dgwXuKv1iav3tgOpXkLMRMMAia4D5A/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：ppCase-Based or Rule-Based: How Do Transformers Do the Math?摘要：尽管在各种复杂任务中表现出色，但现代大型语</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526060&amp;idx=3&amp;sn=265c9952c080b9f5b5b2cdae937afbb7&amp;chksm=ea1e9932ce1415c8c105503be657f66a80eabfce5720be165c9789a0b141a9a4d3929f46e7ea&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Feb 2024 10:07:42 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | LLM中的大规模激活]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajWbMuYPhiasg38HRaY1icgxFvs5E1BhcrN5Hv0kfx4UY8gEIic2dgwXuKv1iav3tgOpXkLMRMMAia4D5A/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：ppMassive Activations in Large Language Models摘要：我们在大型语言模型（LLM）中观察到一种经验现象-很少有激活表现出比其</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526060&amp;idx=4&amp;sn=fe28921906663fc20bb6353b2240918a&amp;chksm=ea6edb12bb1f4072535a8970f9ac0e004bc2077fef7a0e92710b947ab601abb5ccf3b1b34b79&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Feb 2024 10:07:42 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | Agent-Pro：通过策略级反思和优化学习进化]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajWbMuYPhiasg38HRaY1icgxFvs5E1BhcrN5Hv0kfx4UY8gEIic2dgwXuKv1iav3tgOpXkLMRMMAia4D5A/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：ppAgent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization摘要：大型语言模</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526060&amp;idx=5&amp;sn=106084c4db6dc5c26c6c5f43cd0a7391&amp;chksm=ea55c0c2e3cdb73bd4d1a79f023d1128d5a5ada9aa5b14c9afb6d63044afc519def8f880c295&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Feb 2024 10:07:42 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[要读博，6个月发CVPR2024经验分享]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajrubhz2QKFnww37gz4OJ5YUN6ictSUAAgKTlADMBK1DVia8VyGytCH5ibCYksBtIVWsHQlvdLFyRQJg/640?wxtype=jpeg&amp;wxfrom=0"/><p>建议发论文的同学不要卡着deadline来，一定要给自己的科研论文留出足够的创作时间。科研论文写作时间安排国际顶级会议科研论文如果你要发国际会议建议预留4-6个月写论文的时间：以CVPR2024为例，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247525912&amp;idx=1&amp;sn=698c5aa9f0fad288e5bc1744cda793b0&amp;chksm=ea50a0b19fecf76293ffdfe470b539d43534a803009fe94ab2e2964ff7d90bc46a43a14b3ef1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 28 Feb 2024 06:48:39 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[全网最细致大模型MoE原理+代码手撕版]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajrubhz2QKFnww37gz4OJ5YrnVL0iaT8QC3ywNRhFpicPU0Z4ibdRJ5uhibiakaiahMJof7Vs49UrOc3mXw/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：大猿搬砖简记大家好，时隔不知多少月，LLM并行训练系列终于又有更新了（抱头防打），这一章我们来讲MoE并行，同样分为原理篇和源码篇做讲解。关于MoE并行，我体感最深的一点是，它的定义不像tp/d</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247525912&amp;idx=2&amp;sn=551b18bb023e761ee7e592431692e65c&amp;chksm=eaf30d6d8079ece50462c5b451b728cc27e1b65f54ce30a1a4acbf2e94b77ff7506c77921c45&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 28 Feb 2024 06:48:39 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[8/8/6/3的Mamba论文，最终还是被ICLR 2024拒了，网友：悬着的心终于死了]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajrubhz2QKFnww37gz4OJ5Y5WBPcgorZIJ2NCRCLe3ftACQeWt7LlE2THJDM7Zeq8nGykAPXeU7FQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>机器之心报道编辑：蛋酱几天前，ICLR 2024 的最终接收结果出来了。大家应该还记得，Mamba 被 ICLR 2024 大会 Decision Pending（待定）的消息在 1 月份引发过一波社</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247525912&amp;idx=3&amp;sn=e5f644440073260571a941fd21987fc2&amp;chksm=ea8f42f8617fa3e1e60fee8b2c081f2219b08b64ff960956479b87cb5529f1e3eaf7e6efd561&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 28 Feb 2024 06:48:39 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
