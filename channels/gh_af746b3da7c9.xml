<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_af746b3da7c9.jpg</url>
      

      <title>gh_af746b3da7c9</title>
      

    </image>
    













    <item>
      <title><![CDATA[ICLR 10分论文如何做到？原来掌握这些就能发顶会！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajL4qyqLTUgMznUduTUF3fzfD5hkWxo8SP1yiccQEPxHC2yicAfHtNyKUelibzMJVJSEmqpVicFxakCIQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>SCI期刊和CCF会议论文发表流程（从确定方向到撰写论文6个步骤），成功发表过或者写过论文的同学都熟悉了（没发过的同学参考下图）。那发高区论文非常重要的几个点你知道吗？比如：1.高区怎么选方向；2. </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534289&amp;idx=1&amp;sn=1450d691da4cc74499614a504a6e6647&amp;chksm=ea11e313a471cae60b0c60018389a9ae8e9b1f260efe27b80f8fce1cff20ca8fdfbf1b1f1bba&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 12 Dec 2024 07:15:26 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2024 | 智能体不够聪明怎么办？清华&amp;蚂蚁团队：让它像学徒一样持续学习]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajL4qyqLTUgMznUduTUF3fz8nyNc7biaE5WeuB8aNFPqKp2yHm7M3iaZuJ0IojmSOSmxOuJv4mOTDlA/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：机器之心此项研究成果已被 NeurIPS 2024 录用。该论文的第一作者是清华大学计算机系博士生关健（导师：黄民烈教授），目前任蚂蚁研究院副研究员，其主要研究领域为文本生成、复杂推理和偏好对齐</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534289&amp;idx=2&amp;sn=e512761f0708fdf88616e1e3a80b0c3d&amp;chksm=ea7bf0d702beaf144bba8851163baa5490f69bdbbde5f6160993416190066a57ca1797fbdcb2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 12 Dec 2024 07:15:26 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[强化微调是个什么东西？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajL4qyqLTUgMznUduTUF3fzRVvw0ntR5Z3LR94KxMkJ0NMIpXoarpOMbntea3ia7Dk95sIbNCuwHsg/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：包包算法笔记大家都听说过监督微调SFT，强化微调是个什么东西？这次为期12天的OpenAI发布系列的Day2，就搞出了一个让开发者震惊的玩意儿。强化微调（Reinforcement Fine-T</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534289&amp;idx=3&amp;sn=77956e5e9adf814ee3de340feb33d5d8&amp;chksm=ea507b6e61b9588cff6ed4926be45837b615eee9b6671b1b039ebabe896bf7546a73028c18e1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 12 Dec 2024 07:15:26 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[揭秘强化微调(ReFT)：重塑大语言模型推理格局的突破技术]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajL4qyqLTUgMznUduTUF3fzAvHLadZxvnZ0uVib06ibwHWgJibd3wIRlapAoythkNENMWWweZQvTmHCQ/300?wxtype=jpeg&amp;wxfrom=0"/><p> 来自：旺知识提升大语言模型推理能力成为关键任务，现有通过监督微调（SFT）结合思维链（CoT）注释的方法因仅依赖单一推理路径数据，致使模型泛化能力受限。例如数学问题求解中，训练数据里通常每题仅有一条</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534289&amp;idx=4&amp;sn=4919f7dab3f09f1d9f2a52cac8b955b5&amp;chksm=ead58ac0dc40adfde91b8158f48fc77dec2c8978f2fb281b166cf58569a0385a069d3c2d98c2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 12 Dec 2024 07:15:26 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[开源 Agent 小屋]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajpXQmR5NJQX5Uf5zhicg77bHk9eMkF5zThKoeXWVhGTicKbBvYu8ibIw2r4QT23WAosVr2Zl1NtJWGA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：何枝地址：https://zhuanlan.zhihu.com/p/9096314010「深度学习自然语言处理 公众号」做了重新排版，转载授权请联系原作者Live Demo（网站在进入前可能会加</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534261&amp;idx=1&amp;sn=baf9e20683ca84533832c241b02c675a&amp;chksm=ea509505a07671c2291f969d3032e362177bdc786525bd730389942fb4d2871bcccb9e9e3904&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Dec 2024 12:35:10 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[让英雄查英雄，让好汉查好汉：LLM-as-a-judge综述]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajpXQmR5NJQX5Uf5zhicg77bZokfWMWOvmUloMkxt8dibBibvO3gM4FmXiczTZUhPIxwdt8AG7q89okXw/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题让英雄查英雄，让好汉查好汉：LLM-as-a-judge综述时间12.11  20:30-21:30 周三引言最近，LLM出色的表现推动了“LLM-as-a-judge”概念的诞生，即利用LLM </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534261&amp;idx=2&amp;sn=230d5c02da8d4132ddde6cc711c325f8&amp;chksm=ea0936cf54a04697dbd05ba54229e9ed0e20f99b5e92b5d7cd9578122f7cdc358f88cd7d4f6d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Dec 2024 12:35:10 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[基于LLM的社交代理在博弈论场景中的综述]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajpXQmR5NJQX5Uf5zhicg77bUpdwokULz01y2Klicatc27zsWl4jvGACSeTHLlTukfNXvB2cgXfZiaFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>这篇文章题为《基于大型语言模型的社交代理在博弈论场景中的调查》，系统回顾了现有研究，总结了大型语言模型（LLM）在博弈论场景中作为社交代理的应用和进展。文章分为三个核心部分：博弈框架、社交代理和评估协</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534261&amp;idx=3&amp;sn=b5d7612a6a5878d2c4d1edb4f9075ece&amp;chksm=ea9c14b989c319d2f390904a5f27e0eabd70ea44ca01bfe556232d17e1308d06ab4f84742c83&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Dec 2024 12:35:10 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[微软提出ICAE：从上下文压缩的角度看待推理加速]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaDfmm8v5c78M5cgVsb2XpGe3IkaSTscwGVk8AYtdKwGFMzuh62nnsUYso2mRXeqeSbBlsy31zClQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者：难赋由于目前主流的注意力机制依然是二次复杂度，对长上下文的语言建模仍是目前Transformer模型的一大挑战。之前的工作主要放在对模型架构的改进，如注意力模块稀疏化在注意力中引入记忆信息对外部</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534206&amp;idx=1&amp;sn=fce02234571138f1c9777380e568f437&amp;chksm=ea71d5d9db2b8bc31152107edcd00e793a99677975ff66812e7e7482406a947feda572c9b527&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 11:41:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从答案到问题：一种新的学习目标让LLM更擅长推理]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaDfmm8v5c78M5cgVsb2XpGsDClO025hEy1wwDkZpfKRlJ9Hfu9CCwQXxnCTYAGUtibAxic80WWmKmQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>整理：深度学习自然语言处理 公众号这篇文章介绍了REVTHINK框架，通过数据增强和学习目标来增强大型语言模型（LLMs）的反向推理能力。研究表明，REVTHINK在常识推理、数学推理和逻辑推理等12</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534206&amp;idx=2&amp;sn=0fa464f55cc83a71eda8197847617ae4&amp;chksm=ea47adf190ac68cd8257c7bbaadff2bc6e36e81352bb549213bd7874be22fd5bac5b29911ff4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 11:41:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[二战字节大模型算法岗，拿下70K offer！！！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWf2hHnKAd9nBNkIoAuXZh8IZUfQ1NOLnytWQqCg2Lr2lsbQ3p5vGoTYUHgHxFtfS4tVL6FtdicKw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在牛客网看到有同学211本电气专业大三面试字节实习，连简历初筛都没过，毕业后因为kaggle金牌经历，成功二战拿下offer。其实无论是互联网还是传统行业，招聘软件上但凡是头部大厂的算法相关岗位，基本</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534150&amp;idx=1&amp;sn=616d03cbd2644f943ddd5fad73c47565&amp;chksm=ea6b3352a1aad0ae301e20e610e632a810651a6cf7bca788a3e310e2137ccbc8d829df398a74&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[盘点 2024 年的视觉语言模型VLMs]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWf2hHnKAd9nBNkIoAuXZhmlIa0M81LibPNSIrQLFurGZrELFNWia2DyZHoErNplFNxqA0Ph9TL00Q/300?wxtype=jpeg&amp;wxfrom=0"/><p> 作者：AI椰青原文：https://zhuanlan.zhihu.com/p/7827587018编辑：青稞AI1 引言视觉语言模型（Vision Language Models, VLMs）是一类</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534150&amp;idx=2&amp;sn=565767dabc308b02d0ce01e9110dfc6a&amp;chksm=ead536732668a767c0a2ac7b8185eed793e61aaa2d995fdf290a12162d76d7d7e3f490578a1e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型时代的科研之路：写给过去的自己]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagGbtKhPK0Y9HNmNy5K43smJ3tnP9RA16oKIZdaXXYicjXHrXjc7uEHvN6B9xCpic5cBrBmtuSkicpsA/640?wxtype=jpeg&amp;wxfrom=0"/><p>主题大模型时代的科研之路：写给过去的自己 时间2024.12.08 10:30-11:30 周日引言假如时光可以倒流，回到大语言模型刚刚兴起的时刻，你是否依然会选择这条研究之路？如果会，你最想告诉当时</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534134&amp;idx=1&amp;sn=1d49b9b766ccf1cd578e39e096ffeb03&amp;chksm=eae17a3185993adb355c6a560c9cd634665e38310c3dd96d0511601cb3169ebefdda27182bbc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 03 Dec 2024 06:35:51 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[VL-RewardBench: 通往多模态 self-play 的试金石]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagGbtKhPK0Y9HNmNy5K43smwocxIYiaEr5vmL7JPj5xjtGHRCSJYDzTy0GWq9icn67TtH0H4UxalWibQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：Lei Li地址：https://zhuanlan.zhihu.com/p/9426034901随着模型能力的不断强化，合成数据 + Self-Play 已经成为 LLMs 和 VLMs Pos</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534134&amp;idx=2&amp;sn=e7df91206731ef15e86b3229c531a0c5&amp;chksm=eaff2202d5f7fb5937a8f4625fb059c92ee48a35a700ca22594c4740c8b2d751f53ab6021d6b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 03 Dec 2024 06:35:51 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[浙江大学刘佐珠/吴健课题组 6篇论文被EMNLP 2024主会/Findings录用]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaQCsNRqkrRJyiak6MMBKGfvarBN0Cw595AqGPQXDUVx6LCaBpwUQWNrHIAicRkWAabwic8VPBJC65Mg/640?wxtype=jpeg&amp;wxfrom=0"/><p>2024年EMNLP会议于2024年11月12日-16日在美国迈阿密召开，浙江大学研究员刘佐珠、浙江大学求是特聘教授吴健课题组，联合新加坡国立大学、新加坡A*Star、北京大学、时代天使公司等单位，共</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534111&amp;idx=1&amp;sn=62a533d49e7cc77f23ce45d80542925a&amp;chksm=ea5dd167ebc39b5a4613128113b6116a0a021af07c9fa9149a76338350a2b9e4b6a99e2cf0c2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 01 Dec 2024 14:01:04 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM不会CoT隐性推理，只会显性推理！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah8ia2Tb5tVvK9Vq6sua8bMZfa43eE7HAwtmGymzZ9T36HzeEfriaSrlibAfkic7Yh9vibzJXle48ic8ohw/300?wxtype=jpeg&amp;wxfrom=0"/><p>这篇文章探讨了大型语言模型（LLMs）在隐式推理中的表现，发现尽管隐式推理理论上更为高效，但实际上并不等同于显式推理链（CoT）。研究表明，LLMs在进行隐式推理时并未真正进行逐步计算，而是依赖于经验</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534111&amp;idx=2&amp;sn=5536dab237bf4274d69e9224007ad377&amp;chksm=ea0c2386c8611d2192b3efbf0435c502362da3e28fd6707d40d65b80c79d5bf9a15e705e137d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 01 Dec 2024 14:01:04 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[本科生大作业给8分，iclr评审机制的失灵？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baj9UNxfkzBKLNYMPCJOArsQnHQ7VHTrAIbYBkIE9d8qp1ndN80Wnqca5XdysNT62XiaXtia693Nzo5w/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者：难赋——以前看见个评论说ai的十分制会议中的10分指的是“如何这篇文章没有中，我将断绝与该会议的一切联系”，要是今天讲的这篇论文中了iclr的话，没准这句话对于iclr来说可以改成对8分的评价了</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534079&amp;idx=1&amp;sn=9e43cdc345f2f0d424a9690715916746&amp;chksm=eacb2d7df51c493cc89d5833fcde5d8559e5556ed19c0adc9eaf08c81f474fe20d5887484568&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 29 Nov 2024 15:58:45 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
