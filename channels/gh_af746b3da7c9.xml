<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    























    <item>
      <title><![CDATA[大模型领域，你心目中 idea 最惊艳的论文是哪篇？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiacs3wOoAu29m8s9DFL55DcEic1KteF6JjtOZ49df6j0fOBCbN1J3w4tgEswOqZ5iax2vslj9WmZTEQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：Beyond Hsueh链接：https://www.zhihu.com/question/665735775/answer/3611972970推荐一篇 ICLR 2023 的文章：Seman</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529407&amp;idx=1&amp;sn=389d20bc6bcb690bea856ff3cf28c167&amp;chksm=eae651667c64cc2a2b21a07c331e74eba24ea64fbf852051bbb596b9cd6d8a4a195864d00c83&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 01 Sep 2024 14:33:29 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[入坑大模型18个月的反思与贩私]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiacs3wOoAu29m8s9DFL55DcIbdNqXEicgB2ZXhhHOmhlfD2NyB3dlWox6S1Gm2lDcxHEo9fiauiaSQIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎: Minogame链接: https://zhuanlan.zhihu.com/p/717402693编辑：包包算法笔记前几天开完一个有高层参加的会议，会后组里的技术大佬直接就开喷“要规划没规划</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529407&amp;idx=2&amp;sn=a0a026af84edf98625592f37305c5cbf&amp;chksm=eabd2b06ee51966d351723f3f36fdcecb4cc24f7733715935c74794ce73863ed5739050c7590&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 01 Sep 2024 14:33:29 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[一周打完1000场官司，中科院发布首个AI法庭AgentCourt！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiacs3wOoAu29m8s9DFL55DcdAiaicpIJoH92dSUKH8WVialPzLu5S46u0HnQ91YGhjZ8cY8YogTvoQYQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>编辑：LRST来自：新智元在人工智能重塑各个行业的今天,法律界也迎来了前所未有的变革。传统的法律实践面临着效率低下、成本高昂等挑战，而AI技术的出现为解决这些问题提供了新的可能。从最初斯坦福小镇火遍全</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529407&amp;idx=3&amp;sn=1c410330c388fbddf813e6177de9d4cc&amp;chksm=eac38bb256dc524e0a245a68fda7e2db32039a4df32655ee65fa1cd05c148a2c737f155e8fc0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 01 Sep 2024 14:33:29 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[LLM的范式转移：RL带来新的 Scaling Law]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajBLl5SltLUyttT3j67qAo9JeJKicMm9dTonKJ4RTia4Zm82V4xjiaBElBfmedltibyibIVuRJCcSviaEUA/640?wxtype=jpeg&amp;wxfrom=0"/><p>来自：海外独角兽作者：Cage从几周前 Sam Altman 在 X 上发布草莓照片开始，整个行业都在期待 OpenAI 发布新模型。根据 The information 的报道，Strawberry</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529378&amp;idx=1&amp;sn=fff2afb33393f0bbe797176aeb599f13&amp;chksm=ea1ae624d0ed56dffd1d6e48094cd3e2e39e827a00ef3c60a66aa83147c1c3cfbd165e784212&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 09:06:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[揭秘LLMs不确定性背后的隐患：后门攻击的悄然兴起]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajBLl5SltLUyttT3j67qAo9NHibQTBWsoX3VZrtRR76EudSR4nyc3ubXiaA4TmoLzJVByJhjvttoVAQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Uncertainty is Fragile: Manipulating Uncertainty
in Large Language Models链接：https://arxiv.org/pdf</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529378&amp;idx=2&amp;sn=b286cdb0c4aee358cc4821a24b1e21e3&amp;chksm=eaa971bd9cee36bc90d8794529c8ce528e7d1b6534a2a9843eb44c1ddf4350d0718383be8aa3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 09:06:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[vllm代码更新太频繁，我该怎么办？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajBLl5SltLUyttT3j67qAo9pNVibGic5hyQZBRNY3OEGjgByJljdVlCiaegg7ZziaibSov1kG0WRQPe2Rw/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：大猿搬砖简记大家好，大家在读vllm源码解读系列时，肯定会有以下疑惑：“vllm仓库当前主分支的代码，好像和当前文章中展示的代码，存在许多不同之处，这是为什么呢？”这是因为vllm的开源社区非常</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529378&amp;idx=3&amp;sn=fe853ad92ffa461e46bb026acc27402a&amp;chksm=ea47e7092172e86de15156c255c7b89dc9e664fae8d099417157771b2f8ef6dd6eb58c0be44b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 31 Aug 2024 09:06:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Concise Thoughts：仅通过长度限制，获取更简明的CoT能大幅度提升效果]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaDVwQm4MoPzagh9RFKrHOFDWBuAiae3dKy2IicKEe5ZDEzhcsrS8lfjwJU7TNmjNibEffTsd3Wic7WVg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost地址：https://arxiv.org/pdf/2407.</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529318&amp;idx=1&amp;sn=ab30fd1618261cdd15e513025bb13c6e&amp;chksm=ea024875b56d743e8d91c49990bb87b3f7c49ee592d09197f4992660df6667c865b0624d6fa5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 30 Aug 2024 11:59:02 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[校招生做大模型，选预训练还是SFT？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaDVwQm4MoPzagh9RFKrHOFlKibSiaJAeYxl02WJ87rcZ7sbQBVAj1KA5SmLZhNrvsjQDjkvVmQukAg/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎: ybq链接: https://www.zhihu.com/question/635761315/answer/3608088928我推荐选 pretrain，理由如下：pretrain 提高工</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529318&amp;idx=2&amp;sn=a861858641f35f356f109f4ea8d8ac87&amp;chksm=ea5835d57ac335d91f0ac1de7fd5087750d6272d97d2d685562fb7c685222f95a9f9745d0fb1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 30 Aug 2024 11:59:02 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[情感分析的终极形态：全景式细粒度多模态对话情感分析基准PanoSent]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaDVwQm4MoPzagh9RFKrHOF4Hw8y9uJXo48Ju6Uic7PTpFicQzvbSTowbmX0WjAAib4HFWnPElJibkMoQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：机器之心罗盟，本工作的第一作者。新加坡国立大学（NUS）人工智能专业准博士生，本科毕业于武汉大学。主要研究方向为多模态大语言模型和 Social AI、Human-eccentric AI。‍情</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529318&amp;idx=3&amp;sn=966e58907a6a84f7fad9aba8289f4771&amp;chksm=ea30814a66c76c9359858eca4603bc9c83d145727febaccc7baaaa570a7866d6fc78ec484595&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 30 Aug 2024 11:59:02 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OpenRLHF：大规模分布式RLHF训练系统介绍]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagF7vOVhF7zsTulJhE1yuQqibZkN63WpibrfPMg7ZI8QZfQkLHHbicM6qskdibIE2ILFcyAM7ZWOuuASQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>主题大规模分布式RLHF训练系统介绍时间2024.9.1 10:30-11:30 周日入群大纲1. RLHF背景知识2. RLHF性能分析3. 基于DeepSpeed的TRLX/TRL/LMF4. 基</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529101&amp;idx=1&amp;sn=dafb1342af7932b5c1153b72a9383949&amp;chksm=ea093157743e62ec282c18b8f68161a4eba40f06844f2d615976fbe417a24b7cc6d3529bf062&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 07:27:41 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[如何看待 Show-o 和 Transfusion 等 AR+Diffusion 的工作？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagF7vOVhF7zsTulJhE1yuQq3ZTRaV3b8nNyDXdJ0BMVUJruhibwGCQ0uHibddn8GyicdYkMrpACuotiag/300?wxtype=jpeg&amp;wxfrom=0"/><p>Show-o大致如下：作者：Mike Shou链接：https://www.zhihu.com/question/665151133/answer/3608387516来源：知乎好久没来知乎了，简单总</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529101&amp;idx=2&amp;sn=62b96defafc7a89a2a336a855d66e1dd&amp;chksm=ea4abd5fbac4b4ef5af83693f46491bd2db5b8d417a7a5713e5279e973398b951ab8054b1e3e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 07:27:41 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[微解读 | 到底要不要使用Code？探索Code对pre-training的影响]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagF7vOVhF7zsTulJhE1yuQq78IEhicMo3j5lZKibonoicTRBEKxU0fq7Iae2xHmsaxFAtDRsQZVsTXBQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：李磊 香港大学To Code, or Not To Code?Exploring Impact of Code in Pre-traininghttps://arxiv.org/pdf/2408</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529101&amp;idx=3&amp;sn=0d3fcf8ae500b94799649cca2953a4e2&amp;chksm=ea06aa77370b4800bd3df15bb37587ed1865c8d714ddb36b492d0faa5b5179d26b465e660b16&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 29 Aug 2024 07:27:41 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[当心环境内容的干扰：大模型GUI智能体的忠实度研究]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahPP2qia6PeP50RxGWyJ0xIw8l0eIh2OoPL6S1wYA4rfBHGBzB8UwBwkZj4peAqla3Hj1td4MGQJdQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>主题当心环境内容的干扰：大模型GUI智能体的忠实度研究时间2024.8.31 10:30-11:30 周六入群论文：Caution for the Environment:
Multimodal Ag</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529076&amp;idx=1&amp;sn=587a7e753aa7b7350c0a4586b85e6a9d&amp;chksm=ea70e0657cbf77ef4fabf24aabc9c96164b945c3973b0cccac9da0da892c973fee1d8389f24e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 28 Aug 2024 06:37:36 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GLM-4-Flash竟然免费了，还可免费微调...  让其他公司咋玩]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaoLxCmibLGCicJHacBiaVibAgC6PYktA933tg7sMukWSLH5K2Jbz64Lwq56qiaDwCK4maFELN92jCBSibw/640?wxtype=jpeg&amp;wxfrom=0"/><p>NLP开发者的又一福音！！8月27日，智谱AI BigModel开放平台宣布：GLM-4-Flash 全部免费，同时开启了GLM-4-Flash 限时免费微调活动。值得注意的是，GLM-4-flash</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529067&amp;idx=1&amp;sn=eb6316de6f0d2f97fa68ad565c0923a2&amp;chksm=ea8d621d94d0eedc8e2a5ab49e0a6ea7064ccf29c4be9643145621c9478f78a978fa4cbb9fa2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 09:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[以DeepSeek-VL为例，详解视觉语言模型原理及代码]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baghRaTqb1LSylABzStpsklUCnVpW9M9GPA0wvbYIKjpzfDicB3Me0rJ04om6zHskLiaq0KhwnmEibibfw/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：炼钢AI 最近开始看看视觉语言模型（VLM）相关的东西了，之前没特别仔细看过代码。翻了几篇比较知名的开源VLM技术报告，感觉DeepSeek-VL算是写的比较好的，因此本文就以DeepSeek-</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529067&amp;idx=2&amp;sn=29be5db70aaf3c9514b77bced4f88eba&amp;chksm=eae81c76e67cd6039a126466f7cc057637d7c7a8e8a27dbe192b79d3dab301efa1d5598b3807&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 09:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Yann LeCun不看好强化学习：「我确实更喜欢 MPC」]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baghRaTqb1LSylABzStpsklUyt7LUdZZTM3ApDKhMtX6Via6t0hWUKIVPdvyuHeOly9nequL84tHyDg/300?wxtype=jpeg&amp;wxfrom=0"/><p>机器之心报道编辑：张倩、小舟五十多年前的理论还值得再研究一下？  「相比于强化学习（RL），我确实更喜欢模型预测控制（MPC）。至少从 2016 年起，我就一直在强调这一点。强化学习在学习任何新任务时</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529067&amp;idx=3&amp;sn=b6305de1b20d45612d91be52f24e1c3c&amp;chksm=ea597b0ea8acae416e3acad1a04e183795ca2d406cd35c6a71598cfce08b4d200d4618052386&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 09:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[MIT研究：LLM对世界模型的探索]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baghRaTqb1LSylABzStpsklUFsr7Gqj9blYS2z07Gf7FzbBP2zvcpm1VRtydG0VKYiabgertUZDUrHg/300?wxtype=jpeg&amp;wxfrom=0"/><p>本文转自新智元【导读】MIT CSAIL的研究人员发现，LLM的「内心深处」已经发展出了对现实的模拟，模型对语言和世界的理解，绝不仅仅是简单的「鹦鹉学舌」。也就说，在未来，LLM会比今天更深层地理解语</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529067&amp;idx=4&amp;sn=403a018b80d07fcbc56730b556a43fa7&amp;chksm=ea260c5d2a5fc7d058013b69ff668d66fdc4220ed80d2f9652de627b3bcbbf01f0ab2fb0c725&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 27 Aug 2024 09:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[科研工作者该如何平衡科研和生活？ - 有个p的平衡...]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaoLxCmibLGCicJHacBiaVibAgCetMZA4VdcDRpDIHoXcTblCDEtTMnayTUYWKCGvpSciantHZrR8FwI1A/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者：柯烟链接：https://www.zhihu.com/question/652294237/answer/3496229608来源：知乎有个p的平衡，你甚至没有选择，你只能all in科研。我刷</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529027&amp;idx=1&amp;sn=bd59247ffd38883be1f73281122febc7&amp;chksm=eacade80af2a74ccad0186666f69dde2142dd67b67d98523d4305ee84633491f4fe528cd46aa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 26 Aug 2024 15:04:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从token到patch，一种LLM加速训练策略]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaoLxCmibLGCicJHacBiaVibAgCC5GSvZ57IYVCjTfYAYKM6aDMMia0RF0LuLIwlEbZzfltwRqE73G0ojQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>1前言来自：炼钢AI    此篇文章出自论文《Patch-Level Training for Large Language Models》，主要思路非常简单，就是把相邻的token embeddin</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529027&amp;idx=2&amp;sn=e1759af8503f42cd0701e7c5e5d6f19a&amp;chksm=ea904a05258850816ed88afb00ab684ce13903563db604a66d21e7dd65355bbb57cb20d73762&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 26 Aug 2024 15:04:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图文跨模态检索研究综述]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaoLxCmibLGCicJHacBiaVibAgCqykfxmnxD0AyPB8VRvvxWhoKclEFglJQ7gdkeEJX2mUHHY05U1B2hw/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：张振兴，王亚雄来源：《北京交通大学学报》编辑：陈萍萍的公主@一点人工一点智能原文：https://jdxb.bjtu.edu.cn/CN/10.11860/j.issn.1673-0291.20</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529027&amp;idx=3&amp;sn=9c4bcc58fe2578a465759280aee5b188&amp;chksm=ea26348c73ccd2272523da482acc90395b2cadafe57c335ec6ecd9ce7897c86582dcb7ef5464&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 26 Aug 2024 15:04:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[长文本 Embedding 模型中的“迟分”策略]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaoLxCmibLGCicJHacBiaVibAgCwvibPVm2Od44ibqibbQ55g9d5grbBl1OUib2Jk1IjEc2ibO59ibEeFpCOXlw/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：Jina AI大约一年前，2023 年 10 月，我们推出了全球首个支持 8K 上下文长度的开源 Embedding 模型 —— jina-embeddings-v2-base-en。自此，长文</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529027&amp;idx=4&amp;sn=1db3a7b6c0003625859ed861844352a5&amp;chksm=ea11fa1b1c1a9ffc71895b5be8c18ae0e0e979cf2034795bafb2755ff46d3ac12c389d24d880&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 26 Aug 2024 15:04:27 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
