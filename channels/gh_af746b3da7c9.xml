<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    






















    <item>
      <title><![CDATA[扩展词表是必须的吗？中文Mixtral实践与分析]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxiasZSrce1NIx6VCUZbXm1pWUvDbGpib97ONoqFHBqY3zLNjibKOsdleMA/640?wxtype=jpeg&amp;wxfrom=0"/><p>来自：HFL实验室Mixtral混合专家大模型受到了广泛关注。在本文中，我们基于Mixtral模型训练出中文Mixtral和中文Mixtral-Instruct，相关资源已对外开源。接下来本文重点探讨</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526826&amp;idx=1&amp;sn=038f4718c4c6359749fca37f813f79d4&amp;chksm=ead5a0841ba1962c9e5aee9d0b7079d6f5d70f671928afbdaae57470ee919e2d80f63d820492&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 21 Mar 2024 14:15:10 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[角色扮演大模型的碎碎念]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxSyqQelgFoMJIHg8N29Lat7y2VZ0HQzdk0Tbzl8RwiaEEibxEv3IMk5jw/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：NLP工作站今天给大家带来一篇角色大模型相关思考的文章，来自知乎@快乐子涵酱（已授权）。知乎：https://zhuanlan.zhihu.com/p/685823865什么是角色扮演大模型？首</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526826&amp;idx=2&amp;sn=74b88db04a5e487437f29ae0db7cdcdd&amp;chksm=ea3f5bed632c2bc03d3618a380bbcf53ca1616a5055850ccee4f08057682598f6f7fcf42db25&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 21 Mar 2024 14:15:10 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | TeaMs-RL: 通过强化学习让LLM自己学会更好的指令]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagnXA6l6APicbEuI9VoDt3S6vrYtDDCfnAcd5UYUwK4oxmrvBPiaHUvdhjvZyvv2P0leicA1sUsXuvYw/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：大语言模型（LLM）的开发经常面临挑战，这些挑战源于强化学习与人类反馈（RLHF）框架中对人类注释者的严重依赖，或与自我指导范式相关的频繁而昂贵的外部查询。在</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526826&amp;idx=3&amp;sn=af88e754f414830563676c2d0c842da2&amp;chksm=eac545ad1dc375410e333f9a5aecd0ca4958ecc3275edd2f70de187de0cddbf395d5ae66acc8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 21 Mar 2024 14:15:10 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | BCT: 偏见增强一致性训练缓解CoT中的偏见问题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagnXA6l6APicbEuI9VoDt3S68vfCX2b3TFpsQyaFs68QMtcQYnGgSsbmLa3OAuLCUibMbxgdClO87Ig/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：虽然CoT有可能提高语言模型推理的可解释性，但它可能会系统性地误导影响模型行为的因素--例如，根据用户的意见合理化答案，而不提及这种偏见。为了缓解这种有偏差的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526826&amp;idx=4&amp;sn=95ef63f9a767849124bfe95625c177d3&amp;chksm=ea3817380b9b8f768b93db0c7b3dc4b18697f8267e62652398304a29cb8fb3d7c2e82472f633&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 21 Mar 2024 14:15:10 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[每日论文速递 | DMC: 动态内存压缩-在推理时压缩KV Cache]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagnXA6l6APicbEuI9VoDt3S68vfCX2b3TFpsQyaFs68QMtcQYnGgSsbmLa3OAuLCUibMbxgdClO87Ig/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：Transformers已成为大型语言模型（LLM）的支柱。然而，由于需要在内存中存储过去标记的键值表示缓存，其大小与输入序列长度和批量大小成线性比例，因此生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526826&amp;idx=5&amp;sn=3a020fe1b2ec429e1118f8ae5b168adc&amp;chksm=ea647287a8658b2fe3272a416e8c2cf09380bc6abc5fc6b4b9faad8d3a63fc731aba068d34df&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 21 Mar 2024 14:15:10 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[从0开始训练1.4b中文大模型的直播经验分享 | NICE十一期]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia35yu4lyaAN2NYIuuAM0e59nlDYLWbXicA1cSfZuZWkbX4OwrdvtDib0ICyCY1Ek5dd204hdPCjXQw/640?wxtype=jpeg&amp;wxfrom=0"/><p>主题从0开始训练1.4b中文大模型的经验分享[1]个人简介黎健进（知乎：Lil2J）23年在华南师范大学软件工程毕业的一名硕士研究生，在校期间，我专注于自然语言处理（NLP）领域的研究，致力于探索语言</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526778&amp;idx=1&amp;sn=4d4288ce6e4d029c07e6b6d5c756c206&amp;chksm=ea0dfe94861f30a905273147cbeab317df015b4e0d25d143468ec65492e60f6f014f7add0031&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Mar 2024 13:51:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | AutoLoRA:通过meta learning学习LoRA最优秩]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxUDOv2981jHvd77dhCRTdnZcOpVjfj3gfbnyK8XyMnOvT781EPK238Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：在各种 NLP 任务中，大规模预训练和针对特定任务的微调取得了巨大成功。由于对大型预训练模型的所有参数进行微调会带来巨大的计算和内存挑战，人们开发出了几种高效</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526778&amp;idx=2&amp;sn=6db1186da9b4148307a993e6f678dc08&amp;chksm=eac0cf9205bb8dcce409d8b34acddb0dcafdfa4370ce5b1754e0d7fdf41cecd9dcc3c2324725&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Mar 2024 13:51:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 李飞飞领衔建立具身AI最新数据集BEHAVIOR-1K]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxUDOv2981jHvd77dhCRTdnZcOpVjfj3gfbnyK8XyMnOvT781EPK238Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：我们推出的 BEHAVIOR-1K 是以人为中心的机器人技术综合模拟基准。BEHAVIOR-1K 包括两个部分，由 "您希望机器人为您做什么？"的广泛调查结果</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526778&amp;idx=3&amp;sn=ab52820c8b81871d92018639bf07bfb2&amp;chksm=ea7ef119c87a4f6c0ece8cd8bac52981b6c9af45fcbf8417b5e44a3d0a490e3c5d6e2ca4f503&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Mar 2024 13:51:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | ALARM:通过分级Reward对齐LLM]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxUDOv2981jHvd77dhCRTdnZcOpVjfj3gfbnyK8XyMnOvT781EPK238Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：我们介绍了 ALaRM，它是第一个在人类反馈强化学习（RLHF）中模拟分层奖励的框架，旨在增强大语言模型（LLM）与人类偏好的一致性。该框架通过将整体奖励与特</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526778&amp;idx=4&amp;sn=0f92bcf0bb124652617c18da5a0b7a46&amp;chksm=ea44c804c51dc050bcba8158003e684aca8e7e23a8f9c685a4f1e74705e4e4c8570c167e7de7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Mar 2024 13:51:27 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[SCI征稿！这些期刊，录用最快1-2月！香爆了！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxV03YGwZDXsryC7y879GkdibPPxK5S14sLwKyGHq1Zx7JkF17EqOeOHg/640?wxtype=jpeg&amp;wxfrom=0"/><p>开学了，很多朋友的SCI写作计划已经提上了日程！SCI的征程，不仅是写论文的过程，更是一个期刊投稿与论文发表的过程，在文章完成之前，一定要对SCI期刊和投稿过程有一定的了解。但是，这么多期刊，甚至不同</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526745&amp;idx=1&amp;sn=b7a6c4309445323e4357d19632704017&amp;chksm=eac0edc2995e701bd59c67dd10446d121622fd786d6131c2986aa8c63f086259125103db8a54&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 12:43:33 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | Next Token Prediction 陷阱]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxUDOv2981jHvd77dhCRTdnZcOpVjfj3gfbnyK8XyMnOvT781EPK238Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：单纯的下一个next-token predictor能否真正地模拟人类智能？我们将这一文献中支离破碎的直观问题具体化。作为出发点，我们认为必须区别对待下一个标</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526745&amp;idx=2&amp;sn=dd32529c85028177c1b73826a08bec7f&amp;chksm=eac83c032748fc9fcdd205c19a50fbd85db4c9b82c1aa7eaa04db4797fc46e5a1a151dfbb93a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 12:43:33 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 使用对比Reward改进RLHF]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxUDOv2981jHvd77dhCRTdnZcOpVjfj3gfbnyK8XyMnOvT781EPK238Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：来自人类反馈的强化学习（RLHF）是将大语言模型（LLM）与人类偏好相匹配的主流范式。然而，现有的 RLHF 在很大程度上依赖于准确、翔实的奖励模型，而奖励模</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526745&amp;idx=3&amp;sn=c4ccef140a34b097fb230d0a290c5cd2&amp;chksm=ea04e38d4c5b4425d3ac46a5d9bd9ad63574ae8b3cf02f452084bbd7e1abe3fc7a413c01073d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 12:43:33 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | DeepMind提出在线偏好对齐新方法：IPO-MD]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bag8NaOXicl7iaZPic50DD7PflxUDOv2981jHvd77dhCRTdnZcOpVjfj3gfbnyK8XyMnOvT781EPK238Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：确保语言模型的输出与人类偏好相一致，对于保证有用、安全和愉快的用户体验至关重要。因此，近来人们对人类对齐问题进行了广泛研究，并出现了一些方法，如人类反馈强化学</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526745&amp;idx=4&amp;sn=740b9f320a54e4f44094144b4511c638&amp;chksm=ea27eaa3dc0b4b07e3495decdadaa9130bb06aecc6daca5034b1fc12a0bfaecb16a42fc22d13&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 19 Mar 2024 12:43:33 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型推理：A100/H100 太贵，何不用 4090？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagjCouMeAEPjZ1xhuth5bqOZoRv7H9mK4wxDdnnHTqWSukc9uXwzQia5HvaBx0ryMwicq8fOcg5U75A/640?wxtype=jpeg&amp;wxfrom=0"/><p>作者：李博杰， Logenic AI 联合创始人、中科大与MSRA联培计算机博士、华为天才少年主页：https://01.me/声明：本文只做分享，版权归原作者，侵权私信删除！https://zhua</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526700&amp;idx=1&amp;sn=a2798afa7c48c786883eef6bf92dc0ff&amp;chksm=eab16e905028620a6f9d05b5a08a9b4c4ffebca857c69166603290ede125ef547048c694f4b8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Mar 2024 14:00:45 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[书生·浦语大模型实战营第二期正式启动，内容全面升级！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagjCouMeAEPjZ1xhuth5bqOSgPewy6HJiaW5TdaJl2FTR9lh9cRRzv7ggQNu28t494rgUraXsqDeuA/300?wxtype=jpeg&amp;wxfrom=0"/><p>为了帮助社区用户高效掌握和广泛应用大模型技术，我们重磅推出书生·浦语大模型实战营系列活动，旨在为开发者们提供全面而系统的大模型技术学习课程，并建立一个友好的交流平台，便于大家在大模型实践开发中分享经验</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526700&amp;idx=2&amp;sn=3c60fb5a1daf16bc539f3624e322fef7&amp;chksm=ead82660d0e5c3c8ac9c8a429fbd2c77e469ae65e73ef731c6097ed8fbfee3f6440cfeed9503&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Mar 2024 14:00:45 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 苹果发文：VLMs离视觉演绎推理还有多远]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDY5icLZxqHW8K0O6J0kwBv5aaVnia4TpyyZKQaghGMnRuPNzj61ZkjoqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：最近，GPT-4V 等视觉语言模型（VLM）在各种视觉语言任务中取得了令人难以置信的进步。我们深入研究了基于视觉的演绎推理这一更为复杂但探索较少的领域，并发现</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526700&amp;idx=3&amp;sn=96ce3f748dbc7df4216f5e7b1d56d68d&amp;chksm=eaf1d7019562c835be207d3cea7351816f3a7e62143c682b609e3177027d5af49b7aa0e00647&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Mar 2024 14:00:45 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | Meta提出Branch-Train-Mix 混合专家大模型训练方法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDY5icLZxqHW8K0O6J0kwBv5aaVnia4TpyyZKQaghGMnRuPNzj61ZkjoqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：我们研究了训练大语言模型（LLM）的高效方法，使其具备多个专业领域的能力，如coding、数学推理和世界知识。我们的方法被命名为 "分支-训练-混合Branc</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526700&amp;idx=4&amp;sn=3d20715a2a8d74ca3f799023bc0131a6&amp;chksm=ea12c210798f1625456ae9c6fcdb76f90a99b6d19b88b2cde22c7939b720386e2a77bab153df&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Mar 2024 14:00:45 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | GEAR:高效 KV Cache 压缩框架]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDY5icLZxqHW8K0O6J0kwBv5aaVnia4TpyyZKQaghGMnRuPNzj61ZkjoqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：键值（KV）缓存已成为加快大语言模型（LLM）推理生成速度的事实。然而，随着序列长度的增加，缓存需求也在不断增长，这使得 LLM 推理变成了一个内存约束问题，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526700&amp;idx=5&amp;sn=f2ae434b7d275341b83a13cbd624c610&amp;chksm=eaac5d123d3343628ae55d0c42ac67a160f1c3d1ed534a539570953b0cdba150c25017a44952&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Mar 2024 14:00:45 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【原创】一文读懂RAG的来源、发展和前沿]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDOU2fvfbCgEUPghd6Vicj7qtFqy5RMLjUbic573G2icz8PfapSlHz0yIpA/640?wxtype=jpeg&amp;wxfrom=0"/><p>检索增强生成(Retrieval Augmented Generation，RAG)结合了检索 (Retrieval) 和生成 (Generation) 两个过程，旨在提高机器生成文本的相关性、准确性</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526630&amp;idx=1&amp;sn=a4f18fba5d836ffdba2fe76d4b18f9ae&amp;chksm=ead657a32fe18157d77efb6beae4f89376d13ee0677b1a7adfc77b7f0bd50ba4583e850e009a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 15 Mar 2024 10:02:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 陈丹琦新作：启发式核心-理解PLM子网络]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDY5icLZxqHW8K0O6J0kwBv5aaVnia4TpyyZKQaghGMnRuPNzj61ZkjoqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：之前的研究发现，使用不同随机种子进行微调的预训练语言模型（LMs）可以获得相似的域内性能，但在句法泛化测试中的泛化效果却大相径庭。在这项研究中，我们发现即使在</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526630&amp;idx=2&amp;sn=2152c2c0ebd07b11f553183d9689dd16&amp;chksm=ea3cdc53997428e296a00d7cacf6738d25e39a456ce5d55db21a9a8e11fd5d8b65a2ceebb135&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 15 Mar 2024 10:02:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | InterrogateLLM: 大模型幻觉检测框架]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDY5icLZxqHW8K0O6J0kwBv5aaVnia4TpyyZKQaghGMnRuPNzj61ZkjoqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：尽管大语言模型（LLMs）取得了许多进步，并以前所未有的速度迅速发展，但由于种种原因，它们对我们日常生活方方面面的影响和整合仍然有限。阻碍其广泛应用的一个关键</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526630&amp;idx=3&amp;sn=dc88d991312cd9185d3fdce88d3de006&amp;chksm=eaed8c92b995a1f2a1987a5acd65ca0f68135c180c9c33e1a5d55142ed927ae4837649fb34e5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 15 Mar 2024 10:02:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | IntactKV: 用Pivot token进行无损量化的方法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaaNF6dCVvryDiaWXTxq64gDY5icLZxqHW8K0O6J0kwBv5aaVnia4TpyyZKQaghGMnRuPNzj61ZkjoqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：大语言模型（LLM）在自然语言处理中表现出色，但需要密集的计算。为了缓解这一问题，人们探索了各种量化方法，但这些方法都会影响 LLM 的性能。本文揭示了 LL</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247526630&amp;idx=4&amp;sn=783973e2a2d4d37f2a9537e48a18275f&amp;chksm=eab13b5876543bd644ebf55775591efaca887dd5449d08b73a1e855658debea4146fbe8079c7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 15 Mar 2024 10:02:16 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
