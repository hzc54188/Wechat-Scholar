<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    




















    <item>
      <title><![CDATA[技术上，如何复现 o1?]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagGZenX4LXy3M8yoliboFIq47tFTI2cOzXB0SiaguXcKfTb7pfU0NZric8iaEqw2NUD8pOwRjgZ8ia1xBA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：周舒畅链接：https://zhuanlan.zhihu.com/p/720127190基础模型搞 o1 首先需要一个基模，这个基模必须是：能进行“长”生成。注意这和“长 context”不是一</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530330&amp;idx=1&amp;sn=243a8ae021c7252d644a2b8aa56cf481&amp;chksm=eac5d6c78f789052b34a71811accf35f5e6d4b4d9e8add81658615d59f09094eb34cb191e90d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 15 Sep 2024 13:22:55 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[OpenAI o1背后的技术：LLM的快思考与慢思考路线之MCTS]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagGZenX4LXy3M8yoliboFIq4UUJzJoxgfP0Fxuic0d9JC3drUzY8QYib2MQDuHRAtZk4e8y58UkSiagHQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：皓天 链接：https://zhuanlan.zhihu.com/p/659230417在上一篇文章[1]中，我们初步探索了基于EBM-MCTS的方法，并在多个数学数据集上完成实验验证。相比使用</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530330&amp;idx=2&amp;sn=073985546d01e00231278970ca43b1c3&amp;chksm=ea84e3c9061117516838fcf0a84c610a6a8ce903a87aca61edb059944649c2aafd011b1ece5a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 15 Sep 2024 13:22:55 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[2025智谱AI校园招聘正式启动！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagGZenX4LXy3M8yoliboFIq4a7vMNqEaGxBN7S7CeYhozzeJibdj5xoAb2k74I4DEUA2HES2ribQicGhg/300?wxtype=jpeg&amp;wxfrom=0"/><p></p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530330&amp;idx=3&amp;sn=7362f2bc6293855289eb172edbb82a53&amp;chksm=eab0cfb60daf15300b657d2eb35b6d44c5a44853922005f00b245cb2ee641e7033bbafe5ab5a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 15 Sep 2024 13:22:55 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[如何用1024张显卡训练一个模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah9yWXAm1bx4425eIiaUwIiaiaTkzKCzuWC9rI609Kst9S5OMSFMRJXyibNPDOY8IJW5Jd9YHppUEZAtw/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：你的真实姓名链接：https://www.zhihu.com/question/650979052最近看到知乎一个回答，把千卡训练的难度吹上天了。但其实真正用过千卡就会发现也就那么几个点。于是想</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530294&amp;idx=1&amp;sn=15cfd3cbc240394bec555231c4f991d8&amp;chksm=ea4606c1d9776b5be562e1895eeff98b96cd6b7c7eb2afdd21cb187d1317d720b93be5483b0d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 14 Sep 2024 14:03:44 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[黄哲威与丁霄汉为初学者撰写AI会议论文写作手册，独具一格！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah9yWXAm1bx4425eIiaUwIiaiaiaBjvWq8LUxvgqmJlZlpStapdpBM7x5Y7DfJQu985XOalBGo7TXhMKw/300?wxtype=jpeg&amp;wxfrom=0"/><p> 作者: 黄哲威、丁霄汉知乎：https://www.zhihu.com/question/438031462/answer/3624649447链接：https://github.com/hzwer</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530294&amp;idx=2&amp;sn=6d9419cb01cc9296990623f9f205d2ad&amp;chksm=eaeb68f117b0a480605d5568d851269a19f865126086d626565a40f1c0f199e86e9c71bea568&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 14 Sep 2024 14:03:44 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[RWKV作者对OpenAI 发布 o1 系列模型的看法，很深刻]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahc1TuMYrLVjlQwkfY7LK4TWiateDR5bGRUqtrE1I2OC5j32AJxE4cYdTrKxGwv1vBNmicQnQe9wDKA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：PENG Bo链接：https://www.zhihu.com/question/666991594/answer/3624168868大家都知道长期CoT可以提升性能，而且很快我们会看到其它家</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530267&amp;idx=1&amp;sn=f4af702820978a91f17c4be7d35988b7&amp;chksm=ea664fccb2c72032b85b7b8dc263f311277c44f056bc4a86315d4b0b9571013464fee913802b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 13 Sep 2024 04:18:32 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[小模型在大型语言模型时代的角色：一项全面调查]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahc1TuMYrLVjlQwkfY7LK4TDaibr8ibgbStr5VzLjTqkqgyp7Oicj3gvicCt6eiaeACgMYY9N6icCpwqkjg/300?wxtype=jpeg&amp;wxfrom=0"/><p>这篇论文探讨了在大型语言模型（LLMs）时代小型模型（SMs）的角色，特别是它们在协作和竞争中的表现。论文：What is the Role of Small Models in the LLM Er</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530267&amp;idx=2&amp;sn=c31c1fec72c3bb62dfa598263bb568c2&amp;chksm=ea0c3d8bffb7f3b18e843aeeef27ebb01864c57cb81e71116a9559d69a9c6dfab71c6b98fe08&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 13 Sep 2024 04:18:32 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2万字的SFT for Alignment 总结纪要]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiafc1RFGJeZhbYvLsCcpcFM8JqFEAWibGnNGPKLYqxtICxrO63Dczq2qBl0jtMZClMNYEFkbAlcCDQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：张峻旗链接：https://zhuanlan.zhihu.com/p/717553974本文是个人大模型学习笔记的第二十五篇，以18K再次刷新了单篇字符记录，感兴趣的话可以点击专栏阅读其余笔记，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530233&amp;idx=1&amp;sn=d76435f1541362dc7df3d77f2b56dc18&amp;chksm=ea0694f6d1068a42e6a8fc3ab380f645ffa5f12a56e4a7590aac190b62d0fcfd64b55fe38843&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 12 Sep 2024 15:05:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[中科院提出GPT-4o实时语音交互的开源对手：Llama-Omni]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiafc1RFGJeZhbYvLsCcpcFMVg8vrcmb34fURUWYibmWVutJRt5AYmvKR2tRdedRusatklCJ7VyC2bQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：LLaMA-Omni: Seamless Speech Interaction with Large Language Models地址：https://arxiv.org/pdf/2409.0</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530233&amp;idx=2&amp;sn=231c7b5f293ddd570e0ca3f102d8907f&amp;chksm=ea4f80dd2e4f816f8a7448283a6b66a221dbe733bfb6a705fbe7c36720cc2e85162fdd2174c8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 12 Sep 2024 15:05:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[研究表明，LLMs的幻觉问题是我们永远无法逃避的...]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiafc1RFGJeZhbYvLsCcpcFMvBUicggtY0C2whDBbjSS0uyUbMALTfebSib89ECtkmPfjB3KkFfI181A/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：LLMs Will Always Hallucinate, and We Need to Live With This地址：https://arxiv.org/abs/2409.05746研究背</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530233&amp;idx=3&amp;sn=a2148b4e87c9018df8561bade19d5bd2&amp;chksm=eac43c44490b6a848e6719c67ceec8645026d836fece3d62c7b7c37d2d9e5261f3ba90c25024&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 12 Sep 2024 15:05:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[探究大模型微调 Lora 的不同形态(上篇): AdaLora、 AsLora、 PiSSA、 DoRA]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3FW4qrWQibzyEib2vGkNibkQFfx7HlCkZNgMibohyfJ06RZIOmBuHch6fe4g/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：周星星链接：https://zhuanlan.zhihu.com/p/719438707排版：AI椰青@深度学习自然语言处理 公众号前言最近本人一直在研究 SFT 的落地工作，其中 LoRA 是</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=1&amp;sn=e61e2f3a3f64e075a94824a0c30f73c3&amp;chksm=eab5dc6bee13e9ede0e52ec9e5be505420323b1ccb0cb9efb924447cd07d8eb0d057bbad6501&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[AnyGPT | 基于离散表示统一多模态理解与生成：把一种新模态当作一门外语 -- NICE27期]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3FGT1OkdPwzgEJKPEiaQO5ibk2qnB8GwicPzSkTkAuhUCDBwfTj3dnrbUjA/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题基于离散表示统一多模态理解与生成：把一种新模态当作一门外语时间2024.9.14 20:00-21:00 周六入群论文：AnyGPT: Unified Multimodal LLM with Di</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=2&amp;sn=4076ce39610f66d44afec4115f0a4413&amp;chksm=eae45b90731034ea50dff645548bcefae18c86129c81591ed1dece3052a0c6380a9392ca4181&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[“晚期分块”：用长上下文嵌入模型拯救文本检索]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3F5kArxSSjpCmgCZRibvt2AWK8jR7A8vzbaCZB1iabz87cEJRnBPmGtctw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Late Chunking: Contextual Chunk Embeddings Using Long-Context
Embedding Models地址：https://arxiv.or</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=3&amp;sn=e71a090979cbf36c86bd62e054c2e981&amp;chksm=ea0ec863a2a5ee64faa97632e701b3f207de155364e284e86314c795499233972acdab5d22e2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Sirius：一种高效的上下文稀疏性校正方法，恢复稀疏模型在推理任务上的性能]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3FwT441jyWr08n4kldUKU64nBQulqXJdWmFETmpLLU8tZWPtqs9iabo2w/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Sirius: Contextual Sparsity with Correction for Efficient LLMs地址：https://www.arxiv.org/abs/2409.0</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=4&amp;sn=ab78bc470146524ec3761d9e8c31c298&amp;chksm=ea60c3909ece491de7ade3fc15d54d6865c9f05f3248e3050e588bc843914d88dcef33958d27&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[中科院提出大模型“基准泄露”排行榜，Qwen模型位居榜首]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagzSibbmeLOLFXzP6J5pZkiby2pN9bQD0PTMqOcneqUr2AZUgvVeKZ9jSrG1C9nYtVxiaWmceUG6Lbdw/640?wxtype=jpeg&amp;wxfrom=0"/><p>编辑&amp;整理：深度学习自然语言处理 公众号近期，大规模语言模型在多个自然语言处理的基准测试中取得了显著的进展。这些模型之所以能够取得成功，部分原因在于它们通过对互联网上收集的庞大语料库进行广泛的预训练。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529789&amp;idx=1&amp;sn=2ec2dd35a7174f984c054c4ceb68e3f4&amp;chksm=eaad8f43bd1c73cea215cd6aa96fc7d2ed94a7980f61eb3d7f3915942ca45ecaccc018a97053&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Sep 2024 09:27:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[DPO，RM，RLHF 傻傻分不清楚]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagzSibbmeLOLFXzP6J5pZkibyOibctBMMYAo6He7EwhyJvxVndgrdcVlwCjC0eRxZY7Gcljso8w6UFyQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：曹宇链接：https://zhuanlan.zhihu.com/p/718913850编辑：AI椰青 | 深度学习自然语言处理 公众号纯学术分享，侵删DPO 的论文引用最近已经破千了，成了斯坦福</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529789&amp;idx=2&amp;sn=535a178e6e3da1b347924e58abe435d3&amp;chksm=eab40b35d51d2f170f8d970351fcfd6de0335f01d18e698ee5c987cec9d1bf12365c030fe25f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Sep 2024 09:27:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLMs 能够生成创新的研究想法吗？——一项针对 100 多位 NLP 研究者的大规模人类研究]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagzSibbmeLOLFXzP6J5pZkibyOvTonicJ5w7Cibe1pk6pIqFwia5z3mjgWdRIqRRTn900DokEXGibQBNgYw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers地址：htt</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529789&amp;idx=3&amp;sn=40cd15e464510e8bb8997a695e3369f8&amp;chksm=eaa1f9c0e9f24debc5ba1856f698c3f8ca7015ac0786a54418b68d2339a0fc3d9aeb78ac3ad8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Sep 2024 09:27:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2024年大模型Alignment偏好优化技术PPO,DPO, SimPO,KTO,Step-DPO, MCTS-DPO,SPO]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia9l9AcgGZtGgl2zib28uNApkRn3Wq4NB0M6aUeJsgwSC7iaA3f0TnrEpDibtYofOOmwuTt852xLqygw/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：是念链接：https://zhuanlan.zhihu.com/p/710021282学术分享，侵删今年做过一段时间的alignment工作，做得有点不开心，各种social的原因，觉得自己的发</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529712&amp;idx=1&amp;sn=141763b569a7fe274261545c6c25deb9&amp;chksm=ea6ce00b713ba37e9918c528b750aaa9a09e299487374388d444fbb212624eefcc7d790c2658&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Sep 2024 12:09:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[探索自然语言中的计划搜索：提升大型语言模型代码生成性能的新方法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia9l9AcgGZtGgl2zib28uNAp1uGMAvD60GVRq5EFVElVia1ziblec5IicUep6wZ5HjUNiaSdg4llccuYqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Planning In Natural Language Improves LLM Search For Code Generation 链接：https://arxiv.org/pdf/240</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529712&amp;idx=2&amp;sn=9636b9860ce3959aa37a335f93b7a6a1&amp;chksm=eafaa3d431cd9d03fe054738d831e26105a492abbf81170e0dadf1db504aa4bbf137b4dfafe3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Sep 2024 12:09:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[揭秘MagicDec：如何推测解码让长文本处理不再纠结于延迟与吞吐？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia9l9AcgGZtGgl2zib28uNApoFZib61IIxR6Ku93abAEmL5UzVYc4rGiakgicV769UCKXTnXRloGCYJicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：MagicDec-part2: Breaking the Latency-Throughput Tradeoff for Long Contexts with Speculative Decod</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529712&amp;idx=3&amp;sn=178cdbb2485c5e43f8376bf1f37eaf8e&amp;chksm=eaf9379a84965ee2a86e3bb3a52b667c666200e55aed6606665d62c94219e3b6bfdf3d815643&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Sep 2024 12:09:35 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
