<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    



















    <item>
      <title><![CDATA[2万字的SFT for Alignment 总结纪要]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiafc1RFGJeZhbYvLsCcpcFM8JqFEAWibGnNGPKLYqxtICxrO63Dczq2qBl0jtMZClMNYEFkbAlcCDQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：张峻旗链接：https://zhuanlan.zhihu.com/p/717553974本文是个人大模型学习笔记的第二十五篇，以18K再次刷新了单篇字符记录，感兴趣的话可以点击专栏阅读其余笔记，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530233&amp;idx=1&amp;sn=d76435f1541362dc7df3d77f2b56dc18&amp;chksm=ea0694f6d1068a42e6a8fc3ab380f645ffa5f12a56e4a7590aac190b62d0fcfd64b55fe38843&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 12 Sep 2024 15:05:16 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[中科院提出GPT-4o实时语音交互的开源对手：Llama-Omni]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiafc1RFGJeZhbYvLsCcpcFMVg8vrcmb34fURUWYibmWVutJRt5AYmvKR2tRdedRusatklCJ7VyC2bQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：LLaMA-Omni: Seamless Speech Interaction with Large Language Models地址：https://arxiv.org/pdf/2409.0</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530233&amp;idx=2&amp;sn=231c7b5f293ddd570e0ca3f102d8907f&amp;chksm=ea4f80dd2e4f816f8a7448283a6b66a221dbe733bfb6a705fbe7c36720cc2e85162fdd2174c8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 12 Sep 2024 15:05:16 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[研究表明，LLMs的幻觉问题是我们永远无法逃避的...]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiafc1RFGJeZhbYvLsCcpcFMvBUicggtY0C2whDBbjSS0uyUbMALTfebSib89ECtkmPfjB3KkFfI181A/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：LLMs Will Always Hallucinate, and We Need to Live With This地址：https://arxiv.org/abs/2409.05746研究背</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530233&amp;idx=3&amp;sn=a2148b4e87c9018df8561bade19d5bd2&amp;chksm=eac43c44490b6a848e6719c67ceec8645026d836fece3d62c7b7c37d2d9e5261f3ba90c25024&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 12 Sep 2024 15:05:16 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[探究大模型微调 Lora 的不同形态(上篇): AdaLora、 AsLora、 PiSSA、 DoRA]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3FW4qrWQibzyEib2vGkNibkQFfx7HlCkZNgMibohyfJ06RZIOmBuHch6fe4g/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：周星星链接：https://zhuanlan.zhihu.com/p/719438707排版：AI椰青@深度学习自然语言处理 公众号前言最近本人一直在研究 SFT 的落地工作，其中 LoRA 是</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=1&amp;sn=e61e2f3a3f64e075a94824a0c30f73c3&amp;chksm=eab5dc6bee13e9ede0e52ec9e5be505420323b1ccb0cb9efb924447cd07d8eb0d057bbad6501&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[AnyGPT | 基于离散表示统一多模态理解与生成：把一种新模态当作一门外语 -- NICE27期]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3FGT1OkdPwzgEJKPEiaQO5ibk2qnB8GwicPzSkTkAuhUCDBwfTj3dnrbUjA/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题基于离散表示统一多模态理解与生成：把一种新模态当作一门外语时间2024.9.14 20:00-21:00 周六入群论文：AnyGPT: Unified Multimodal LLM with Di</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=2&amp;sn=4076ce39610f66d44afec4115f0a4413&amp;chksm=eae45b90731034ea50dff645548bcefae18c86129c81591ed1dece3052a0c6380a9392ca4181&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[“晚期分块”：用长上下文嵌入模型拯救文本检索]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3F5kArxSSjpCmgCZRibvt2AWK8jR7A8vzbaCZB1iabz87cEJRnBPmGtctw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Late Chunking: Contextual Chunk Embeddings Using Long-Context
Embedding Models地址：https://arxiv.or</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=3&amp;sn=e71a090979cbf36c86bd62e054c2e981&amp;chksm=ea0ec863a2a5ee64faa97632e701b3f207de155364e284e86314c795499233972acdab5d22e2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Sirius：一种高效的上下文稀疏性校正方法，恢复稀疏模型在推理任务上的性能]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahxGzA2ppWZrRV8IEAYiaU3FwT441jyWr08n4kldUKU64nBQulqXJdWmFETmpLLU8tZWPtqs9iabo2w/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Sirius: Contextual Sparsity with Correction for Efficient LLMs地址：https://www.arxiv.org/abs/2409.0</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247530186&amp;idx=4&amp;sn=ab78bc470146524ec3761d9e8c31c298&amp;chksm=ea60c3909ece491de7ade3fc15d54d6865c9f05f3248e3050e588bc843914d88dcef33958d27&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Sep 2024 14:49:38 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[中科院提出大模型“基准泄露”排行榜，Qwen模型位居榜首]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagzSibbmeLOLFXzP6J5pZkiby2pN9bQD0PTMqOcneqUr2AZUgvVeKZ9jSrG1C9nYtVxiaWmceUG6Lbdw/640?wxtype=jpeg&amp;wxfrom=0"/><p>编辑&amp;整理：深度学习自然语言处理 公众号近期，大规模语言模型在多个自然语言处理的基准测试中取得了显著的进展。这些模型之所以能够取得成功，部分原因在于它们通过对互联网上收集的庞大语料库进行广泛的预训练。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529789&amp;idx=1&amp;sn=2ec2dd35a7174f984c054c4ceb68e3f4&amp;chksm=eaad8f43bd1c73cea215cd6aa96fc7d2ed94a7980f61eb3d7f3915942ca45ecaccc018a97053&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Sep 2024 09:27:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[DPO，RM，RLHF 傻傻分不清楚]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagzSibbmeLOLFXzP6J5pZkibyOibctBMMYAo6He7EwhyJvxVndgrdcVlwCjC0eRxZY7Gcljso8w6UFyQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：曹宇链接：https://zhuanlan.zhihu.com/p/718913850编辑：AI椰青 | 深度学习自然语言处理 公众号纯学术分享，侵删DPO 的论文引用最近已经破千了，成了斯坦福</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529789&amp;idx=2&amp;sn=535a178e6e3da1b347924e58abe435d3&amp;chksm=eab40b35d51d2f170f8d970351fcfd6de0335f01d18e698ee5c987cec9d1bf12365c030fe25f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Sep 2024 09:27:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLMs 能够生成创新的研究想法吗？——一项针对 100 多位 NLP 研究者的大规模人类研究]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagzSibbmeLOLFXzP6J5pZkibyOvTonicJ5w7Cibe1pk6pIqFwia5z3mjgWdRIqRRTn900DokEXGibQBNgYw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers地址：htt</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529789&amp;idx=3&amp;sn=40cd15e464510e8bb8997a695e3369f8&amp;chksm=eaa1f9c0e9f24debc5ba1856f698c3f8ca7015ac0786a54418b68d2339a0fc3d9aeb78ac3ad8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Sep 2024 09:27:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2024年大模型Alignment偏好优化技术PPO,DPO, SimPO,KTO,Step-DPO, MCTS-DPO,SPO]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia9l9AcgGZtGgl2zib28uNApkRn3Wq4NB0M6aUeJsgwSC7iaA3f0TnrEpDibtYofOOmwuTt852xLqygw/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：是念链接：https://zhuanlan.zhihu.com/p/710021282学术分享，侵删今年做过一段时间的alignment工作，做得有点不开心，各种social的原因，觉得自己的发</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529712&amp;idx=1&amp;sn=141763b569a7fe274261545c6c25deb9&amp;chksm=ea6ce00b713ba37e9918c528b750aaa9a09e299487374388d444fbb212624eefcc7d790c2658&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Sep 2024 12:09:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[探索自然语言中的计划搜索：提升大型语言模型代码生成性能的新方法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia9l9AcgGZtGgl2zib28uNAp1uGMAvD60GVRq5EFVElVia1ziblec5IicUep6wZ5HjUNiaSdg4llccuYqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Planning In Natural Language Improves LLM Search For Code Generation 链接：https://arxiv.org/pdf/240</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529712&amp;idx=2&amp;sn=9636b9860ce3959aa37a335f93b7a6a1&amp;chksm=eafaa3d431cd9d03fe054738d831e26105a492abbf81170e0dadf1db504aa4bbf137b4dfafe3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Sep 2024 12:09:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[揭秘MagicDec：如何推测解码让长文本处理不再纠结于延迟与吞吐？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia9l9AcgGZtGgl2zib28uNApoFZib61IIxR6Ku93abAEmL5UzVYc4rGiakgicV769UCKXTnXRloGCYJicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：MagicDec-part2: Breaking the Latency-Throughput Tradeoff for Long Contexts with Speculative Decod</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529712&amp;idx=3&amp;sn=178cdbb2485c5e43f8376bf1f37eaf8e&amp;chksm=eaf9379a84965ee2a86e3bb3a52b667c666200e55aed6606665d62c94219e3b6bfdf3d815643&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Sep 2024 12:09:35 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[关于如何做科研的一些个人经验 -- 清华AP、Mooncake作者]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahfcMKOVo1DH2AsU1CHEzEDLzJ9ASGsjOh5zxiadrHEhtBQib3icRU8x9iaceUB6micGicDWeibumt73Ge0A/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：ZHANG Mingxing链接：https://zhuanlan.zhihu.com/p/718156903最近开学季，被抓来给研究生新生们做个关于“如何做研究的”的入学教育报告。会后大家希望</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529635&amp;idx=1&amp;sn=daa01cb6a383800378b0c0b1e88b7f06&amp;chksm=ea9ffe2e7973981f315a3ddd92ab0e9decb72de9eac96344879cb5f0d300785049db20e19e3a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Sep 2024 11:36:07 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[重复采样魔法：用更多样本击败单次尝试的最强模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahfcMKOVo1DH2AsU1CHEzED5eNEANHGY7JvYxGhbCbibjVbHKNDP826qORM5bjG7ib33sG0qicxt4q0A/300?wxtype=jpeg&amp;wxfrom=0"/><p>这篇文章探讨了通过增加生成样本的数量来扩展大型语言模型（LLMs）在推理任务中的表现。研究发现，重复采样可以显著提高模型的覆盖率，特别是在具有自动验证工具的任务中。研究还发现，覆盖率与样本数量之间的关</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247529635&amp;idx=2&amp;sn=0af2dc8dd1a157e41fe4f1681f24d26a&amp;chksm=eac1ee2013daffd4fc327c3cf9e5ea30ae540adb0d6782953952a6d88cfbb8a3850e30fc111e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Sep 2024 11:36:07 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
