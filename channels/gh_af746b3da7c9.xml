<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    
















    <item>
      <title><![CDATA[一文全面盘点算法工程师大模型微调技能图谱]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaotYYdfgU4rUOxdaTnNhWvVrnaHddlQ5agfcxaicyCRq0N5Qta0ErBWTj1ww0Ia3ynN3K5KZz1n9w/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着大模型的飞速发展，在短短一年间就有了大幅度的技术迭代更新，从LoRA、QLoRA、AdaLoRa、ZeroQuant、Flash Attention、KTO、PPO、DPO、蒸馏技术到模型增量学习</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527281&amp;idx=1&amp;sn=6d7e8808feddba3b6233623576e21fd5&amp;chksm=eac412c935c3a39642218e8e6236f5429598c5e7b9801c98d13badf02a8c2d05d440bbf6be21&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 15 Apr 2024 10:09:14 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[魔改RNN挑战Transformer，RWKV上新：推出2种新架构模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaotYYdfgU4rUOxdaTnNhWvYx615HbE5A5xN4ia5JCBPpibrsVWgQXjqQhoic6sDaaVphWzTNAJkJwuQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>RWKV 投稿量子位 | 公众号 QbitAI不走Transformer寻常路，魔改RNN的国产新架构RWKV，有了新进展：提出了两种新的RWKV架构，即Eagle (RWKV-5) 和Finch（R</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527281&amp;idx=2&amp;sn=7bdb38ae001b4759fdf760f836f383a2&amp;chksm=ea519f3ad20528ca1316c0943ca80ae37673e88db9c18f50affeb917144408b27b41150797cb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 15 Apr 2024 10:09:14 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[多模态大模型的最优预训练范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaotYYdfgU4rUOxdaTnNhWvtu6g16fuZcribibyg179ZSTNohmFApggUK3K1avnqVBnCbyhzckBrwcQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：AI小飞侠，CV算法工程师/多模态声明：本文只做分享，版权归原作者，侵权私信删除！原文：https://zhuanlan.zhihu.com/p/685586296编辑：青稞AI目前主流的多模态</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527281&amp;idx=3&amp;sn=fa7c2429946e45c58094508d42189c25&amp;chksm=ea907edaecff7f5fc443b4dddd69818532fbaa16167f1838be48b4167aece1ed1d6bc2ecf733&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 15 Apr 2024 10:09:14 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[HADES: 利用视觉弱点越狱多模态大语言模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaotYYdfgU4rUOxdaTnNhWvzKDcrbjZskfrib0JmKkQkLia9JGsZV6CbLOxIZj9P22Jiak3iaacNicicmFA/300?wxtype=jpeg&amp;wxfrom=0"/><p>© 作者｜李依凡机构｜中国人民大学研究方向｜多模态大语言模型来自 | RUC AI Box本文提出了一种针对多模态大语言模型的越狱方法HADES，使用精心设计的图片隐藏和放大原有有害意图，利用图像侧是</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527281&amp;idx=4&amp;sn=bbcfe62c44ddada545f138fedb889cf4&amp;chksm=ea3a0f5b5a872eb0f4a2a2cd190af399ce7402681887055ab610978cd374c6920decdd3da850&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 15 Apr 2024 10:09:14 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[MOE vs MOT 让LLM更加有效]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajf3mibiauPpz0VfpyQNB8QejUjyvPrvicerFvcbJhWLqQ7LpAfyM6Uxf9009ibuZEGfibxhtjwa7PLrXw/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：北方的郎链接：https://zhuanlan.zhihu.com/p/691070810翻译自：https://www.superannotate.com/blog/mixture-of-ex</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527253&amp;idx=1&amp;sn=67ac3f80e022164ebc00ad14a7b39300&amp;chksm=ead7cc9f18491a4632891a5d38e5c1e7be8a5d27dddde617237dbedb319b05c1322a770e3a00&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 14 Apr 2024 15:04:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[国内20家公司大模型岗位面试经验汇总]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajf3mibiauPpz0VfpyQNB8QejnxWu9jHgmee4Xw6DiaZvHe0MJctgJZE1oKicTqh0WhtOajkxP4DTyE7w/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：林夕，阿里巴巴集团高级算法工程师，专注多模态大模型研究。声明：本文只做分享，版权归原作者，侵权私信删除！原文：https://zhuanlan.zhihu.com/p/690801254编辑：青</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527253&amp;idx=2&amp;sn=73abf1bb10d41b4300629f74fa8624e9&amp;chksm=ea05c3e9eb480cec11979ee9334b419c67bc5d05d432a199e60516fa0980f79148d1217b215f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 14 Apr 2024 15:04:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[RAG系统中答案无关的片段对LLMs生成结果有何影响？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajf3mibiauPpz0VfpyQNB8Qejw5SRZicD2TPAubUbDiaDPMx7xbQFAAYcpSjR0do1gKa9McRu1glU7WFg/300?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面来自：NLP工作站RAG（检索增强生成）通过检索系统找到用户问题相关的信息片段，利用大模型综合生成一个答案，极大解决了大模型幻觉、信息更新不及时等问题，已经成为了大模型落地的重要手段。但在检索</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527253&amp;idx=3&amp;sn=0e8acc3643fd1880bcf63fba30b40966&amp;chksm=ea5978e4bd6313afd2946445b93adda87128333de166f634806aff3037697b6c2986d112f50c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 14 Apr 2024 15:04:08 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[教你从0开始发一篇SCI，科研小白必看！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWAJgQIahQH2SuxZMgrGRSLRcxpM9OgxMlzuExWloqCxhdU6op04UiaoA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天向所有在2024以及未来几年内发论文的同学分享一些资料：23年各大顶会论文合集、80个代码中的即插即用模块、论文写作方法论、以及完成初稿后的论文润色。发论文，首先大家需要解决idea的问题。最有效</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=1&amp;sn=9df06a828cf03a91f18547f401759456&amp;chksm=ea44d0f81511eaa6d5a72b0bd4866fb9dfd8f523116744c4749fce099ed17e400fc40d473d88&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 邱锡鹏团队新作：探索LLM预训练的Data Mixing Laws]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWu5nJHLJIB9zhbDAbJ6d7s1hTblTGatbEV2xqOLzzWdH3cP5gv2LTnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：大语言模型的预训练数据由多个领域（如网络文本、学术论文、代码）组成，其混合比例对结果模型的能力有着至关重要的影响。现有的研究依靠启发式方法或定性策略来调整比例</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=2&amp;sn=1d761a15296acc6bcfee5e1febf4477d&amp;chksm=ea8eebfca822d6d8df3fa93eb57dc977fd53342df88af294e7995497d6766d198db3a42b70e9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | DeepMind提出SAFE，用LLM Agent作为事实评估器]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWu5nJHLJIB9zhbDAbJ6d7s1hTblTGatbEV2xqOLzzWdH3cP5gv2LTnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：大语言模型（LLM）在回答开放式话题的事实搜索提示时，经常会生成包含事实错误的内容。为了对模型在开放域中的长式事实性进行基准测试，我们首先使用 GPT-4 生</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=3&amp;sn=c983c5f727685842f22d1d6becb0a323&amp;chksm=ea59a648e1c549d8ea6c2cd1443cba873bbce7b01c77928cbe35c698ddf263ea8bdcfe3d75f3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | sDPO-不要一次就把对齐数据用完]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiabjuicHwWSRk8x0LuofO4pWu5nJHLJIB9zhbDAbJ6d7s1hTblTGatbEV2xqOLzzWdH3cP5gv2LTnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：https://arxiv.org/abs/2403.13269Q1: 这篇论文试图解决什么问题？A：这篇论文试图解决的问题是如何在大型语言模型（LLMs）的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527226&amp;idx=4&amp;sn=ee248439173e5c359944fb4b37043458&amp;chksm=eaaaa58fccd65f96a9c3ed0ad0e8288ac47ac3382be1ce0f3952459f514812319bb7c636e267&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 10 Apr 2024 09:06:48 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[全面解析LoRA、QLoRA、RLHF，PPO，DPO，Flash Attention、增量学习等大模型算法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOoS9b7j62Xe1s04eVP0YlXAC5J9EicHZYTVbmRbsEBUw5XKI9a6GQIYg/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着大模型的飞速发展，在短短一年间就有了大幅度的技术迭代更新，从LoRA、QLoRA、AdaLoRa、ZeroQuant、Flash Attention、KTO、蒸馏技术到模型增量学习、数据处理、开源</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=1&amp;sn=e9439999d46eb2c4987061d8d53c275a&amp;chksm=ea0df41f1cdd732cb15061e3ed4bca93b94545693c6ba646bfc14ec4336a08abe5f1ec8b2dd2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[RAG实践中的关键模块解析]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOBEBqTuhfvWh8HYqOkxvibFCxaXdctZhl0QLtUfep34M0Y8ZQunpF0ibA/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者：孙鹏飞，南京大学 · 计算机科学与技术，互联网行业从业人员声明：本文只做分享，版权归原作者，侵权私信删除！原文：https://zhuanlan.zhihu.com/p/682253496编辑：</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=2&amp;sn=aef77d67acbd9cc7db697711f4ad21f6&amp;chksm=ea224ac74e2715aedf6c6dbd47cb098afd3ac3aeff44ba65db3a7951d29ead130906e61c5e36&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[复旦MOSS团队：数据配比的scalinglaw]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOiaOGs9EdqqFQM2rYic4ItPJaCE4oGLJvyZiaVafMf6Cd8eWJqMrv6ppJQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：包包算法笔记在前文我们提到过，大模型训练中数据的多样性和质量是最重要的两个维度，并且在结尾挖了一个大坑，希望有大佬愿意研究多样性的scaling laws。这次，复旦MOSS团队带着数据配比sc</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=3&amp;sn=e49f6c0aaaddd6d1c89001e1f84eae32&amp;chksm=eaff9e2a54ba6ddbb2e10ee2f8016a24aeb93970fd4854203cf99fec84ef5def3e2544a18ca6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[长文本之罪：Claude团队新越狱技术，Llama 2到GPT-4无一幸免]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagWo55Xn4teoF7yT93rwpicOicp314zx0hiaURDrGSglDvr5bibQQm12TYogNHnqpZ94rDJgPEtOAaaeg/300?wxtype=jpeg&amp;wxfrom=0"/><p>机器之心报道作者：杜伟、陈萍Anthropic 发现一种新型越狱漏洞并给出了高效的缓解方案，可以将攻击成功率从 61% 降至 2%。刚刚，人工智能初创公司 Anthropic 宣布了一种「越狱」技术（</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527194&amp;idx=4&amp;sn=9fe5a5dea900cc4bfb000ebc9acefe35&amp;chksm=ea2e69ec2929bc4a6cd45bc752d0e060d34b8329211651118fbb7430fe1b1eacd6bed640f569&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 07 Apr 2024 02:33:50 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从 大模型接受弱智吧再教育 谈指令微调对齐]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTrmk0LnGnGAcmLLO8sEgPaZKHMmjLQ6LzW1S67oTEjwthXEdYgH1RHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：hzwer原文链接：https://zhuanlan.zhihu.com/p/690667537仅学术分享，侵删这两天一篇论文以离谱方式火了：CQIA：“用弱智吧数据训练的 AI 爆杀了所有中文</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=1&amp;sn=6c2d6bf3594b5201c52881630044ecb2&amp;chksm=ea2aa0c5959e97c86265cbcea09b58a4a33178d163671a9a671b23c65cc5fe822d79c242effa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | RLRF: 从反思反馈中不断迭代进行强化学习对齐]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTvFIQJ5VKDGrr016eIX7XwOcT1BbtM95hfvHjxa6OvQUt1B1EiaBGia1Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：尽管 RLHF 在使 LLM 与人类偏好相一致方面大有可为，但它往往会导致表面上的一致，优先考虑风格上的变化，而不是改善 LLM 的下游性能。不明确的偏好可能</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=2&amp;sn=ec0cfedef98843c710d3420d61f52b43&amp;chksm=ea1d70b19a543682530b3cdca5f5c5502a2dc214070ba02f7222deca5ebc9ee74f648e4900b9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | 一次编码平行解码：高效Transformer解码]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTaJcsKibBMYNsK1NgowaS7syPCl3snNZU2kssSP3U5dHvFWs07Qhfsmg/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：基于Transformer的 NLP 模型功能强大，但计算成本较高，限制了应用场景。经过微调的编码器-解码器模型在专业领域很受欢迎，其性能优于 GPT-4 等</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=3&amp;sn=114cf4ec777732490531ab76845c637d&amp;chksm=ea42d2342e3183b712c87cdc87f38c5ae29cd6447da94b62ac79283903da5cbf6d21b148b65f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[每日论文速递 | NAACL'24：自生成翻译记忆缓解翻译持续学习遗忘问题]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahgLwWzwMYxDNvdjWDfcofTVpa9Rlt2sUtSKKnSAq8QUvgZCsDydzX3ks4libtavx0HribfsGq7xyKQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习自然语言处理 分享整理：pp摘要：现代神经机器翻译系统在几种不同的语言中表现出强劲的性能，并在不断改进。然而，它们的持续学习能力仍然受到灾难性遗忘问题的严重限制。在这项工作中，我们利用enco</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247527163&amp;idx=4&amp;sn=fc226988e6685cee9941423d8bdd1270&amp;chksm=eaa699a9b6fd4ea5e31732682fc45c5296a957bbf5251997a03743d54452b8064cbbb2442700&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 04 Apr 2024 10:01:19 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
