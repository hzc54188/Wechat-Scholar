<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    



































    <item>
      <title><![CDATA[谷歌DeepMind重磅推出多视角视频扩散模型CAT4D，单视角视频也能转换多视角了。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emYIXZcOoWmiamNNy78gGxDxw4uWDBzPA32XByk1moUtrt0vCzrccsjPryianfay5w8IibgLtxibuRokQ/640?wxtype=jpeg&amp;wxfrom=0"/><p> 单目视觉4D重建再突破！谷歌DeepMind推出多视角视频扩散模型CAT4D，单视角视频也能转换多视角了。单目视觉4D重建再突破！谷歌DeepMind等团队，推出了多视角视频扩散模型CAT4D，它支</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489450&amp;idx=1&amp;sn=7851ba01590a76d477fac52cba8aca30&amp;chksm=fd2775915fbb675cb500d85048ad1797442887d051b66c1ddf15e4e12fd17aa91953f1bcb6d0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 16 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[文本转视频模型Allegro，可以生成长达 6 秒、15 FPS 和 720p 分辨率的高质量视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXSHQPic5rJ8IBKiaNosJKl47LGniaXW6NGticOnwCibeK5T8qlic4oPfsd5XwpM5KFOOqbuSEJIUictd6A/300?wxtype=jpeg&amp;wxfrom=0"/><p>Allegro 是一个强大的文本转视频模型，可以通过简单的文本输入生成长达 6 秒、15 FPS 和 720p 分辨率的高质量视频。主要特点• 开源：完整的模型权重和代码可供社区使用，Apache 2</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489450&amp;idx=2&amp;sn=28d1f03cfcb84fbb86f0668a57626a78&amp;chksm=fd7ad852685570429d07a97cfbf1697a9759502dd40da475e225ed2d74d4f8066bcf95369674&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 16 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[可控视频合成框架MIMO：可以模拟复杂运动并进行物体交互。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em63XUns9BWribhLaZgPeFEFgoPib1ZtNgvOGwxMrocc8qUiaTicFia2PrtUPkF5ux4TTfzxpOAuHia69uA/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里提出的MIMO是一种可控视频合成的通用模型，可以模拟任何地方任何人的复杂运动，并进行物体交互。给定参考图像，MIMO 可以通过几分钟的推理合成可动画的头像。它不仅可以通过简单的用户输入合成具有可控</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489450&amp;idx=3&amp;sn=eb712eed9199266ffc3d7d0a9a6d065e&amp;chksm=fd9c3fa474ab5e919a96ee3683f01e50f4a7216931511dcbc5aa5bc8d7b1a16bb78bd21549db&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 16 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[港大和字节提出长视频生成模型Loong，可生成一分钟具有一致外观、动态和场景过渡的视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eny4Iriba5NSXkHvLxicLITJD5gFTkWLoBqMSNfUicQxXgibIh9n6vokK5ia5EOh7ZDJLVHGEsbaLz86XA/300?wxtype=jpeg&amp;wxfrom=0"/><p>HKU, ByteDance｜⭐️港大和字节联合提出长视频生成模型Loong，该模型可以生成外观一致、运动动态大、场景过渡自然的分钟级长视频。选择以统一的顺序对文本标记和视频标记进行建模，并使用渐进式</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489450&amp;idx=4&amp;sn=03f7991ec725479ac365f2e593a36d6f&amp;chksm=fdba8d7332ea9ddf818507735d117943fa076ccc208ba7bd47332afc35894afd427408333807&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 16 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[一图看尽AI文生图未来，北大发布文生图十年综述：超440项工作回顾。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enDXLyhv5gUBA9w7NggpzadRrialXnXJ0hpkUPylInNS5ibB5unS9uBgxThVxDiaMNn2QsZM4tTQg5Fw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的文章来自北大发布的文生图十年综述，文章回顾了超过440项相关工作，重点探讨了生成对抗网络（GAN）、自回归模型（AR）和扩散模型（DM）在T2I任务中的应用和演变。还涉及了T2I技术的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489410&amp;idx=1&amp;sn=49d2a180ed2e8e04f11583ed6067eaa3&amp;chksm=fdbaf41cdb4c68adaa06592a5ee06319eecd652131e280d9a969a3c546a787257d735c6584ab&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 15 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[多模态图像生成模型Qwen2vl-Flux，利用Qwen2VL视觉语言能力增强FLUX，可集成ControlNet]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYHx9RLCm1u1zJr61WGBPZZicviaGPyXN8y5ZTaZE9jpPcdNSX1nmUlib5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2vl-Flux 是一种先进的多模态图像生成模型，它利用 Qwen2VL 的视觉语言理解能力增强了 FLUX。该模型擅长根据文本提示和视觉参考生成高质量图像，提供卓越的多模态理解和控制。让 F</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489410&amp;idx=2&amp;sn=58aca5f9ec80f84ee30d84b44fcd0989&amp;chksm=fddd2cc339cf3b1c8c44bfa5120911b0d71e2c20a8f5761fb489d8e4fe82c617d4b8d9e289d4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 15 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OminiControl：一个新的FLUX通用控制模型，单个模型实现图像主题控制和深度控制。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYEn3PFyv0qTxQYEgq8VntmUj91vEEYPJjMADiamfkH94icSBs7fF1Tn1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中和大家介绍过Flux团队开源了一系列工具套件，感兴趣的小伙伴可以点击下面链接阅读~AI图像编辑重大升级！FLUX.1 Tools发布，为创作者提供了更强大的控制能力。OminiContro</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489410&amp;idx=3&amp;sn=5f228f6148037bec3ebb954d2733d3d6&amp;chksm=fdfacc6108ea2fc643053635f2ae5d122c29074522939beb98c36b3797e8ca5579f480aa17cf&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 15 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[文生图像编辑来了！英伟达提出Add-it，无需训练，可根据文本提示向图像添加对象。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emmkDiagtskaHJodPFibMTUYJZY50N6JpzSdpSqpDMMdhm1JHxUv7E3vPPDa6XmXuygFoa0eiaBct3Bg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Nvidia提出了Add-it，这是一种无需训练的方法，可根据文本提示向图像添加对象。Add-it 适用于真实图像和生成的图像。该方法利用现有的文本转图像模型 (FLUX.1-dev)，无需额外训练。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489410&amp;idx=4&amp;sn=fb84276718fa115a4b4300a3d8fac1f1&amp;chksm=fd7060319ec2da894c68bee5ad914cb4231fdf8a6ee8c6bccc0e21c024c595a2ee19686d35f8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 15 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[万众期待，谷歌正式发布 Gem)ini 1.0 ，包含三个版本：Ultra、Pro 和 Nano。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el4eIaML40rNcaURA3WTSibqvHsHMchQTkJGsGjibUAC9vHubXy9en1avzJH5dic3qgdVAMXIKKmrLXg/640?wxtype=jpeg&amp;wxfrom=0"/><p> Gemini 发布一周年, 万众期待，谷歌正式发布 Gemini 1.0 ，包含三个版本：Ultra、Pro 和 Nano。Gemini Ultra——最大、最有能力的模型，适用于高度复杂的任务。 </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489408&amp;idx=1&amp;sn=988453237b24c69bc8d95d2e25ecc0db&amp;chksm=fd5017904ecb1be9f4e5f37b8ede8f2ab79be327a60326a331bd087465b588d10355c55c5678&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 14 Dec 2024 16:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Mistral AI 开源 Pixtral 12B 多模态 LLM，多场景能力理解，支持中文指令遵循！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekz6brgZd8MLzCQaTHcS8fyr4AQVZYW5XicicYO0FJtMDtMHtrnYOrMFWqicrbq1hkwWuJJFah4YbZlw/300?wxtype=jpeg&amp;wxfrom=0"/><p>Mistral AI 开源了 Pixtral 12B 多模态 LLM。具有自然场景理解，代码生成，图像转代码，图像理解，多图指令跟随，图表理解与分析以及复杂图形推理等多项能力。从效果演示来看模型的能力</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489408&amp;idx=2&amp;sn=8123fe719f1843b45a966c19a8610599&amp;chksm=fd39c88ce92e5e73f758196920e3414724edc83128cd5491c31b955cfe87bf1cc67befe93774&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 14 Dec 2024 16:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[EAFormer：场景文本分割新SOTA，图像文本擦除无痕迹！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enWwUtW60pCT5iccd79FiakHIjXQOCEnv6AoiaoS0khCUKncrp8Xpp3mPYm7VNByYYo7TBDhcFnZ8vRQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>文章链接：https://arxiv.org/pdf/2407.17020 git链接：https://hyangyu.github.io/EAFormer/亮点直击为了在文本边缘区域实现更好的分割性</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489408&amp;idx=3&amp;sn=7b15a1fd9f8af950a957fc7753226380&amp;chksm=fdab42d6e2e5e96f64f111cb36e066b7b03b1600ff9525ef0b18554ba3d0d34c6740135d2233&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 14 Dec 2024 16:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ViewCrafter：一张图像就可以制作影视特效和游戏画面！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPOajsP01qNvmwKPWKzicOsEEbzBxMRtYWNicS7fSTCWsMB1YH3tWIYmKrNLzGfsI0qOR2rIwTbN4g/300?wxtype=jpeg&amp;wxfrom=0"/><p>北大和港中文联合腾讯人工智能实验室提出了 ViewCrafter，这是一种利用视频扩散模型的先验从单个或稀疏图像合成一般场景的高保真新视图的新方法。可以简单理解为将复杂的图像转换成新角度的图像版本。首</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489408&amp;idx=4&amp;sn=da310383a01b736a7d7f02677a860482&amp;chksm=fddcc81ffaf4409430344073a387ddab594ab17e3f0c478834fd1a57af444195e88eec950e24&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 14 Dec 2024 16:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一个LoRA同时处理内容和风格？UIUC提出UnZipLoRA，可同时训练两个LoRA，与原有LoRA兼容。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekK8oWGMLQzxWWfB4pkH8CCntib22WMYOGLwrnJpjeM7SCyxLnkZxmUEMEJADyvx3g4ZicOs12gOU1Q/640?wxtype=jpeg&amp;wxfrom=0"/><p> 一个LoRA可以同时处理内容和风格了？UIUC提出UnZipLoRA， 可将元素从单个图像中分离出来同时训练两个LoRA，与原有LoRA兼容。伊利诺伊大学厄巴纳-香槟分校的研究者们提出了一种将图像分</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489407&amp;idx=1&amp;sn=0e472503463147e1ecd0dc0c79a5cfdb&amp;chksm=fdb21f54fc2987c00c539755e5bfecd6f0fc21141a395d31119aa6e585168cc2df281dc408d1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 13 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ConsisID实现无缝身份一致的文本到视频生成（北大&amp;鹏城实验室）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icoia2uAgx8oolzr9Eh1njnhCcveJeiaQoLY6rDURbQqk3MEdSTvQja2CmLYicTKIHZ8Xx3WUV9qmvmSLg/300?wxtype=jpeg&amp;wxfrom=0"/><p>点击下方卡片，关注“AIGC Studio”文章链接: https://arxiv.org/abs/2411.17440项目链接: https://pku-yuangroup.github.io/Co</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489407&amp;idx=2&amp;sn=1ce5c33b33b57feb232cb98bc000d3f3&amp;chksm=fdc3bc2807d40d3b079419b142cb466ed90c0ad1624ce593ca72e179a42ba8c88665430355bf&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 13 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[AI也能自然的说话！MDT-A2G：可根据语音同步生成手势！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enDan2VfS4eXEuxWGbYkxL3FPN4uR139dAKhibeZkVlguIIC8zVmtHAgxfEyW1wgmeQCFAfpfNkQ4w/300?wxtype=jpeg&amp;wxfrom=0"/><p>复旦&amp;腾讯优图等提出MDT-A2G，这是一个专门用来生成与语音同步手势的先进模型。想象一下，当我们说话时，身体自然会做出手势。这个模型的目的是让计算机也能像人类一样，根据说话的内容来生成合适的手势。它</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489407&amp;idx=3&amp;sn=0a896dc82c0881d167a172dbcc0cf97c&amp;chksm=fd3bb1c17d7f8a7bd3d1aced4126596590f9c2f7a346976a32ffed23445a4079a49a63e9275a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 13 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[AI也能生成电影大片！MovieDreamer：纯AI生成电影引爆热议！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enM9EmGpEygEm1sRYerx5zsLbbBcQQNnCyG3XqO7M69mCHE7iczf1LQ124ILzTbxf0wq9VnrUcdicug/300?wxtype=jpeg&amp;wxfrom=0"/><p>视频生成领域的最新进展主要利用了短时内容的扩散模型。然而，这些方法往往无法对复杂的叙事进行建模，也无法在较长时间内保持角色的一致性，而这对于电影等长篇视频制作至关重要。对此，浙大&amp;阿里发布了一种新颖的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489407&amp;idx=4&amp;sn=52b647829c9d0f44b8ec59216040c532&amp;chksm=fd8d955ca066b7558cbf81942b341073cb4cb737090741e48d9440200cbb0b0aaff44e249289&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 13 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[数字服装生成模型AIpparel, 可根据文本和图像等多模态输入生成复杂、多样、高质量的缝纫图案。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekK8oWGMLQzxWWfB4pkH8CCqH5lKluTEKKOzvgciav9PiaxM9qbPvlMxC1buuBneLzEFCRLZuyUCHFg/640?wxtype=jpeg&amp;wxfrom=0"/><p> 多模态数字服装生成模型AIpparel, 可根据文本和图像等多模态输入生成复杂、多样、高质量的缝纫图案。斯坦福大学和苏黎世联邦理工学院提出一种数字服装的多模态生成模型AIpparel，通过在自定义缝</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489406&amp;idx=1&amp;sn=dc2ef0610e0b571fae46345a0e69ab7d&amp;chksm=fdcd173d629c06e04ebe0c00caf779db39173f5e9b672b4d9b1a2062e326819175df038a4f76&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 12 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[3D服装生成新SOTA！谷歌和CMU提出FabricDiffusion：可将织物纹理从单个图像迁移到3D服装]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUwdDAlYv2ITgdKDOu7W8riapJOLuXkDL8cFpr3uMCxGiaooqI4z7Zcl4cQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>3D服装生成新工作！谷歌和CMU提出FabricDiffusion：一种将织物纹理从单个服装图像迁移到任意形状的 3D 服装的方法。FabricDiffusion性能表现SOTA！同时可以泛化到uns</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489406&amp;idx=2&amp;sn=66f3ece25264b61a334fc96958c081a0&amp;chksm=fd5cd5d15c91f931ee8df3c1e49672064aa73bfca0b1ceec269d4754da2accb6459741bab3d0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 12 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[多模态图像生成模型Qwen2vl-Flux，利用Qwen2VL视觉语言能力增强FLUX，可集成ControlNet]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYHx9RLCm1u1zJr61WGBPZZicviaGPyXN8y5ZTaZE9jpPcdNSX1nmUlib5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2vl-Flux 是一种先进的多模态图像生成模型，它利用 Qwen2VL 的视觉语言理解能力增强了 FLUX。该模型擅长根据文本提示和视觉参考生成高质量图像，提供卓越的多模态理解和控制。让 F</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489406&amp;idx=3&amp;sn=1c47dac198c56fa6e5074f817587a1ad&amp;chksm=fda329e1a4da5bad339aec159593a99651006bb211653452e9f4d56bde85a45c5a2fda9dc42b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 12 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GarmentAligner，解决服装生成中语义对齐、数量、位置和相互关系等问题。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek9OAibiaD8uzvEUOVyfeBR4Jdf4ILiayFhFoSg9mrsibicO6wic29rCpGI5ibIyargoQS8aFXOwkjoq5a1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>中山大学和联想研究院提出一个能够根据文字描述生成服装图像的智能工具GarmentAligner。它可以从已有服装图像中提取出各个组成部分，并记录下它们的位置和数量。接着根据你的描述进行匹配，找出最吻合</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489406&amp;idx=4&amp;sn=d9520e565e4d8b0ab924c4dd652dda79&amp;chksm=fd6d883d6d5334c319e1893de57b8311a45aab4f0e0ba4ea2995f87e9ca74844002175c8d357&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 12 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekB7CXUYR45xqh1P2Q9zWuxgmicJiaO6JPkkhoaibkSARt6qftWXI9ofZjt9NK9vuibg0UrfhA2kTPRaQ/640?wxtype=jpeg&amp;wxfrom=0"/><p> 腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT今天介绍的文章来自公众号粉丝投稿，腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT，给定一个人像图像和一个衣物图像，就可以生成一个</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489405&amp;idx=1&amp;sn=cf723a5a8e5ff9e52225aefe68ba3482&amp;chksm=fd8ed9244b3d5c22d540e66c0db83042073308426a43c183694a6b9fe656bab0b326bb49a368&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[刚刚，阿里重磅开源基于FLUX的In-Context LoRA，可一次生成多张风格和ID一致的图片集。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwaZq27QklGnm2euIIenh9a4vvHuaekrpqk2PuGae3ghYQjGRse85BXsA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍In-Context LoRA 这个项目太强了，前几天发布的时候就引起了许多小伙伴的关注，但是当时还没有开源，就在刚刚，作者开源了In-Context LoRA项目。它基于FLUX训练，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489405&amp;idx=2&amp;sn=60426388ec666cd6ac8a102639c24e38&amp;chksm=fda32e3f5abf7016ab6204579fe1475c0b0dfaed22b4186ac7ecea3a3f36329186a0403fded8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[统一的图像生成模型OmniGen：可以根据多模态提示直接生成各种图像，无需额外插件。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489405&amp;idx=3&amp;sn=e0b562f61d2733643a8c2c4509e646ca&amp;chksm=fd4236c3070650208c6545afff33cb82ec6d1d9ee608f62b00101484e7798640b1967a203efe&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GroundingBooth：一个用于文本到图像的定制框架，支持多主题和文本联合接地定制！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIxoR4VWqymzeNXHxsW4rxM6qoeicJM6XkODXXx3zP4H0duuNP0vk91Sg/300?wxtype=jpeg&amp;wxfrom=0"/><p>GroundingBooth是一个用于文本到图像的接地定制框架。首先提取文本描述和图像的特征，然后通过一种特殊的注意力机制来控制这些特征的结合。这个机制就像是一个精密的筛子，确保每个对象和背景之间的信</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489405&amp;idx=4&amp;sn=8afd63c4861afba6c02c46f948fd2f0b&amp;chksm=fd50ebfffbce63e534f5b0aadb1cfd945b9e0306d795cf63ba1790bccfe9c11588c7c50d995d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 11 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LiFT：利用人工反馈实现文本到视频模型对齐]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekB7CXUYR45xqh1P2Q9zWuxkuFceIwRBDcNLXKdaAmCkqmFBQ9yBx50KHAhV7I3F6zibhiaKfKZoG8Q/640?wxtype=jpeg&amp;wxfrom=0"/><p> LiFT：利用人工反馈实现文本到视频模型对齐今天给大家介绍的文章来自公众号粉丝投稿，这项研究提出了一种新颖的微调方法 LiFT，利用人类反馈通过三个关键阶段进行 T2V 模型对齐：(1) 人类反馈收</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=1&amp;sn=177f1bb43481f51757f4415353850345&amp;chksm=fdab21f8834c3e7cabe292e6138a247b551555460f47c88b31e60d3028e045bab4fd9f319d0a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Still-Moving文生视频模型定制框架，引领AI创作新潮流！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekGichZJXKzLt6ibAHt5XWRKcyeHmiaGH56bVYpGzicX3nlRa02RTJCWdXkENGbP3adodafagNCgKyZYQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，定制化文生图（T2I）模型取得了巨大的进展，特别是在个性化、风格化和条件生成等领域。然而，将这一进展扩展到视频生成仍处于起步阶段，主要是由于缺乏定制化视频数据。Google DeepMind </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=2&amp;sn=5c12c8d6cd69b1b67fa6a343e7d8e862&amp;chksm=fd7fc3345e12b0a86f483d824d990a59157691d509c0acbbc43431fc19a83e24df8a67f77485&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[个性化图像生成新SOTA！阿里开源MIP-Adapter，可将IP-Adapter推广到多个参考图像！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en553ETYWe4BYUv7R7Iibote1AADVDfRBnkq3JPLgRiclNJjwcPXztGYwW8ChWH8NjEcr9ibKS4asmjg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍阿里最近开源的个性化图像生成的新方法MIP-Adapter，将无需微调的预训练模型（IP-Adapter）推广到同时合并多个参考图像。MIP-Adapter会根据每个参考图像与目标对象的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=3&amp;sn=cb0ee069216fe3e7efc8fddb2237517e&amp;chksm=fd7b5e890e0b8b87ba9ea38cda5aa8148b5b0e3d3109ea7974da86385c266354935c3163651b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[MoMA：即插即用、无需调优的快速个性化生成方法！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekcDtU5TRWR4b0ttfgqrxKDOQAmDscucEyJqSeHYUm1lVuU1IS2LukibibiaoOxpibhtu00EDvRCvPshQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节提出一种即插即用的快速个性化生成方法-MoMA。不需调优，只需一张主体的图像(下图蓝色圈出)，就可以生成文本对齐的、保留身份的同一主体的新图像，只需要一次向前传递。我们的模型既支持重新语境化，即相</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=4&amp;sn=9475c30f628487a85414141019e153f3&amp;chksm=fd3320b3cbc4912959aa5313986b0923dd608c85e76011b3cf0b6b6e9a62132875f600fa9d36&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
