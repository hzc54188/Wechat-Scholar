<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    
































    <item>
      <title><![CDATA[LiFT：利用人工反馈实现文本到视频模型对齐]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekB7CXUYR45xqh1P2Q9zWuxkuFceIwRBDcNLXKdaAmCkqmFBQ9yBx50KHAhV7I3F6zibhiaKfKZoG8Q/640?wxtype=jpeg&amp;wxfrom=0"/><p> LiFT：利用人工反馈实现文本到视频模型对齐今天给大家介绍的文章来自公众号粉丝投稿，这项研究提出了一种新颖的微调方法 LiFT，利用人类反馈通过三个关键阶段进行 T2V 模型对齐：(1) 人类反馈收</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=1&amp;sn=177f1bb43481f51757f4415353850345&amp;chksm=fdab21f8834c3e7cabe292e6138a247b551555460f47c88b31e60d3028e045bab4fd9f319d0a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Still-Moving文生视频模型定制框架，引领AI创作新潮流！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekGichZJXKzLt6ibAHt5XWRKcyeHmiaGH56bVYpGzicX3nlRa02RTJCWdXkENGbP3adodafagNCgKyZYQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，定制化文生图（T2I）模型取得了巨大的进展，特别是在个性化、风格化和条件生成等领域。然而，将这一进展扩展到视频生成仍处于起步阶段，主要是由于缺乏定制化视频数据。Google DeepMind </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=2&amp;sn=5c12c8d6cd69b1b67fa6a343e7d8e862&amp;chksm=fd7fc3345e12b0a86f483d824d990a59157691d509c0acbbc43431fc19a83e24df8a67f77485&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[个性化图像生成新SOTA！阿里开源MIP-Adapter，可将IP-Adapter推广到多个参考图像！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en553ETYWe4BYUv7R7Iibote1AADVDfRBnkq3JPLgRiclNJjwcPXztGYwW8ChWH8NjEcr9ibKS4asmjg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍阿里最近开源的个性化图像生成的新方法MIP-Adapter，将无需微调的预训练模型（IP-Adapter）推广到同时合并多个参考图像。MIP-Adapter会根据每个参考图像与目标对象的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=3&amp;sn=cb0ee069216fe3e7efc8fddb2237517e&amp;chksm=fd7b5e890e0b8b87ba9ea38cda5aa8148b5b0e3d3109ea7974da86385c266354935c3163651b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[MoMA：即插即用、无需调优的快速个性化生成方法！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekcDtU5TRWR4b0ttfgqrxKDOQAmDscucEyJqSeHYUm1lVuU1IS2LukibibiaoOxpibhtu00EDvRCvPshQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节提出一种即插即用的快速个性化生成方法-MoMA。不需调优，只需一张主体的图像(下图蓝色圈出)，就可以生成文本对齐的、保留身份的同一主体的新图像，只需要一次向前传递。我们的模型既支持重新语境化，即相</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489358&amp;idx=4&amp;sn=9475c30f628487a85414141019e153f3&amp;chksm=fd3320b3cbc4912959aa5313986b0923dd608c85e76011b3cf0b6b6e9a62132875f600fa9d36&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 10 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[海螺AI发布最新图生视频I2V-01-Live：一键将静态图像转化为动态视频，表现力无敌！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emYIXZcOoWmiamNNy78gGxDxibqKbSefT8384qic8CJSaHHKUJRRexld0rOacyFHmNWCJHJUk83sicicuQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>海螺AI视频的最新模型——I2V-01-Live，正是这样的一次技术迭代，它以其独特的功能，为静态图像注入了生命力，让它们在屏幕上“活”了起来。这不仅是对现有视频制作流程的一种补充，更是对创意实现方式</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489357&amp;idx=1&amp;sn=897bf56e53fe2ea9284a919e5695b44f&amp;chksm=fd04132efcc76c8e519d536d030d72252ff952e0096990c2954682590f09d1dfc4bf7a71553c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[登顶Hugging Face文生图模型榜首！Recraft V3可精确处理复杂长文本和手指等解剖学细节！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emo5W5SskRqTMcQ39iaLjtibdfWiaulu8SLia1CmOhUK8o1DFXv1ZuIO9wgBw4mH09eF6uw4vAzibRT3rQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>Recraft推出了其最新的图像生成模型—Recraft V3。通过引入设计语言思维，Recraft V3为图像生成树立了新的卓越标准。在Hugging Face 人工智能文本转图像模型排行榜。它以 </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489357&amp;idx=2&amp;sn=bdffa34e151e40b1bfa46e50455e70d4&amp;chksm=fda9fc6a7e20d309629de56a9768c56431b1418f9b719195f79393fc56cba21c1918ffbbcdc7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[UniCon：可以在一个模型中针对目标图像条件对实现多样化的生成行为]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek42oQiaPZEylT2W0D4Wu6cvYsicLg7LOGv9Z2qo7Fm3eh6ibafUbUnWckhokIBrRiciaay05Sl0OF58eQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>UniCon 支持在一个模型中针对目标类型的图像和条件实现多样化的生成行为。UniCon 还提供灵活的条件生成能力，自然支持自由格式输入和多种模型的无缝集成。SJTU, Google亮点• UniCo</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489357&amp;idx=3&amp;sn=c8711042ffbe888d24756b3577eb4827&amp;chksm=fd64bb7c9c5653845f7e7fa1f1ee8b78dba57ad4b19d3af77ba01e881027d21d391c1dcb5eea&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ScribbleDiff：使用涂鸦精细引导扩散，实现无需训练的文本到图像生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en4dVnOT75Vve5gBZeAMAcqnHFQnQNTu2jZ3gdtvtEhgfeuBiawdPpo4eRXb4xIj7t0TCyfMVB3Rhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>ScribbleDiff可以通过简单的涂鸦帮助计算机生成图像。比如你在纸上随意画了一些线条，表示你想要的图像的轮廓。ScribbleDiff会利用这些线条来指导图像生成的过程。首先，它会分析这些涂鸦，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489357&amp;idx=4&amp;sn=96086521e768b5900ef1826c1df6e4a5&amp;chksm=fd2f883c916559b660c74a9394f1fdba870e0e2f49cf99dc667e3fedafed573359e55ab5218b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 09 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[字节 &amp; 清华大学提出 AnyDressing ：通过潜在扩散模型实现可定制的多服装虚拟试穿。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el4eIaML40rNcaURA3WTSibqnpQ2g1NFlaI6Rqk2xKeeM63oDt2Siblvd3Z2JVicAdFicCgWdSPicLJVzw/640?wxtype=jpeg&amp;wxfrom=0"/><p> 字节&amp;清华大学提出AnyDressing:可利用参考服饰和文本定制化人物，解决多服饰组合搭配、文本响应以及服饰细节的问题。今天的文章来自公众号粉丝投稿，清华大学联合字节提出了一项虚拟试穿新方法Any</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489316&amp;idx=1&amp;sn=a04ef98d257197ae06cda24b03a8bb31&amp;chksm=fd495e2a543ff9b3f8f931ffc9cd89c4fe3b4d572ff57ce9b6c15dad63524bed43006711b8ea&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 08 Dec 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[CatVton升级版？CatVton-Flux：AI虚拟试衣方案新选择。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYW6IaEp7gbVv0yHQMOcuZ65LzSPaIPcK5ZBqJ9AhEjNULUK7MKY05CQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面：本公众号已经给大家介绍总结了一些列关于虚拟试衣的文章和方法，小伙伴们可以在公众号菜单栏点击AI虚拟试衣查看，也可以直接在公众号内搜索"虚拟试衣"之前的文章中已经和大家介绍过虚拟试衣方案Cat</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489316&amp;idx=2&amp;sn=9c6dba829aafb1ce88f8a46c97f7d026&amp;chksm=fd6652ed82431964dbe0ac5720d3e18646231fdee57b75f24c038e589aa7b34ea930c55cb934&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 08 Dec 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[CatVTON：轻量化架构与高效训练，助力虚拟试衣技术落地应用！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enM9EmGpEygEm1sRYerx5zs4mgMPgDicoDS6emdT0nVUdvTwqYibENyLL2gg3X7xs5yMuia7FRkm9zRg/300?wxtype=jpeg&amp;wxfrom=0"/><p>本篇文章来自粉丝投稿，文章内容是关于虚拟试衣技术。本公众号在之前已经介绍了许多关于虚拟试衣技术的文章，感兴趣的小伙伴在公众号中搜索“虚拟试衣”阅读相关文章~近日，中山大学和 Pixocial 联合发布</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489316&amp;idx=3&amp;sn=84afaea18d19ef9cd7b325a6c9ff1303&amp;chksm=fdd40dbb715b57b7d1f629dbfda59973302a13ef4f8c79a7c73f0d1ff8814988b601c454e5ff&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 08 Dec 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[超级智能“试衣镜”！GarDiff：高保真保持目标人物特征和服装细节，虚拟试穿技术新SOTA！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIef94PhQmUMcfJSkj4W9NicELKlw377icJuhpfjx2VUNPWKHMM0Gqib5Eg/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍了很多关于虚拟试穿的文章，本公众号也总结了虚拟试衣专题在公众号菜单栏，感兴趣的小伙伴可以在公众号内搜索“虚拟试衣”阅读～今天给大家介绍一个最新的虚拟试穿技术GarDiff，它可以分析</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489316&amp;idx=4&amp;sn=09a97b23f5c152a251fe188c3f88f213&amp;chksm=fd1fdf43b7db2dee6e68aa9ee65a0a875bd24b36540a2509b991b490dcc490dc4f1a495c140f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 08 Dec 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Boow-VTON，无需训练即可进行试穿，解决野外试穿任务难题！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elkkuWrNlw62n8iayUtU0k8ylNWLA8s3GygmDIvkMMozsRJV3j6OdhM9D45R5ibKHqZgjgenJNePSicA/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍了很多关于虚拟试穿的文章，本公众号也总结了虚拟试衣专题在公众号菜单栏，感兴趣的小伙伴可以在公众号内搜索“虚拟试衣”阅读～今天给大家介绍阿里最新提出的虚拟试衣方法BooW-VTON，结</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489316&amp;idx=5&amp;sn=c80215658420de02afa62bc5ef606e5d&amp;chksm=fdd96d9b203db372d3426552fdc5b3d39f96d58b98f90e44ceaf1f7378747743c3236060c72c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 08 Dec 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ClotheDreamer：3D数字人也能实现穿，脱衣自由！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emPnvPXL7KLqPvkIAqtoJ3wAARpkHstN4O33m9M1haEAL9bqcv7I3brE6IfBs4EUXTjOuq6l2WvZg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天,给大家介绍上大、腾讯等提出的3D服装合成新方法ClotheDreamer,它以其革命性的能力,从简单的文本提示直接生成高保真、可穿戴的3D服装资产,正在重塑电商与空间计算领域的未来。数字人也能实</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489316&amp;idx=6&amp;sn=e176a8b04ec6524441918afac60b9651&amp;chksm=fdc66dfdbe6803e14f6efce2a4f6c81d845c726502c724f768a1ce21346ebf78f56d47171962&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 08 Dec 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OpenAI Day2：OpenAI 的强化微调研究计划。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el4eIaML40rNcaURA3WTSibqBAVCso4OxicOe7oiaoluZkpsL62X0hIYzYQu06PicX7zbzfOHb1nicVlTA/640?wxtype=jpeg&amp;wxfrom=0"/><p> OpenAI Day2：OpenAI 的强化微调研究计划OpenAI 12 天 – 第 2 天的实时更新，包括 ChatGPT、Sora、o1 等 关于 OpenAI 12 天活动你需要知道的一切O</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489315&amp;idx=1&amp;sn=98f5fb16f6438c18a471c208912be35d&amp;chksm=fdcfd2108659e6e150a04ed57b8cd096022d26684370ac89a1263cfb64cd78d255cb40c99d40&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 07 Dec 2024 16:09:26 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OpenAI Day1：推出o1, o1 Pro, ChatGPT Pro，更可靠、更准确。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emYIXZcOoWmiamNNy78gGxDxS3PoMk88cxH1RaHOhD6ZYvxwl0wIsuqrxefANib2nZVPsIpNaPaBhwQ/300?wxtype=jpeg&amp;wxfrom=0"/><p> 先跟大家播报一条Open AI 最新数据：• ChatGPT每周活跃用户达3亿• ChatGPT每天收到10亿条用户消息• 在美国有130万开发者基于OpenAI进行开发OpenAI 在 X 上宣布</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489315&amp;idx=2&amp;sn=46af8519b5dc0fcb50b6882ba508fb45&amp;chksm=fdb52713a4d48dd3ef120067ff0bce8bf73cf123eebfc39e2c68510f2b62e1964c719295ab82&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 07 Dec 2024 16:09:26 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OpenAI连开12天发布会Day1：推出200美元每月o1, o1 Pro, ChatGPT Pro，更可靠、更准确。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emYIXZcOoWmiamNNy78gGxDxS3PoMk88cxH1RaHOhD6ZYvxwl0wIsuqrxefANib2nZVPsIpNaPaBhwQ/640?wxtype=jpeg&amp;wxfrom=0"/><p> 先跟大家播报一条Open AI 最新数据：• ChatGPT每周活跃用户达3亿• ChatGPT每天收到10亿条用户消息• 在美国有130万开发者基于OpenAI进行开发OpenAI 在 X 上宣布</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489267&amp;idx=1&amp;sn=531b53494bb87b80dbf08dde27a645fa&amp;chksm=fd54ba3bda929674c601068aa4c4d208dadc2ecf4070ae86a4d9bceae6509e66ea5a0655a71c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[InstantX 重磅开源 FLUX.1-dev-IP-Adapter 模型，文中附模型和comfyui工作流下载。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eltYhV1JmK1ib9FbmIt2gIyP6xwE5MPwewm6kG9gwsXpPEaHHTOicBN3XsV2LsRvJ5qWf00gn2UwKaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>InstantX 团队的研究人员开源了 FLUX.1-dev-IP-Adapter，这是一个常规 IP-Adapter，新层被添加到 38 个单块和 19 个双块中。使用siglip-so400m-p</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489267&amp;idx=2&amp;sn=4f3805401c57a78d63b7217582b9ce7a&amp;chksm=fde5272bad7923d923b115a0dbc707b7bc398e6e3aaac83e495f4236ea1edb42b152f08eca7e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[FLUX.1 Tools发布，为创作者提供了更强大的控制能力。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eltYhV1JmK1ib9FbmIt2gIyPDOrcibpBhOX3OVdibquclM6ib7Tsxn7qhfDiclnY9wlicfpKLJib4fLuY7Og/300?wxtype=jpeg&amp;wxfrom=0"/><p>AI 图像编辑昨晚迎来了一次重大升级！ BlackForestLabs 发布了 FLUX.1 Tools套件，为创作者提供了更强大的控制能力。FLUX.1 Tools套件介绍这次发布包括四项新功能：F</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489267&amp;idx=3&amp;sn=1bae772eb79ec2029688106a8aae95dd&amp;chksm=fd2e00bf7151110eff2f5533ca1754ab4c8a4acdd733926cf162e22c93537b5863b0b8eb2387&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ComfyUI官方桌面版正式发布，适用多平台，免费向所有人开放。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emFSyib6eJvfckn9qtbFXKmI6ybr39dw3U5vwoMsaPPndtuKxZufMlnEMa3f8OuXnwLibJKMxEj2cyw/300?wxtype=jpeg&amp;wxfrom=0"/><p>Comfyui 官方桌面版本正式向所有人发布，支持Windows（NVIDIA显卡）和macOS（M系列芯片），旨在为用户提供更便捷的设置导入、日志集成和模型下载功能，尽管仍处于Beta阶段，但未来将</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489267&amp;idx=4&amp;sn=4b632f8d8f1cde4bfed6c11a05ae427f&amp;chksm=fdc2a214f99f31e55ca7597b974b2e9d294a0f268be0d5c84b279a96c636fa426d7f38fe9883&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[海螺AI发布I2V-01-Live：一键将静图转为动态视频！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src=""/><p>I2V-01-Live以其简洁而强大的能力，让创作者能够轻松地将2D插画转化为动态视频，这一过程无需复杂的动画制作技巧，也不需要昂贵的设备支持，为个人创作者和小型企业打开了新的可能性。  </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489239&amp;idx=1&amp;sn=099215f55a071429c5c7a829ea22fbc2&amp;chksm=fd6f61ef52136ded349cf76491da5e7d3a511f0e36cf32a3d5f21d3f9b92a216573c1cf646b1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 12:23:57 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[AI时光机上线！用MyTimeMachine一键体验从童年到白发的神奇旅程，让AI带你穿越时空。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enDXLyhv5gUBA9w7NggpzadXQzDTPawT0bfoK6ibr2LeHFQlxEcvjESOibkicIb6YJEiawdBI58gLItSA/640?wxtype=jpeg&amp;wxfrom=0"/><p>AI时光机MyTimeMachine正式上线！只需上传50张照片，AI便能根据你的面部特征，模拟不同年龄段的你，无论是重回青春、见证中年风采，还是预见未来的老年模样，统统轻松实现。想知道自己20岁时的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489200&amp;idx=1&amp;sn=c03b085dc1088a2f7275ff4c9a4b9420&amp;chksm=fd29d19a79bdee7d122e9605b44a793213f3dddbe5a0f13e140cb3f97087fd999323a51b0526&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 05 Dec 2024 16:02:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Siggraph Asia 2024 | Adobe发布MagicClay：可通过文字引导对3D模型特定部分进行雕刻！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldYoicjqyr71olTXNsd2FpJRUrSFOicVlJ9EDVptvjqu3DXUX2qZCR4C6D35wDfPGjSibEhSk8DyhGQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>Siggraph Asia 2024 | Adobe发布MagicClay：可通过文字引导去对3D模型中的特定部分进行雕刻今天给大家介绍一篇来自Adobe研究人员在Siggraph Asia 2024</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489200&amp;idx=2&amp;sn=d21798b627fdae88e7f31df7d3eacf07&amp;chksm=fd661e60020bbb0090dcd5658ccb1864c6ae2ccd6af7b037c821093ec5fa8e3a7fb74dcbf786&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 05 Dec 2024 16:02:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯发布HunYuan-3D，支持文本到3D和图像到3D，10秒即可生成高分辨率细3D模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elMhPFZCKibTiaBKrjL4Yql4lFH5tVlYMlAnW2RYL3JiaF4vHrEFr3z5TWpyzCAENlicH9DuH5PnpYic3g/300?wxtype=jpeg&amp;wxfrom=0"/><p>HunYuan-3D支持文本到3D和图像到3D功能，包括网格和纹理提取在内，整个过程在 10 秒内完成。文本到 3D：用户可以通过简单的文本描述生成 3D 对象。例如，描述一片绿叶或一把棕色吉他，模型</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489200&amp;idx=3&amp;sn=d744c6248929a6990fa1cfea8b086209&amp;chksm=fde17fb4e04e1222a38fc55f28571928d27319b37e83543465bf2caf1c54307b62efb6107f37&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 05 Dec 2024 16:02:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[升级版本的EchoMimicV2来了！一张半身照+音频，就能生成带手势的数字人视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emwVPXqNlN4T1anTjic9RlprbaDXZrG9liaHmF4qP4TqeGSFB4j1aqeAaysqA48YKP0ibTgb4E8ic61Bg/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经给大家介绍过蚂蚁集团的开源数字人项目EchoMimic，感兴趣的小伙伴可以点击下面链接阅读~蚂蚁集团放大招！EchoMimic来袭，音频+面部标志，让你的肖像“活”起来，直呼效果逼真</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489200&amp;idx=4&amp;sn=54a570e07e40a5e38a686f919e78faad&amp;chksm=fddc3e66c5493bdcf8083eab231d3a294af6247a2724ff06c4f82e18160ee1f2e03b2ac36b25&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 05 Dec 2024 16:02:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[超越Hallo和AniPortrait？音频驱动肖像动画新方法LetsTalk,可生成与音频一致的逼真视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elS0nh744Xc7tB6W08RA4SgfkcDFOIyH4xJ5xG8Ar9Y0Uvicw6xicJzbmEtb2yOzCDNqYMjzkS4eYpw/640?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中已经给大家介绍过许多关于音频驱动的肖像图像生成动画方法，感兴趣的小伙伴可以点击下面链接阅读~复旦开源Hallo：只需输入一段音频和一张照片就可以让人物说话。开源EMO再升级！复旦|百度|南</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489162&amp;idx=1&amp;sn=f57624bb544e0990e16bdb2db92c84e6&amp;chksm=fd2c76ffb0f770f7d97f6253c968476aec6515f61c34dd1efcea1f759d9f72200a7d0ff17a5e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[告别大头娃娃，东京大学开源数字人TANGO：能根据目标语音音频生成同步全身手势的视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en6pFnDNvWHLiaftE66MEoWpW84gLODaQtSQ9LfJlyThsRQM3P6M7VVqstMXBRlYemO0FShZxQIGrA/300?wxtype=jpeg&amp;wxfrom=0"/><p>目前已经有很多面部和唇形同步的数字人项目了，但大多只支持头像和上半身，前几天介绍的Hallo2音频驱动图像生成视频小伙伴们都非常关心，后台也有留言问有没有支持全身视频生成的方法。开源EMO再升级！复旦</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489162&amp;idx=2&amp;sn=211d99016a9a22076ed219c9820f7a10&amp;chksm=fdfac4f4a1c01b50bdcda54b86f7814601d174babea59f189210c61617df12943236c84e5d01&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[开源EMO再升级！复旦|百度|南大推出Hallo2：可以生成4K，一小时的音频驱动的视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eltJu44TqnY7VwyFHlz5hNmzgFx5WXQibibH6vJRBJhBmhnZYZKxgXcibPxaACh1nF70mRz6kzVMSZuw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经和大家介绍过复旦大学开源的Hallo项目，感兴趣的小伙伴可以点击以下链接阅读~复旦发布开源版本的EMO，只需输入一段音频和一张照片就可以让人物开始说话。复旦开源版本EMO:真实人物效</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489162&amp;idx=3&amp;sn=d80bba8108adbcfd32d0365d1db816a9&amp;chksm=fd8a30127f8916fd4888e18dbc5e677ec183dc905535fc7868ade7e2a98e9d2df01cdc8b0a77&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[复旦开源Hallo：只需输入一段音频和一张照片就可以让人物说话。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emmX3AJiczS9uocCYrX3Enlaia0e7A5oBvt6Ejqza4BfEXLNSyWs0q80XhcU9QMoTln8FqIUtNe5ibHg/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前和大家介绍过阿里的EMO和腾讯的AniPortrait，用户只需要提供一张照片和一段任意音频文件，EMO即可生成会说话唱歌的AI视频。最长时间可达1分30秒左右。感兴趣的小伙伴可以点击下面链接阅读</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489162&amp;idx=4&amp;sn=12a48f598dd36644d88285b31f010710&amp;chksm=fd86c99c4cbe0d0c30703b09065c3c411fe55cc335dd5a643a6150b0b30c24a6f78b3f9bc5f1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯提出AniPortrait：音频和参考肖像图像驱动生成高质量动画。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekGB20aoopfDW9Ia72SmdXIicTEOSQdNotOKRQycycAchbz7X13BECiaz9Sb46TEia25wWuBVJMXSaOQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯提出了一种新颖的框架-AniPortrait，用于生成由音频和参考肖像图像驱动的高质量动画。通俗讲，就是给张照片生成说话的视频。类似阿里的EMO，大家先可以简单看下效果。相关链接论文：arxiv.</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489162&amp;idx=5&amp;sn=f9fa2a12e41b791de2d5adc1d6cefa1c&amp;chksm=fdbed370f0e40bc1e3b12cb44facdf0bba0c0db12a88e7cf8f7d6ae769924ed335043832d1d9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里EMO：强哥也能上刑法课了！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src=""/><p>只需要输入图片和音频就可以生成富有表现力的视频，并且嘴型是可以跟声音匹配的。支持多语言、谈话、唱歌以及快语速的适配，可以根据输入视频的长度生成任意持续时间的视频。强哥也能上刑法课了！</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489162&amp;idx=6&amp;sn=3009c52d8e867c3aecf8118fb1e0ab8f&amp;chksm=fdaf7effb7e462b47c4c4adc78b5717ef4d42950e1de714873bf622c3823b6935e19663118b0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 04 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
