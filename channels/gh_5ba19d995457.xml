<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    





















    <item>
      <title><![CDATA[阿里发布新ID保持项目EcomID, 可从单个ID参考图像生成定制的保ID图像，ComfyUI可使用。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwazEmicf1F7ZjrxQSj4JNP3x3icluxM84Et2UYGdsdxfDOXnd9OlZYFCwg/640?wxtype=jpeg&amp;wxfrom=0"/><p>阿里妈妈发布了一个新的ID保持项目EcomID，旨在从单个ID参考图像生成定制的保ID图像，优势在于很强的语义一致性，同时受人脸关键点控制。EcomID 方法结合了 PuLID 和 InstantID</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488850&amp;idx=1&amp;sn=8d16e99274bc4b5bc03f0e497b5bab3f&amp;chksm=fd549325f683f5b64ae025623a456a4383f87dfab475a69089ec8128438508e4b13ffb105b0e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Nov 2024 16:01:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[​IC-light V2：基于FLUX训练，支持风格化图像，细节远高于SD1.5。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXSHQPic5rJ8IBKiaNosJKl4zjuJBWaX5OT2Lf7ZUfribJIKIVPQd63HVFcGOR4owDUYVuia3JgZoueg/300?wxtype=jpeg&amp;wxfrom=0"/><p>IC-light V2：支持处理风格化图像“IC-Light”全称是“Imposing Consistent Light”，IC-Light 是一个操纵图像照明的项目。目前已经发布了两种类型的模型，两</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488850&amp;idx=2&amp;sn=1523c167357fce882dba3da1b9b26978&amp;chksm=fda0b03bcdf8eab3001abdb7d8d21baa7de903183b2fe0344de52c680bf49e3294a60632e460&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Nov 2024 16:01:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Flux Lora｜可以和二次元合影啦！RealAnime-Detailed V2，可将动画与真实人物风格融合！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUwqgvk05w7f8uadCgfhevafdaLgHvO27pEwiaeOFibIj2ibunwDjv1xj7oQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个很有意思的flux Lora：RealAnime-Detailed V2，可以合成二次元和真实人物混合的图片！它一个使用LoRA技术开发的Flux模型，专注于将动画角色与真实人物风格</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488850&amp;idx=3&amp;sn=265311a35f91a326e9b82b03780ff4bc&amp;chksm=fdbfdc55ae193e59afa0854965c820a8a9910a11ea16cc92dbadcbc3ad1352d854897e7a241d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Nov 2024 16:01:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Glyph-ByT5-v2，支持10国语言图文海报生成，效果惊艳！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekUlTbakAQ9PkRVtjuPOYtMklfrlDVxgTLUqQxQB6Xzp3hd7zxxMa0HXnBhpURAxPhMlClBWcF7eQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>清华&amp;北大&amp;微软&amp;利物浦大学联合提出Glyph-ByT5-v2这款工具支持多语言图文生成，包括英语、中文、日文、韩文、法文、德文、西班牙文、意大利文、葡萄牙文和俄文。以下分别展示中、英、日、韩图文的视</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488850&amp;idx=4&amp;sn=518d4d1f7e1847aa26b7bdc5213f5f5c&amp;chksm=fdccafa78042c3c1275ed61b9df1fff595baec4d5717d894d1a6f95c8396e4198bc9d96fc5e9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Nov 2024 16:01:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[ComfyUI-Detail-Daemon：用于控制图像生成细节的ComfyUI节点，文中附工作流下载。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elMhPFZCKibTiaBKrjL4Yql4l5tSPbWv6ISeSB5iazRTGNmX1GIWphk4G8iavT8x0dOTgM97dgvchODEg/640?wxtype=jpeg&amp;wxfrom=0"/><p>ComfyUI-Detail-Daemon将muerrilla的sd-webui-Detail-Daemon移植为ComfyUI的节点。通过调整生成过程中的sigma值，该工具帮助用户在生成高分辨率图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488849&amp;idx=1&amp;sn=b756cd14a1a2f7eedb739e1a67000fc3&amp;chksm=fd8ebb166dac841961b7e81bcf26a020deb05f861ca37b4c96caf5a4644e4c6b020b51204b3a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Nov 2024 16:12:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Flux LoRA | Then and Now：可将历史照片和现代场景融合，实现不同时间点的对比展示。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwa5IZKDYBAO7LLOlXfTp24Tpqlh9Q3AG0qQKRWByqvmjXWQzoSf3LVqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家推荐一个我最近发现的特别棒的概念 LoRA 模型-Then and Now，能生成过去和现在的照片相匹配的图像，可将历史照片和现代场景融合在一个画面中，实现不同时间点的对比展示。作为概念训练</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488849&amp;idx=2&amp;sn=bdc4fdc3db6368e2bd3004338b51a55f&amp;chksm=fddc6c0da6a9c8c6db63ead228fe0223442c313cac59f3d39adead18338d49caabd235debd78&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Nov 2024 16:12:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[​Controlnet作者新作IC-light V2：基于FLUX训练，支持处理风格化图像，细节远高于SD1.5。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXSHQPic5rJ8IBKiaNosJKl4zjuJBWaX5OT2Lf7ZUfribJIKIVPQd63HVFcGOR4owDUYVuia3JgZoueg/300?wxtype=jpeg&amp;wxfrom=0"/><p>IC-light V2：支持处理风格化图像“IC-Light”全称是“Imposing Consistent Light”，IC-Light 是一个操纵图像照明的项目。目前已经发布了两种类型的模型，两</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488849&amp;idx=3&amp;sn=9355a06bb63228a3ae7900ae260246f8&amp;chksm=fd7055ba3322f918687a5d0076fd692a8e78115a7a0d6e641bdb3da79ee204bcc2329e28890e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Nov 2024 16:12:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯发布HunYuan-3D，支持文本到3D和图像到3D，10秒即可生成高分辨率细3D模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elMhPFZCKibTiaBKrjL4Yql4lFH5tVlYMlAnW2RYL3JiaF4vHrEFr3z5TWpyzCAENlicH9DuH5PnpYic3g/640?wxtype=jpeg&amp;wxfrom=0"/><p>HunYuan-3D支持文本到3D和图像到3D功能，包括网格和纹理提取在内，整个过程在 10 秒内完成。文本到 3D：用户可以通过简单的文本描述生成 3D 对象。例如，描述一片绿叶或一把棕色吉他，模型</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488848&amp;idx=1&amp;sn=e68202c2a057ca9f3c986de03049d2fc&amp;chksm=fd58d5fdd17b2dce1c37a44c76999ce03e69679ef7b3a657ca861ea06b118b9c54b8974024b1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Nov 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[统一图像生成模型OmniGen：可由多模态提示直接生成各种图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488848&amp;idx=2&amp;sn=b485d9ca45a43ecc8a6e180d6b0abade&amp;chksm=fdf4b9f87d236d84f6423e17c10b8c2e374ad8feaa7e1db6d58aed12e58eea9b0df5ffc5768d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Nov 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像编辑大一统？多功能图像编辑框架Dedit:可基于图像、文本和掩码进行图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekDYMeOJw6PMrPrgUmBfVvIibC8Suae7poAtMSSVAkicNMibK5CyJB4RLSAKFiajeuqXiaiaib0vMibRiaSKCQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个基于图像和文本的编辑的框架D-Edit，它是第一个可以通过掩码编辑实现图像编辑的项目，近期已经在HuggingFace开放使用，并一度冲到了热门项目Top5。使用 D-Edit 的编</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488848&amp;idx=3&amp;sn=52d341f054e40fc3d760239bebe3fa3f&amp;chksm=fd1d9bb2f86496eb25af3c5cecbdeb6e949cb6b759e310323eff309eb29a06ea7d7c61e6aa9d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Nov 2024 16:31:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[文本转视频模型Allegro，可以生成长达 6 秒、15 FPS 和 720p 分辨率的高质量视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXSHQPic5rJ8IBKiaNosJKl47LGniaXW6NGticOnwCibeK5T8qlic4oPfsd5XwpM5KFOOqbuSEJIUictd6A/640?wxtype=jpeg&amp;wxfrom=0"/><p>Allegro 是一个强大的文本转视频模型，可以通过简单的文本输入生成长达 6 秒、15 FPS 和 720p 分辨率的高质量视频。主要特点• 开源：完整的模型权重和代码可供社区使用，Apache 2</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488847&amp;idx=1&amp;sn=7e920fd13492baee8e07ce1fa1ee3e58&amp;chksm=fd5338152f52b31169fb2443f34daf6b646929be096615b115094c4133cb6f58501db26b7fe0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NeurIPS2024 | OCR-Omni来了！字节&amp;华师提出统一的多模态生成模型TextHarmony。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUw9icM2HlibUyI4RtyIsiaB9FOY9taoKCFlibTeImBZT585GC3ias7FialR6UA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能领域，赋予机器类人的图像文字感知、理解、编辑和生成能力一直是研究热点。目前，视觉文字领域的大模型研究主要聚焦于单模态生成任务。尽管这些模型在某些任务上实现了统一，但在 OCR 领域的多数任务</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488847&amp;idx=2&amp;sn=2f66668760c3bba06fcc7c993b9ea415&amp;chksm=fdb2b1dbd4394e63daf13dce37abbef85d3da445c8bf3040fc869302812dede1705c11b5ea53&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe发布TurboEdit：可以通过文本来编辑图像，编辑时间<0.5秒！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKcprhHqENugIHSUTwb3EOiaaqictMa8fmmNEDqsoISMhGDZH4oZmh7vtMn5sov6khPdhIypPkhDZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍Adobe研究院新的研究TurboEdit，可以通过文本来编辑图像，通过一句话就能改变图像中的头发颜色、衣服、帽子、围巾等等。而且编辑飞快，<0.5秒。简直是图像编辑的利器。相关链接项目</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488847&amp;idx=3&amp;sn=64148d8d2a1c573abeeac043dc1b0961&amp;chksm=fde83fc4064a492acd10d620182d03a053dcda595c7f741ae8dd50f97aff57c1c7316411074b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[3D服装生成新SOTA！谷歌和CMU提出FabricDiffusion：可将织物纹理从单个图像迁移到3D服装]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUwdDAlYv2ITgdKDOu7W8riapJOLuXkDL8cFpr3uMCxGiaooqI4z7Zcl4cQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>3D服装生成新工作！谷歌和CMU提出FabricDiffusion：一种将织物纹理从单个服装图像迁移到任意形状的 3D 服装的方法。FabricDiffusion性能表现SOTA！同时可以泛化到uns</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=1&amp;sn=ce5b99fc9eef591e2d2a32e18685b23a&amp;chksm=fdfb5d58fe40c7553a28295a84c1b7d5e754009ce4a1b03645486a200a0ffae0180afd997908&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[告别大头娃娃，东京大学开源数字人TANGO：能根据目标语音音频生成同步全身手势的视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en6pFnDNvWHLiaftE66MEoWpW84gLODaQtSQ9LfJlyThsRQM3P6M7VVqstMXBRlYemO0FShZxQIGrA/300?wxtype=jpeg&amp;wxfrom=0"/><p>目前已经有很多面部和唇形同步的数字人项目了，但大多只支持头像和上半身，前几天介绍的Hallo2音频驱动图像生成视频小伙伴们都非常关心，后台也有留言问有没有支持全身视频生成的方法。开源EMO再升级！复旦</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=2&amp;sn=9505a3a55ca42f7d92faa5ae63112ded&amp;chksm=fd5c5b9addd27fc9815a9cac470974e71b04eaf0728295a60310a16f8c927488b87a349c40b8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[超级智能“试衣镜”！GarDiff：高保真保持目标人物特征和服装细节，虚拟试穿技术新SOTA！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIef94PhQmUMcfJSkj4W9NicELKlw377icJuhpfjx2VUNPWKHMM0Gqib5Eg/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍了很多关于虚拟试穿的文章，本公众号也总结了虚拟试衣专题在公众号菜单栏，感兴趣的小伙伴可以在公众号内搜索“虚拟试衣”阅读～今天给大家介绍一个最新的虚拟试穿技术GarDiff，它可以分析</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=3&amp;sn=6e2618dabeac5fd5b822c59df894a492&amp;chksm=fdc18973ba2f4374fac3decc02efd056fb681c2a927776c26cdf53afbb42170bdc56c98183d7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GarmentAligner，解决服装生成中语义对齐、数量、位置和相互关系等问题。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek9OAibiaD8uzvEUOVyfeBR4Jdf4ILiayFhFoSg9mrsibicO6wic29rCpGI5ibIyargoQS8aFXOwkjoq5a1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>中山大学和联想研究院提出一个能够根据文字描述生成服装图像的智能工具GarmentAligner。它可以从已有服装图像中提取出各个组成部分，并记录下它们的位置和数量。接着根据你的描述进行匹配，找出最吻合</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=4&amp;sn=77ddbe628b51eea6da4b03d70bd1d3ed&amp;chksm=fdbac772d051d11622b849fa309e41b83c68050c6553642cc45ae1eaf80ac0dc44232c29a49b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Flux LoRA | Then and Now：可将历史照片和现代场景融合，实现不同时间点的对比展示。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwa5IZKDYBAO7LLOlXfTp24Tpqlh9Q3AG0qQKRWByqvmjXWQzoSf3LVqA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家推荐一个我最近发现的特别棒的概念 LoRA 模型-Then and Now，能生成过去和现在的照片相匹配的图像，可将历史照片和现代场景融合在一个画面中，实现不同时间点的对比展示。作为概念训练</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488815&amp;idx=1&amp;sn=03a6f96aa01c56f293f6fc1861d1653d&amp;chksm=fd25cf17002f880c9d817ed398f999cc8d515b38342540e317213142b5ffdcedf2bfb8b1c84a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 08 Nov 2024 22:25:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[英伟达提出ComfyGen：通过LLM来生成匹配文本的工作流。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eny4Iriba5NSXkHvLxicLITJDqnLYd3byQhrC0bKwIGSOFEPvibmO8gTicw8bg8Y16oLT3TR7jWM7j2AA/300?wxtype=jpeg&amp;wxfrom=0"/><p>ComfyGen的核心在于通过LLM来匹配给定的文本提示与合适的工作流程。该方法从500个来自用户的多样化提示生成图像，随后使用一系列美学预测模型对生成结果进行评分。这些评分与相应的工作流程形成了一个</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488815&amp;idx=2&amp;sn=8461e5284d5af2575cdd351e1c1e34bf&amp;chksm=fd7bb8ae9c043cc72ed1417f3f69676c50a5131891fe10a04d85ea49d972dcbd0b7008f54ba9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 08 Nov 2024 22:25:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488815&amp;idx=3&amp;sn=546be76ecc65a94c2317fb1a238555d2&amp;chksm=fd5eaa4f93de7f597980d694e1cb745412fc619e6a3f3ae7ce91dd97913a06a674d1b5e3ffba&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 08 Nov 2024 22:25:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[混合专家模型 (MoE) 详解]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emOmPkEN8wch4M3txAeFlUKiaJVQDmPia8vwoiccAOopcuhcVyF7mZX5Oa03m0QFQpQxO4NicicCvicnbyw/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着 Mixtral 8x7B (announcement, model card) 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的 Transforme</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488814&amp;idx=1&amp;sn=97482b175fcb34cafbe65e8ae8a1402f&amp;chksm=fd85cbfec9e615eee2d84056a0f37f66d74e7c4d9660a9e7cc1e785b0c9a1bbe9c630a9b0a48&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 07 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[统一图像生成模型OmniGen：可由多模态提示直接生成各种图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488814&amp;idx=2&amp;sn=abd2b31fb9f16f038e4bf790b3521c0a&amp;chksm=fd38baa4e4e1de996e775471047d00e83da572de22b61f69c56c49411761f1786fb882b7002c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 07 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NeurIPS2024 | OCR-Omni来了！字节&amp;华师提出统一的多模态生成模型TextHarmony。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUw9icM2HlibUyI4RtyIsiaB9FOY9taoKCFlibTeImBZT585GC3ias7FialR6UA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能领域，赋予机器类人的图像文字感知、理解、编辑和生成能力一直是研究热点。目前，视觉文字领域的大模型研究主要聚焦于单模态生成任务。尽管这些模型在某些任务上实现了统一，但在 OCR 领域的多数任务</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488814&amp;idx=3&amp;sn=9821b297fc93dd6351a7ca88002155fb&amp;chksm=fdcb7867cbd617d238bf53a1ebb94e79f2b0faf0c3aa85d6223595757b67437f32eb34dd6d01&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 07 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
