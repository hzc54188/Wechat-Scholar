<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    





















    <item>
      <title><![CDATA[文本转视频模型Allegro，可以生成长达 6 秒、15 FPS 和 720p 分辨率的高质量视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXSHQPic5rJ8IBKiaNosJKl47LGniaXW6NGticOnwCibeK5T8qlic4oPfsd5XwpM5KFOOqbuSEJIUictd6A/640?wxtype=jpeg&amp;wxfrom=0"/><p>Allegro 是一个强大的文本转视频模型，可以通过简单的文本输入生成长达 6 秒、15 FPS 和 720p 分辨率的高质量视频。主要特点• 开源：完整的模型权重和代码可供社区使用，Apache 2</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488847&amp;idx=1&amp;sn=7e920fd13492baee8e07ce1fa1ee3e58&amp;chksm=fd5338152f52b31169fb2443f34daf6b646929be096615b115094c4133cb6f58501db26b7fe0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 10 Nov 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[NeurIPS2024 | OCR-Omni来了！字节&amp;华师提出统一的多模态生成模型TextHarmony。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUw9icM2HlibUyI4RtyIsiaB9FOY9taoKCFlibTeImBZT585GC3ias7FialR6UA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能领域，赋予机器类人的图像文字感知、理解、编辑和生成能力一直是研究热点。目前，视觉文字领域的大模型研究主要聚焦于单模态生成任务。尽管这些模型在某些任务上实现了统一，但在 OCR 领域的多数任务</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488847&amp;idx=2&amp;sn=2f66668760c3bba06fcc7c993b9ea415&amp;chksm=fdb2b1dbd4394e63daf13dce37abbef85d3da445c8bf3040fc869302812dede1705c11b5ea53&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 10 Nov 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Adobe发布TurboEdit：可以通过文本来编辑图像，编辑时间<0.5秒！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKcprhHqENugIHSUTwb3EOiaaqictMa8fmmNEDqsoISMhGDZH4oZmh7vtMn5sov6khPdhIypPkhDZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍Adobe研究院新的研究TurboEdit，可以通过文本来编辑图像，通过一句话就能改变图像中的头发颜色、衣服、帽子、围巾等等。而且编辑飞快，<0.5秒。简直是图像编辑的利器。相关链接项目</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488847&amp;idx=3&amp;sn=64148d8d2a1c573abeeac043dc1b0961&amp;chksm=fde83fc4064a492acd10d620182d03a053dcda595c7f741ae8dd50f97aff57c1c7316411074b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sun, 10 Nov 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[3D服装生成新SOTA！谷歌和CMU提出FabricDiffusion：可将织物纹理从单个图像迁移到3D服装]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUwdDAlYv2ITgdKDOu7W8riapJOLuXkDL8cFpr3uMCxGiaooqI4z7Zcl4cQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>3D服装生成新工作！谷歌和CMU提出FabricDiffusion：一种将织物纹理从单个服装图像迁移到任意形状的 3D 服装的方法。FabricDiffusion性能表现SOTA！同时可以泛化到uns</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=1&amp;sn=ce5b99fc9eef591e2d2a32e18685b23a&amp;chksm=fdfb5d58fe40c7553a28295a84c1b7d5e754009ce4a1b03645486a200a0ffae0180afd997908&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[告别大头娃娃，东京大学开源数字人TANGO：能根据目标语音音频生成同步全身手势的视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en6pFnDNvWHLiaftE66MEoWpW84gLODaQtSQ9LfJlyThsRQM3P6M7VVqstMXBRlYemO0FShZxQIGrA/300?wxtype=jpeg&amp;wxfrom=0"/><p>目前已经有很多面部和唇形同步的数字人项目了，但大多只支持头像和上半身，前几天介绍的Hallo2音频驱动图像生成视频小伙伴们都非常关心，后台也有留言问有没有支持全身视频生成的方法。开源EMO再升级！复旦</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=2&amp;sn=9505a3a55ca42f7d92faa5ae63112ded&amp;chksm=fd5c5b9addd27fc9815a9cac470974e71b04eaf0728295a60310a16f8c927488b87a349c40b8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[超级智能“试衣镜”！GarDiff：高保真保持目标人物特征和服装细节，虚拟试穿技术新SOTA！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIef94PhQmUMcfJSkj4W9NicELKlw377icJuhpfjx2VUNPWKHMM0Gqib5Eg/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍了很多关于虚拟试穿的文章，本公众号也总结了虚拟试衣专题在公众号菜单栏，感兴趣的小伙伴可以在公众号内搜索“虚拟试衣”阅读～今天给大家介绍一个最新的虚拟试穿技术GarDiff，它可以分析</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=3&amp;sn=6e2618dabeac5fd5b822c59df894a492&amp;chksm=fdc18973ba2f4374fac3decc02efd056fb681c2a927776c26cdf53afbb42170bdc56c98183d7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GarmentAligner，解决服装生成中语义对齐、数量、位置和相互关系等问题。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek9OAibiaD8uzvEUOVyfeBR4Jdf4ILiayFhFoSg9mrsibicO6wic29rCpGI5ibIyargoQS8aFXOwkjoq5a1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>中山大学和联想研究院提出一个能够根据文字描述生成服装图像的智能工具GarmentAligner。它可以从已有服装图像中提取出各个组成部分，并记录下它们的位置和数量。接着根据你的描述进行匹配，找出最吻合</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488816&amp;idx=4&amp;sn=77ddbe628b51eea6da4b03d70bd1d3ed&amp;chksm=fdbac772d051d11622b849fa309e41b83c68050c6553642cc45ae1eaf80ac0dc44232c29a49b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 10 Nov 2024 01:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Flux LoRA | Then and Now：可将历史照片和现代场景融合，实现不同时间点的对比展示。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwa5IZKDYBAO7LLOlXfTp24Tpqlh9Q3AG0qQKRWByqvmjXWQzoSf3LVqA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家推荐一个我最近发现的特别棒的概念 LoRA 模型-Then and Now，能生成过去和现在的照片相匹配的图像，可将历史照片和现代场景融合在一个画面中，实现不同时间点的对比展示。作为概念训练</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488815&amp;idx=1&amp;sn=03a6f96aa01c56f293f6fc1861d1653d&amp;chksm=fd25cf17002f880c9d817ed398f999cc8d515b38342540e317213142b5ffdcedf2bfb8b1c84a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 08 Nov 2024 22:25:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[英伟达提出ComfyGen：通过LLM来生成匹配文本的工作流。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eny4Iriba5NSXkHvLxicLITJDqnLYd3byQhrC0bKwIGSOFEPvibmO8gTicw8bg8Y16oLT3TR7jWM7j2AA/300?wxtype=jpeg&amp;wxfrom=0"/><p>ComfyGen的核心在于通过LLM来匹配给定的文本提示与合适的工作流程。该方法从500个来自用户的多样化提示生成图像，随后使用一系列美学预测模型对生成结果进行评分。这些评分与相应的工作流程形成了一个</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488815&amp;idx=2&amp;sn=8461e5284d5af2575cdd351e1c1e34bf&amp;chksm=fd7bb8ae9c043cc72ed1417f3f69676c50a5131891fe10a04d85ea49d972dcbd0b7008f54ba9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 08 Nov 2024 22:25:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488815&amp;idx=3&amp;sn=546be76ecc65a94c2317fb1a238555d2&amp;chksm=fd5eaa4f93de7f597980d694e1cb745412fc619e6a3f3ae7ce91dd97913a06a674d1b5e3ffba&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 08 Nov 2024 22:25:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[混合专家模型 (MoE) 详解]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emOmPkEN8wch4M3txAeFlUKiaJVQDmPia8vwoiccAOopcuhcVyF7mZX5Oa03m0QFQpQxO4NicicCvicnbyw/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着 Mixtral 8x7B (announcement, model card) 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的 Transforme</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488814&amp;idx=1&amp;sn=97482b175fcb34cafbe65e8ae8a1402f&amp;chksm=fd85cbfec9e615eee2d84056a0f37f66d74e7c4d9660a9e7cc1e785b0c9a1bbe9c630a9b0a48&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 07 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[统一图像生成模型OmniGen：可由多模态提示直接生成各种图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488814&amp;idx=2&amp;sn=abd2b31fb9f16f038e4bf790b3521c0a&amp;chksm=fd38baa4e4e1de996e775471047d00e83da572de22b61f69c56c49411761f1786fb882b7002c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 07 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NeurIPS2024 | OCR-Omni来了！字节&amp;华师提出统一的多模态生成模型TextHarmony。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUw9icM2HlibUyI4RtyIsiaB9FOY9taoKCFlibTeImBZT585GC3ias7FialR6UA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能领域，赋予机器类人的图像文字感知、理解、编辑和生成能力一直是研究热点。目前，视觉文字领域的大模型研究主要聚焦于单模态生成任务。尽管这些模型在某些任务上实现了统一，但在 OCR 领域的多数任务</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488814&amp;idx=3&amp;sn=9821b297fc93dd6351a7ca88002155fb&amp;chksm=fdcb7867cbd617d238bf53a1ebb94e79f2b0faf0c3aa85d6223595757b67437f32eb34dd6d01&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 07 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯发布业界最大开源MoE模型：Hunyuan-Large，具有3890亿参数，在长文本处理、常识推理、数学能力等方面表现出色。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emo5W5SskRqTMcQ39iaLjtibd6J8s54Iu3pUT6mLLnbic6sYmEEkC7TkRkQKSSVTg6VtPoZogicQCHTgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯发布开源 Mixture of Experts（MoE）模型：腾讯混元大模型（Hunyuan-Large），这是目前在业界是规模最大的开源 Transformer 专家模型，具有 3890 亿参数</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488790&amp;idx=1&amp;sn=b99000f43374054ba84a7239ca056342&amp;chksm=fdea95e56c1b06b57aa0546ac9ffb9a37a0cc6a20cfc4355a4d55b0b1f4c6b981e047690663b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 06 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[登顶Hugging Face文生图模型榜首！Recraft V3可精确处理复杂长文本和手指等解剖学细节！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emo5W5SskRqTMcQ39iaLjtibdfWiaulu8SLia1CmOhUK8o1DFXv1ZuIO9wgBw4mH09eF6uw4vAzibRT3rQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>Recraft推出了其最新的图像生成模型—Recraft V3。通过引入设计语言思维，Recraft V3为图像生成树立了新的卓越标准。在Hugging Face 人工智能文本转图像模型排行榜。它以 </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488789&amp;idx=1&amp;sn=00bd975be576f39119dfc98be2a85c0e&amp;chksm=fd0b5a4ade0de5906f69aa87ee309cb8736db086774a9681124a61452c8f8ce29351b7c3dfde&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 05 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[刚刚，阿里重磅开源基于FLUX的In-Context LoRA，可一次生成多张风格和ID一致的图片集。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwaZq27QklGnm2euIIenh9a4vvHuaekrpqk2PuGae3ghYQjGRse85BXsA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍In-Context LoRA 这个项目太强了，前几天发布的时候就引起了许多小伙伴的关注，但是当时还没有开源，就在刚刚，作者开源了In-Context LoRA项目。它基于FLUX训练，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247488724&amp;idx=1&amp;sn=8db864e5b1098b3e661805547439293e&amp;chksm=fd78c7d59b25760d6483cab92f446ff28bca67952f718fb50e66e99aaf71a95bfbb8d204f96e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 04 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
